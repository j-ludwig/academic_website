[["index.html", "Economic Data Analysis Preface Introduction Author Book Navigation", " Economic Data Analysis Julian F. Ludwig (Texas Tech University) May 31, 2024 Preface Introduction This book is your guide to preparing for a career in Economics and Finance. These fields need you to be great at understanding and explaining data. This book will help you improve your data science and analytics skills, and use them to answer questions about business cycles, economic growth, and financial markets. A significant part of this book is about teaching you how to program in R, a popular language used for data analysis and graphics. You’ll also learn how to use complementary software like RStudio, R Markdown, and LaTeX to craft professional reports that showcase your data analyses. But it’s not all about coding. We’ll also explore economic indicators to help you understand business cycles and regional differences in economic performance. You’ll learn about traditional measures like GDP growth, inflation, money supply, and interest rates. We also touch on newer, survey- and text-based indicators, such as consumer confidence indices and news-based indicators of economic uncertainty, which come from language analysis of newspapers, social media, or Google searches. The book is divided into six parts: Part I: Introduction to R Part II: Measurement in Economics Part III: Economic Data Processing Part IV: Patterns in Economics Part V: Insights From Real-Time Data and Surveys Part VI: Insights From Textual Data Each part builds on the previous one, providing you with a coherent learning journey. By the time you finish this book, you’ll be ready to analyze a wide range of economic and financial data. Your learning progress will be assessed based on the completion of DataCamp courses and the production of three data reports, one for each module. Author Julian F. Ludwig Assistant Professor Department of Economics Texas Tech University 253 Holden Hall Lubbock, TX 79409 Website: www.julianfludwig.com Email: julian.ludwig@ttu.edu Education Ph.D., Economics, University of Texas at Austin, 2019 M.S., Economics, University of Texas at Austin, 2016 M.S., International and Monetary Economics, University of Bern in Switzerland, 2014 B.S., Economics, University of Bern in Switzerland, 2012 Research Interests Macroeconomics Time Series Econometrics Courses Offered ECO 5316 Time Series Econometrics ECO 5311 Macroeconomic Theory and Policy ECO 4306 Economic and Business Forecasting ECO 4300 Economic Research: Data-Driven Analysis ECO 3323 Principles of Money, Banking and Credit Book Navigation To make the most of this reading experience, it’s essential to familiarize yourself with the available interactive features of this GitBook.1 You can adjust font size and type using the “A” letter icon located in the top-left toolbar. Clicking on this icon reveals a dropdown menu where you can select font size (using the smaller “A” for reducing and the larger “A” for increasing size) and font type (choosing between Serif and Sans). The same dropdown also allows you to switch between predefined themes: White, Sepia, or Night. Navigating between chapters is straightforward. Arrows on the side of the page, or using the left and right arrows on your keyboard, allow you to move between chapters easily. You can also use the table of contents located in the left sidebar, offering a complete overview and direct links to every chapter and subsection in the book. The left sidebar, which houses the table of contents, can be toggled on or off. Do this by pressing the “s” key or clicking on the icon with four horizontal parallel lines in the toolbar. You can search for specific words or phrases across the entire book. Activate the search function by pressing the “f” key or clicking the magnifying glass symbol in the toolbar. If you prefer to read offline, the book can be downloaded as a PDF file. To do this, click the Adobe Acrobat symbol in the toolbar. On the top right of the page, you’ll find social media buttons for Twitter, Facebook, LinkedIn, Weibo, and Instapaper. These allow you to share individual chapters on your social media profiles and invite comments. By understanding these features and customizations, you can significantly improve your reading and navigation experience when working through this book. References "],["part-i.html", "Part I: Introduction to R", " Part I: Introduction to R Part I introduces R and other free tools used in economic data analysis. The primary focus is on R, a language tailored for statistical computing and graphics. It also delves into RStudio, R Markdown, and LaTeX as tools for creating dynamic documents using R. Readers will be guided on installation and the effective use of these tools, emphasizing their application in economics and finance. "],["software-overview.html", "Chapter 1 Software Overview", " Chapter 1 Software Overview The following software and programming languages are commonly used for conducting economic analyses: R: R (R Core Team 2023) is a programming language designed for statistical computing and graphics. This language is widely used by data scientists and researchers for a range of tasks such as data processing, visualization, model estimation, and performing predictive or causal inference. For instance, one can use R to import GDP data, plot the data, compute the GDP growth rate from this data, and finally, apply time-series modeling techniques to predict future GDP growth. LaTeX: LaTeX (Lamport 1986) is a powerful document preparation system widely used for typesetting scientific and technical documents. Similar to Microsoft Word, LaTeX is a text formatting software, but it offers advanced support for mathematical equations, cross-references, bibliographies, and more. LaTeX is particularly useful for creating professional-looking PDF documents with complex mathematical notation. Markdown: Markdown (Gruber 2004) is designed for simple and easy formatting of plain text documents. It uses plain text characters and a simple syntax to add formatting elements such as headings, lists, emphasis, links, images, and code blocks. Markdown allows for quick and readable content creation without the need for complex formatting options. It is often used for creating documentation, writing blog posts, and formatting text in online forums. Markdown documents can be easily converted to other formats, making it highly portable. R Markdown: R Markdown (Allaire et al. 2024; Xie 2023) combines R with Markdown, LaTeX, and Microsoft Word. This fusion creates an environment where data scientists and researchers can combine text and R code within the same document, eliminating the process of creating graphs in R and then transferring them to a Word or LaTeX document. An R Markdown document can be converted into several formats, including HTML, PDF, or Word. To generate a PDF, R Markdown initially crafts a LaTeX file which it then executes in the background. Thanks to the embedded R code in the R Markdown document, it’s possible to automate data downloading and updating to ensure a financial report remains up-to-date. In fact, the text you’re reading now was crafted with R Markdown. RStudio: RStudio (Posit Team 2023) is an Integrated Development Environment (IDE) for R. An IDE is a software application that combines multiple programs into a single, user-friendly platform. Think of RStudio as the all-in-one tool you’ll use for conducting economic research - it will handle all tasks, running R, Markdown, and LaTeX in the background for you. However, for RStudio to work, R, R Markdown, and a LaTeX processor must be installed on your computer, so that RStudio can use these programs in the background. Pandoc: Pandoc (MacFarlane 2023) is a document converter. Originating outside the R ecosystem, it has been adopted by RStudio and is essential to R Markdown’s flexibility. When you run an R Markdown document, under the hood, it’s Pandoc that transforms the Markdown file into a variety of outputs, whether that’s a Word document, an HTML web page, or a slide show. It comes automatically bundled with RStudio, ensuring that R Markdown users don’t have to go through the manual installation process. TinyTeX: LaTeX is a markup language designed for document preparation and typesetting. By itself, it isn’t an executable software but relies on a TeX distribution for processing. While traditional TeX distributions like TeX Live or MikTeX might come with extensive components not commonly used by everyday R users, TinyTeX (Xie 2024b) offers a minimalistic, lightweight LaTeX distribution. It’s optimized for R Markdown users and is built upon the TeX Live system (TeX Users Group 1996). R packages: R provides a rich set of basic functions that can be extended with R packages. These packages are a collection of functions written by contributors for specific tasks. For example, the quantmod (Ryan and Ulrich 2024a) package provides functions for financial quantitative modeling. All the software and programming languages mentioned above are open-source, meaning they are freely available and actively developed by a community of contributors. By mastering these tools, you will have the necessary skills to perform data analysis, create reproducible reports, and effectively communicate your findings in the field of economics. References "],["software-installation.html", "Chapter 2 Software Installation 2.1 Install R 2.2 Install RStudio 2.3 Install R Markdown 2.4 Install LaTeX 2.5 Install R Packages 2.6 18-Step Test", " Chapter 2 Software Installation For data analysis, RStudio will serve as your environment for writing code and text. However, as an Integrated Development Environment (IDE), RStudio is not a standalone program; it depends on R, R Markdown, and a LaTeX processor installed on your system. RStudio interacts with these programs in the background to generate an output. Below, you will find the installation instructions for each of these programs. Additionally, a set of 18 steps is provided to help you verify whether R, RStudio, R Markdown, and LaTeX have been installed correctly. 2.1 Install R To install R on your computer, follow the instructions below: For MacOS: To download R for MacOS, visit the R project website: www.r-project.org. Click CRAN mirror and choose your preferred mirror. It doesn’t really matter which mirror you choose, simply choose a location close to you, e.g. National Institute for Computational Sciences, Oak Ridge, TN. Select Download R for macOS. Under “Latest release”, read the first paragraph to check whether the program is compatible with your operating system (OS) and processor. To find your computer’s OS and processor, click the top left Apple icon, and click “About this Mac.” Under “macOS”, you will see both the name (e.g. “Ventura”, “Catalina”, “Monterey”) and the number (e.g. “Version 13.4.1”) of the OS, and under “Processor” you will either see that your computer is run by an Intel processor or an Apple silicon (M1/M2) processor. If the operating system (OS) and the processor are compatible, click on the first R-X.X.X.pkg (where X represents the R version numbers). Otherwise, if you have an older OS or an Intel processor, click on a version further down that is compatible with your system. Once the file has downloaded, click it to proceed to installation, leaving all default settings as they are. For Windows: To download R for Windows, visit the R project website: www.r-project.org. Click CRAN mirror and choose your preferred mirror. It doesn’t really matter which mirror you choose, simply choose a location close to you, e.g. Revolution Analytics, Dallas, TX. Select Download R for Windows. Select “base”, and read whether the program is compatible with your Windows version. If it is compatible, click Download R-X.X.X for Windows (X are numbers), and otherwise click here for older versions. Once the file has downloaded, click it to proceed to installation, leaving all default settings as they are. 2.2 Install RStudio Visit the RStudio website: www.rstudio.com and navigate to the download page. Click DOWNLOAD. Scroll down to “All Installers” section. Choose the download that matches your computer. If you have a Mac, it’s most likely “macOS 10.15+”; then click the download link (e.g. “RStudio-2022.07.1-554.dmg”). If you have a Windows, it’s most likely “Windows 10/11” and click the download link (e.g. “RStudio-2022.07.1-554.exe”). Open the file when it has downloaded, and install with the default settings. 2.3 Install R Markdown R Markdown can be installed from inside the RStudio IDE. To download R Markdown, open RStudio, after you have successfully installed R and RStudio. In RStudio, find the “Console” window. Type the command install.packages(\"rmarkdown\") in the console and press Enter. 2.4 Install LaTeX When it comes to installing LaTeX, there are several software options available. While most options work well, I recommend using TinyTeX (Xie 2024b). TinyTeX as it is an easy-to-maintain LaTeX distribution. Other good alternatives include MacTeX and MiKTeX. LaTeX is the underlying program responsible for word processing and generating PDF reports within RStudio. To install TinyTeX using RStudio, follow these steps: Open RStudio after successfully installing R, RStudio, and R Markdown. Locate the “Console” window within RStudio. Type install.packages(\"tinytex\") and press Enter. Type tinytex::install_tinytex() and press Enter. Type install.packages(\"knitr\") and press Enter. 2.5 Install R Packages R provides a set of basic functions that can be extended using packages. To install one, such as the quantmod package by Ryan and Ulrich (2024a), proceed as follows: Open RStudio. In the RStudio window, find the “Console” window. Type the command install.packages(\"quantmod\") in the console and press Enter. Wait for the installation process to complete. R will download and install the package from the appropriate repository. After installation, you can use the package in your script by including the line library(\"quantmod\") at the beginning. Remember to execute the library(\"quantmod\") command each time you want to use functions from the quantmod package in your code. It is common practice to load the necessary packages at the beginning of your script, even if you don’t use all of them immediately. This ensures that all the required functions and tools are available when needed and promotes a consistent and organized approach to package management in your code. As a side note, the quantmod package includes the getSymbols function, which is commonly used to download financial data, such as the S&amp;P 500 index (GSPC): library(&quot;quantmod&quot;) getSymbols(Symbols=&quot;^GSPC&quot;) ## [1] &quot;GSPC&quot; head(GSPC) ## GSPC.Open GSPC.High GSPC.Low GSPC.Close GSPC.Volume GSPC.Adjusted ## 2007-01-03 1418.03 1429.42 1407.86 1416.60 3429160000 1416.60 ## 2007-01-04 1416.60 1421.84 1408.43 1418.34 3004460000 1418.34 ## 2007-01-05 1418.34 1418.34 1405.75 1409.71 2919400000 1409.71 ## 2007-01-08 1409.26 1414.98 1403.97 1412.84 2763340000 1412.84 ## 2007-01-09 1412.84 1415.61 1405.42 1412.11 3038380000 1412.11 ## 2007-01-10 1408.70 1415.99 1405.32 1414.85 2764660000 1414.85 Here, the getSymbols function retrieves the historical data for the S&amp;P 500 index from Yahoo Finance, and stores it in the GSPC object. The head function then displays the first few rows of the downloaded data. R packages provide a wealth of specialized functions for specific tasks. To use a function from a particular package, you can indicate the package by preceding the function with the package name followed by a double colon ::. For example, quantmod::getSymbols() specifies the getSymbols() function from the quantmod package. This practice helps to avoid conflicts when multiple packages provide functions with the same name. It also allows users to easily identify the package associated with the function, promoting clarity and reproducibility in code. 2.6 18-Step Test To ensure that R, RStudio, R Markdown, and LaTeX are installed properly, you can follow the 18-step test provided below. This test will help verify the functionality of the installed programs and identify any potential issues or errors. During this process, you may encounter the following issues: Issue with Generating PDF: If you are unable to generate a PDF file in step 15, it is likely due to an issue with the installation of LaTeX. In such cases, please revisit the instructions for installing LaTeX in Chapter 2.4 and ensure you have followed them correctly. Alternatively, you can consider installing MacTeX or MiKTeX instead of TinyTeX. Non-Latin Alphabet Language: If your computer language is not based on the Latin alphabet (e.g., Chinese, Arabic, Farsi, Russian, etc.), additional instructions may be required. You can refer to this video for specific guidance: youtu.be/pX_fy2fyM30. I encourage you to persist and do your best to install all the required software, even if it takes some time. Downloading and installing programs is a critical skill that is essential in almost every profession today. This is an excellent opportunity to acquire this skill. Keep going and don’t hesitate to seek additional support or resources if needed. It’s common to encounter challenges when installing software, and resources like google.com and stackoverflow.com can provide helpful answers and suggestions. If you encounter an error, simply copy and paste the error message into a search engine, and you’ll likely find solutions and guidance from the community. If you fail to install R, RStudio, and LaTeX, I recommend using RStudio Cloud, an online platform where you can perform all the necessary tasks directly in your web browser. You can access RStudio Cloud at rstudio.cloud. While signing up is free, please note that some features may require a fee. Make a Plot To continue with the test, make sure you have R, RStudio, R Markdown, and LaTeX installed and are connected to the internet. Follow the steps below in RStudio: Type and execute install.packages(\"quantmod\") in the RStudio console. Click on the top-left plus sign then click R Script. Click File - Save As... then choose a familiar folder. Copy and paste the following R code into your R Script: library(&quot;quantmod&quot;) treasury10y &lt;- getSymbols(Symbols = &quot;GS10&quot;, src = &quot;FRED&quot;, auto.assign = FALSE) plot(treasury10y, main = &quot;10-Year Treasury Rate&quot;) Figure 2.1: R Plot Click on Source: (or use the shortcut Ctrl + Shift + Enter or Cmd + Shift + Return). You should now see a plot of the 10-year Treasury rate on your screen. Compare it to the rate displayed on fred.stlouisfed.org/series/GS10. Save Plot as PDF Continue with the following steps in RStudio: Add the line: pdf(file=\"myplot.pdf\",width=6,height=4) before the plot function, and add dev.off() after the plot: library(&quot;quantmod&quot;) treasury10y &lt;- getSymbols(Symbols = &quot;GS10&quot;, src = &quot;FRED&quot;, auto.assign = FALSE) pdf(file = &quot;myplot.pdf&quot;, width = 6, height = 4) plot(treasury10y, main = &quot;10-Year Treasury Rate&quot;) dev.off() Click on Source: (or use the shortcut Ctrl + Shift + Enter or Cmd + Shift + Return). Now navigate to the same folder on your computer where you saved the R script. There should be a file called myplot.pdf - open it. You should now see the PDF version of the plot displaying the Treasury rate. If you encounter no error message but cannot locate the myplot.pdf file, it’s possible that R saved it in a different folder than where the R script is located. To check where R saves the plot, type getwd() in the console, which stands for “get working directory.” If you want to change the working directory and have R save the files in a different folder, type setwd(\"/Users/.../...\"), replacing \"/Users/.../...\" with the path to the desired folder. Run Marked Code To run only one line or one variable, mark it and then click Run: (or use the shortcut Ctrl + Enter or Cmd + Return). Follow these steps in RStudio: Mark the variable treasury10y: Click Run: (or use shortcut Ctrl + Enter or Cmd + Return) You should see the data displayed in your console, ending with 2024-04-01 4.54. Create PDF with R Markdown Next, let’s ensure that R Markdown is working. If you have installed LaTeX and knitr, follow these steps in RStudio: Click on the top-left plus sign then click R Markdown... A dialog box will appear - select Document and choose PDF, then click OK: Figure 2.2: New R Markdown You should now see a file with text and code. Click File - Save As... and choose a familiar folder to save the file. Click Knit: (or use the shortcut Ctrl + Shift + K or Cmd + Shift + K). A PDF file should appear on your screen and also in your chosen folder. Next, locate the following lines: 16 ```{r cars} 17 summary(cars) 18 ``` Replace these lines with the following (do not copy the line numbers): 16 ```{r, message=FALSE,warning=FALSE,echo=FALSE} 17 library(&quot;quantmod&quot;) 18 treasury10y &lt;- getSymbols(Symbols=&quot;GS10&quot;,src=&quot;FRED&quot;,auto.assign=FALSE) 19 plot(treasury10y,main=&quot;10-Year Treasury Rate&quot;) 20 ``` Click Knit: (or use the shortcut Ctrl + Shift + K or Cmd + Shift + K). You should now see a file that looks similar to this: Figure 2.3: PDF File Produced with R Markdown Hint: You can set echo=TRUE to include R code in your report. You can now change the title of the file and the text to create a professional report. If you click the arrow next to Knit: you have options to export your file as an HTML or Word document instead of a PDF document, which is convenient when designing a website or writing an app: Troubleshooting That’s it! If everything worked as expected, you’re good to go. If not, continue troubleshooting until it works. References "],["r-basics.html", "Chapter 3 R Basics 3.1 RStudio Interface 3.2 Basic Operations 3.3 Dates and Times 3.4 Graphs 3.5 Functions 3.6 R Packages 3.7 File Management 3.8 Optimal Workflow", " Chapter 3 R Basics R, being a programming language, offers a rich variety of operations to facilitate data analysis. This chapter offers an introduction to fundamental R operations, utilizing the RStudio interface. RStudio is an Integrated Development Environment (IDE) tailored for R, delivering a use-friendly interface for R programming. In addition to R, RStudio is compatible with other languages such as R Markdown, an instrument for crafting dynamic documents, discussed in Chapter 6. The chapter begins with an overview of the RStudio interface. Subsequently, it navigates through the essentials of R programming, emphasizes efficient coding practices, and highlights some of the R packages that are central to data analysis. 3.1 RStudio Interface After launching RStudio on your computer, navigate to the menu bar and select “File,” then choose “New File,” and finally click on “R Script.” Alternatively, you can use the keyboard shortcut Ctrl + Shift + N (Windows/Linux) or Cmd + Shift + N (Mac) to create a new R script directly. Figure 3.1: RStudio Interface Once you have opened a new R script, you will notice that RStudio consists of four main sections: Source (top-left): This section is where you write your R scripts. Also known as do-files, R scripts are files that contain a sequence of commands which can be executed either wholly or partially. To run a single line in your script, click on that line with your cursor and press the button. However, to streamline your workflow, I recommend using the keyboard shortcut Ctrl + Enter (Windows/Linux) or Cmd + Enter (Mac) to run the line without reaching for the mouse. If you want to execute only a specific portion of a line, select that part and then press Ctrl + Enter or Cmd + Enter. To run all the commands in your R script, use the button or the keyboard shortcut Ctrl + Shift + Enter (Windows/Linux) or Cmd + Shift + Enter (Mac). Console (bottom-left): Located below the Source section, the Console is where R executes your commands. You can also directly type commands into the Console and see their output immediately. However, it is advisable to write commands in the R Script instead of the Console. By doing so, you can save the commands for future reference, enabling you to reproduce your results at a later time. Environment (top-right): In the upper-right section, the Environment tab displays the current objects stored in memory, providing an overview of your variables, functions, and data frames. To create a variable, you can use the assignment operator &lt;- (reversed arrow). Once a variable is created and assigned a numeric value, it can be utilized in arithmetic operations. For example: a &lt;- 60 a + 20 ## [1] 80 Files/Plots/Packages/Help/Viewer (bottom-right): The bottom-right panel contains multiple tabs: Files: displays your files and folders Plots: displays your graphs Packages: lets you manage your R packages Help: provides help documentation Viewer: lets you view local web content The R script, located on the top-left in Figure 3.1, is a text file that contains your R code. You can execute parts of the script by selecting a subset of commands and pressing Ctrl + Enter (or Cmd + Enter), or run the entire script by pressing Ctrl + Shift + Enter (or Cmd + Shift + Enter). Any text written after a hashtag (#) in an R Script is considered comments and is not executed as code. Comments are valuable for providing explanations or annotations for your commands, enhancing the readability and comprehensibility of your code. # This is a comment in an R script x &lt;- 10 # Assign the value 10 to x y &lt;- 20 # Assign the value 20 to y z &lt;- x + y # Add x and y and assign the result to z print(z) # Print the value of z ## [1] 30 The output displayed after two hashtags (##) in the example above: ## [1] 30, is not part of the actual R Script. Instead, it represents a line you would observe in your console when running the R Script. It showcases the result or value of the variable z in this case. To facilitate working with lengthy R scripts, it is recommended to use a separate window. You can open a separate window by selecting in the top-left corner. Figure 3.2: RStudio Interface with Separate R Script Window When the R Script is in a separate window, you can easily switch between the R Script window and the Console/Environment/Plot Window by pressing Alt + Tab (or Command + ` on Mac). This allows for convenient navigation between different RStudio windows. 3.2 Basic Operations This section delves into fundamental operations in R. These commands are typically written inside an R script and can be executed line by line using Ctrl + Enter (or Cmd + Enter). To run the entire script at once, press Ctrl + Shift + Enter (or Cmd + Shift + Enter). 3.2.1 Arithmetic Operations R offers a comprehensive suite of arithmetic operations, similar to what you’d anticipate in any programming language: # Addition 2 + 2 ## [1] 4 # Subtraction 5 - 3 ## [1] 2 # Multiplication 3 * 4 ## [1] 12 # Division 8 / 2 ## [1] 4 # Exponents (raising to a power) 2^3 ## [1] 8 # Remainder after division (modulo operation) 7 %% 3 ## [1] 1 # Integer division (quotient) 7 %/% 3 ## [1] 2 3.2.2 Logical Operations Logical operations are essential in programming to compare and test the relationships between values. In R, there are several built-in logical operators: # Greater than 2 &gt; 3 ## [1] FALSE # Less than 3 &lt; 4 ## [1] TRUE # Equal to 5 == 5 ## [1] TRUE # Not equal to 5 != 6 ## [1] TRUE # Greater than or equal to 5 &gt;= 5 ## [1] TRUE # Less than or equal to 6 &lt;= 5 ## [1] FALSE # Logical AND - Returns TRUE if both statements are true (3 &gt; 2) &amp; (1 &gt; 3) ## [1] FALSE # Logical OR - Returns TRUE if one of the statements is true (3 &gt; 2) | (1 &gt; 3) ## [1] TRUE # Logical NOT - Reverse the result, returns FALSE if the result is true !(5 == 5) ## [1] FALSE # Using boolean values directly: !TRUE ## [1] FALSE # Exclusive OR - Evaluates to TRUE if one, and only one, of the expressions is TRUE xor(TRUE, FALSE) ## [1] TRUE xor(TRUE, TRUE) ## [1] FALSE # Checks if a value is TRUE isTRUE(TRUE) ## [1] TRUE # Checks if a value is FALSE isFALSE(TRUE) ## [1] FALSE These operators are fundamental when creating conditions in loops or functions. The logical “AND” (&amp;) will evaluate as TRUE only if both of its operands are true. The logical “OR” (|) will evaluate as TRUE if at least one of its operands is true. The logical “NOT” (!) negates the result, turning TRUE results into FALSE and vice versa. 3.2.3 String Operations Strings in R are sequences of characters. They’re crucial for tasks like text processing and data cleaning. Here are some basic operations you can perform with strings: To create a string, you can use either single (') or double (\") quotes: &quot;Hello!&quot; &#39;RStudio is fun.&#39; ## [1] &quot;Hello!&quot; ## [1] &quot;RStudio is fun.&quot; Strings can be combined or “concatenated” using the paste() function: paste(&quot;Hello!&quot;, &#39;RStudio is fun.&#39;) ## [1] &quot;Hello! RStudio is fun.&quot; To determine the number of characters in a string, you can use the nchar() function: nchar(&quot;Hello!&quot;) ## [1] 6 To extract specific parts of a string, the substr() function comes in handy: substr(&quot;Hello!&quot;, start = 1, stop = 4) # Outputs &quot;Hell&quot; ## [1] &quot;Hell&quot; If you need to replace parts of a string, the gsub() function can be used: # Replaces &quot;RStudio&quot; with &quot;R&quot; gsub(pattern = &quot;RStudio&quot;, replacement = &quot;R&quot;, x = &#39;RStudio is fun.&#39;) ## [1] &quot;R is fun.&quot; To convert a string to upper or lower case: toupper(&#39;RStudio is fun.&#39;) # Converts string to upper case ## [1] &quot;RSTUDIO IS FUN.&quot; tolower(&#39;RStudio is fun.&#39;) # Converts string to lower case ## [1] &quot;rstudio is fun.&quot; Finally, to split a string into multiple parts based on a specific character, the strsplit() function is useful: strsplit(&#39;RStudio is fun.&#39;, split = &quot; &quot;) # Splits the string at every space ## [[1]] ## [1] &quot;RStudio&quot; &quot;is&quot; &quot;fun.&quot; 3.2.4 Variables Variables in R can be thought of as name tags that store data. Assign Variables You can assign values to variables using the &lt;- symbol. While you can also use =, the &lt;- symbol is more conventional in R. x &lt;- 10 y &lt;- 5 z &lt;- x + y z ## [1] 15 Display Variables In the code above, after performing the addition, simply writing z on its own line instructs R to print its value to the console. This is a shorthand that is often used in interactive sessions for quickly viewing the content of a variable. An alternative, more explicit way to print a variable’s value is to use the print() function: print(z) ## [1] 15 Both methods will display the value of z in the console, but the print() function can be more versatile, especially when you want to incorporate additional functionality like printing inside a loop. A useful feature in R is the ability to assign a value to a variable and simultaneously print it using parentheses: (x &lt;- 12) # This assigns the value 12 to x and also prints it immediately ## [1] 12 In R, variables can hold various data types, such as numerical, logical, or character. They can also house multiple elements in data structures like vectors, which will be discussed in the subsequent sections. 3.2.5 Data Types Knowing a variable’s data type is crucial in R, as this affects its behavior. For instance, if z is a character, operations like z + 4 fail. R has several data types, and the class() function identifies them: Numeric: These are your usual numbers. They can be decimals, integers, or complex. # Double (decimal number) num_double &lt;- 5.5 class(num_double) ## [1] &quot;numeric&quot; # Integer num_int &lt;- 5L # The L tells R to store 5 as an integer instead of a decimal number. class(num_int) ## [1] &quot;integer&quot; # Complex number num_complex &lt;- 3 + 4i # 3 is the real and 4 is the imaginary part. class(num_complex) ## [1] &quot;complex&quot; Character: These are text or string data types. char &lt;- &quot;Hello, R!&quot; class(char) ## [1] &quot;character&quot; Logical: These represent boolean values, i.e., TRUE or FALSE. log_val &lt;- 5 &gt; 4 log_val class(log_val) ## [1] TRUE ## [1] &quot;logical&quot; Misunderstanding data types can lead to errors, as illustrated below: # A seemingly numeric vector that is, in fact, character-based char &lt;- &quot;5&quot; # Endeavoring to amplify the values culminates in an error mistaken_output &lt;- char * 2 ## Error in char * 2: non-numeric argument to binary operator In R, functions exist for converting one data type into another. The error highlighted in the preceding code chunk underscores the importance of these functions in ensuring operations align with the appropriate data types. as.numeric(): Converts to a numeric data type, useful when reading data where numbers are mistakenly stored as text. as.numeric(&quot;123.456&quot;) ## [1] 123.456 as.character(): Converts to a character data type, useful when saving numeric data as text-based file formats. as.character(123.456) ## [1] &quot;123.456&quot; as.integer(): Converts to an integer type, useful for indexing or when whole numbers are needed for specific functions. as.integer(123.456) ## [1] 123 as.logical(): Converts to a logical data type (i.e., TRUE or FALSE), useful when logical conditions are extracted from textual data sources. as.logical(&quot;TRUE&quot;) ## [1] TRUE These conversion functions are particularly useful when reading in data. Often, data read from external sources (like CSV files) might be imported as character strings, even when they represent numeric values. Converting them to the appropriate type ensures correct data processing. 3.2.6 Data Structures In R, besides working with single data points like a number or a text string, you can also organize and store collections of data points, such as a sequence of numbers or strings. These collections can be stored using vectors, matrices, lists, and data frames. Previously, we delved into the concept of a variable’s data type, distinguishing whether it’s a character, numeric, or logical. While the data type focuses on the kind of data a variable contains, the data structure provides insight into its organization - how many items it holds and how they’re laid out. You can utilize the class() function to determine an item’s structure in a way analogous to identifying its data type. Recognizing a variable’s data structure is crucial as it dictates the available operations and functions for that variable. This section offers a brief introduction to these structures, while Chapter 4 provides a comprehensive exploration of functions specific to each structure. Basic Vectors: A vector is a one-dimensional array that holds elements of the same data type. Think of it as a string of pearls, where each pearl (or data point) is of the same type. # Example of a numeric vector scores &lt;- c(95, 89, 76, 88, 92) print(scores) class(scores) # Returns &quot;numeric&quot;. ## [1] 95 89 76 88 92 ## [1] &quot;numeric&quot; If you try mixing different types in a vector, R ensures uniformity by converting all elements to a common type. mixed_vector &lt;- c(&quot;apple&quot;, 5) print(mixed_vector) # Here, the number 5 becomes the character &quot;5&quot;. class(mixed_vector) # Returns &quot;character&quot;. ## [1] &quot;apple&quot; &quot;5&quot; ## [1] &quot;character&quot; Factors: When a vector represents categorical data (for instance, “male” and “female”), it’s more apt to use a special data type called factor. In contrast to a character vector, factors store categories as integers, optimizing computational efficiency. # Create (unordered) factor representing fruit categories fruits &lt;- c(&quot;apple&quot;, &quot;apple&quot;, &quot;banana&quot;, &quot;apple&quot;, &quot;orange&quot;, &quot;banana&quot;, &quot;apple&quot;) unordered_factor &lt;- factor(x = fruits) print(unordered_factor) class(unordered_factor) # Outputs &quot;factor&quot;. ## [1] apple apple banana apple orange banana apple ## Levels: apple banana orange ## [1] &quot;factor&quot; # Get levels and extract numeric representation of a factor levels(unordered_factor) # Outputs &quot;apple&quot;, &quot;banana&quot;, &quot;orange&quot;. as.numeric(unordered_factor) # Shows the numeric representation of the factor. ## [1] &quot;apple&quot; &quot;banana&quot; &quot;orange&quot; ## [1] 1 1 2 1 3 2 1 Factors can also be ordered, like “low”, “medium”, “high”. This ordered arrangement allows for enhanced logical operations not feasible with unordered factors. # Create ordered factor depicting income levels ordered_factor &lt;- factor(x = c(&quot;low&quot;, &quot;low&quot;, &quot;high&quot;, &quot;medium&quot;, &quot;high&quot;, &quot;low&quot;), levels = c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;), ordered = TRUE) print(ordered_factor) class(ordered_factor) # Outputs &quot;ordered&quot; &quot;factor&quot;. ## [1] low low high medium high low ## Levels: low &lt; medium &lt; high ## [1] &quot;ordered&quot; &quot;factor&quot; # Get levels and extract numeric representation of the ordered factor levels(ordered_factor) # Outputs &quot;low&quot;, &quot;medium&quot;, &quot;high&quot;. as.numeric(ordered_factor) # Shows the numeric representation of the factor. ordered_factor &gt;= &quot;medium&quot; # Performs a logical operation on the factor ## [1] &quot;low&quot; &quot;medium&quot; &quot;high&quot; ## [1] 1 1 3 2 3 1 ## [1] FALSE FALSE TRUE TRUE TRUE FALSE Matrices: A matrix is a two-dimensional array where all the elements are of the same data type. Visualize it as a checkerboard, where every square (or cell) holds data of the same type. # Creating a 3x3 matrix matrix_example &lt;- matrix(c(1,2,3,4,5,6,7,8,9), ncol=3) print(matrix_example) class(matrix_example) # Returns &quot;matrix&quot; &quot;array&quot;. ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 ## [1] &quot;matrix&quot; &quot;array&quot; Lists: A list is an ordered collection that can contain elements of different types. Think of it as a toolbox where you can store tools of various shapes and sizes. # A diverse list shopping_list &lt;- list(&quot;apple&quot;, 3, TRUE, c(4.5, 3.2, 1.1)) print(shopping_list) class(shopping_list) # Returns &quot;list&quot;. ## [[1]] ## [1] &quot;apple&quot; ## ## [[2]] ## [1] 3 ## ## [[3]] ## [1] TRUE ## ## [[4]] ## [1] 4.5 3.2 1.1 ## ## [1] &quot;list&quot; Data Frames: A data frame is a table-like structure in R, where each column can have data of a different type. In finance or economics, envision it as a spreadsheet containing stock prices across various dates. Each column might represent stock prices of different companies, and each row could denote a specific date. # Example of a data frame representing stock prices stock_prices &lt;- data.frame( Date = c(&quot;2023-01-01&quot;, &quot;2023-01-02&quot;, &quot;2023-01-03&quot;), Apple = c(150.10, 151.22, 152.15), Microsoft = c(280.50, 280.10, 281.25), Google = c(2900.20, 2905.50, 2910.00) ) print(stock_prices) class(stock_prices) # Returns &quot;data.frame&quot;. ## Date Apple Microsoft Google ## 1 2023-01-01 150.10 280.50 2900.2 ## 2 2023-01-02 151.22 280.10 2905.5 ## 3 2023-01-03 152.15 281.25 2910.0 ## [1] &quot;data.frame&quot; When analyzing datasets in R, it’s essential to ascertain the data structure you’re dealing with. By leveraging the right structure for the task at hand, you can harness R’s capabilities more effectively and streamline your data analysis process. The following section delves into essential functions for working with vectors. Later, Chapter 4 provides a comprehensive overview of functions associated with the other data structures. 3.2.7 Vector Operations Vectors are one of the core data structures in R, designed to hold multiple elements of a single data type, be it numeric, logical, or character. Below is a detailed exploration of the creation, manipulation, and utility functions associated with vectors: Create a Vector: Use the c() function, an abbreviation for “concatenate”, to create vectors. numeric_vector &lt;- c(5, 2, 3, 4, 1) logical_vector &lt;- c(TRUE, FALSE, TRUE, FALSE) character_vector &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;cherry&quot;) The c() function may also include vectors as input. meta_vector &lt;- c(numeric_vector, 11, 12, 13) print(meta_vector) ## [1] 5 2 3 4 1 11 12 13 Employ the factor() function to create vectors with the data type factor. unordered_factor &lt;- factor(x = c(&quot;male&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;), levels = c(&quot;male&quot;, &quot;female&quot;, &quot;other&quot;), ordered = FALSE) ordered_factor &lt;- factor(x = c(&quot;L&quot;, &quot;L&quot;, &quot;H&quot;, &quot;L&quot;, &quot;H&quot;, &quot;H&quot;, &quot;M&quot;, &quot;M&quot;, &quot;H&quot;), levels = c(&quot;L&quot;, &quot;M&quot;, &quot;H&quot;), ordered = TRUE) Label Vector Elements: Vectors in R can be equipped with names for each element. Naming elements during vector creation: named_vector &lt;- c(John = 23, Sarah = 21, Mike = 25) print(named_vector) ## John Sarah Mike ## 23 21 25 Extracting names of a vector using the names() function: names(named_vector) ## [1] &quot;John&quot; &quot;Sarah&quot; &quot;Mike&quot; Adding names to an existing vector using the names() function: named_vector &lt;- c(23, 21, 25) names(named_vector) &lt;- c(&quot;John&quot;, &quot;Sarah&quot;, &quot;Mike&quot;) print(named_vector) ## John Sarah Mike ## 23 21 25 Subset a Vector: To subset vectors in R, utilize square brackets [...]. character_vector[2] # Fetches the second item, i.e., &quot;banana&quot; numeric_vector[c(2, 5)] # Fetches the second and fifth items named_vector[c(&quot;John&quot;, &quot;Sarah&quot;)] # Fetches the elements named John and Sarah ## [1] &quot;banana&quot; ## [1] 2 1 ## John Sarah ## 23 21 The functions head() and tail() are used to obtain the initial and concluding n elements of a vector, respectively. By default, n = 6. head(character_vector, n = 3) # Fetches the first three items tail(numeric_vector, n = 1) # Fetches the final item head(tail(numeric_vector, n = 3), n = 1) # Fetches the third-last item ## [1] &quot;apple&quot; &quot;banana&quot; &quot;cherry&quot; ## [1] 1 ## [1] 3 Replace Vector Elements: To modify specific elements within vectors, square brackets are used for identification. numeric_vector[5] &lt;- -99 # Replaces the 5th element numeric_vector named_vector[c(&quot;John&quot;, &quot;Mike&quot;)] &lt;- c(23.5, 25.1) # Replaces the named elements named_vector ## [1] 5 2 3 4 -99 ## John Sarah Mike ## 23.5 21.0 25.1 Count Vector Elements: The length() function returns the number of elements in a vector. length(numeric_vector) # Outputs: 5 length(logical_vector) # Outputs: 4 length(character_vector) # Outputs: 3 ## [1] 5 ## [1] 4 ## [1] 3 The table() function counts the unique values in a vector. This is especially beneficial for categorical data or discrete numerical values. categories &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;C&quot;, &quot;B&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;A&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;) table(categories) # Shows the frequency of each category ## categories ## A B C ## 3 2 8 Expanding on this with a politics example, table() can be applied to two or more vectors to understand their relationships with a contingency table. Here, one might want to see how many countries of each income status are currently at war or at peace: country_income &lt;- c(&quot;High&quot;, &quot;Middle&quot;, &quot;Low&quot;, &quot;Middle&quot;, &quot;High&quot;, &quot;Middle&quot;, &quot;Low&quot;, &quot;Low&quot;, &quot;Middle&quot;, &quot;High&quot;, &quot;Low&quot;, &quot;Middle&quot;) war_status &lt;- c(&quot;Peace&quot;, &quot;Peace&quot;, &quot;War&quot;, &quot;Peace&quot;, &quot;Peace&quot;, &quot;Peace&quot;, &quot;Peace&quot;, &quot;War&quot;, &quot;Peace&quot;, &quot;War&quot;, &quot;Peace&quot;, &quot;Peace&quot;) table(country_income, war_status) ## war_status ## country_income Peace War ## High 2 1 ## Low 2 2 ## Middle 5 0 Generate Sequences and Repetitions: seq() and seq_along(): Creates sequences. 1:5 # Generates: 1, 2, 3, 4, 5 seq(1, 5) # Same as 1:5 seq(1, 10, by = 2) # Generates: 1, 3, 5, 7, 9 seq_along(character_vector) # Same as 1:length(character_vector) ## [1] 1 2 3 4 5 ## [1] 1 2 3 4 5 ## [1] 1 3 5 7 9 ## [1] 1 2 3 rep() and rep_len(): Allows for repetition. rep(4, times = 3) # Outputs: 4, 4, 4 rep(c(1, 2), times = 2) # Outputs: 1, 2, 1, 2 rep(c(1, 2), each = 2) # Outputs: 1, 1, 2, 2 rep_len(c(1, 2), length.out = 5) # Generates: 1, 2, 1, 2, 1 ## [1] 4 4 4 ## [1] 1 2 1 2 ## [1] 1 1 2 2 ## [1] 1 2 1 2 1 sample(): Random sampling of values. sample(1:5) # Returns 1, 2, 3, 4, 5 in a random order sample(1:5, 3) # Sample only three values, e.g., 4, 1, 2 sample(1:5, 3, replace = TRUE) # Sample with replacement, e.g., 1, 1, 2 sample(x = c(&quot;Head&quot;, &quot;Tail&quot;), # Sample over Head and Tail size = 20, # Total number of draws replace = TRUE, # Draw with replacement prob = c(0.80, 0.20)) # Assign probabilities to Head and Tail ## [1] 2 3 4 5 1 ## [1] 1 2 5 ## [1] 3 2 5 ## [1] &quot;Tail&quot; &quot;Head&quot; &quot;Tail&quot; &quot;Head&quot; &quot;Tail&quot; &quot;Head&quot; &quot;Head&quot; &quot;Head&quot; &quot;Head&quot; &quot;Head&quot; ## [11] &quot;Head&quot; &quot;Tail&quot; &quot;Head&quot; &quot;Head&quot; &quot;Head&quot; &quot;Head&quot; &quot;Head&quot; &quot;Head&quot; &quot;Head&quot; &quot;Tail&quot; Element-Wise Operations: In R, when a function targets a vector, it generally acts on each element individually. This property allows for faster computations without having to write loops. numeric_vector &lt;- c(5, 2, 3, 4, 1) numeric_vector - 10 # Subtract 10 from each element numeric_vector / 2 # Divide each element by 2 numeric_vector^2 # Square each element of the vector ## [1] -5 -8 -7 -6 -9 ## [1] 2.5 1.0 1.5 2.0 0.5 ## [1] 25 4 9 16 1 When two vectors are of equal length, applying a function to both usually operates on corresponding elements. That is, the function is applied to the first elements of both vectors, then the second elements, and so on. numeric_vector_1 &lt;- c(5, 2, 3, 4, 1) numeric_vector_2 &lt;- c(8, 1, 3, 5, 2) numeric_vector_1 - numeric_vector_2 # Element-wise subtraction numeric_vector_1^numeric_vector_2 # Element-wise power ## [1] -3 1 0 -1 -1 ## [1] 390625 2 27 1024 1 For logical vectors, similar element-wise operations can be executed: logical_vector_1 &lt;- c(TRUE, TRUE, TRUE, FALSE) logical_vector_2 &lt;- c(TRUE, FALSE, FALSE, FALSE) !logical_vector_1 # Negate each element logical_vector_1 == logical_vector_2 # Element-wise equality check logical_vector_1 &amp; logical_vector_2 # Element-wise AND logical_vector_1 | logical_vector_2 # Element-wise OR xor(logical_vector_1, logical_vector_2) # Element-wise XOR ## [1] FALSE FALSE FALSE TRUE ## [1] TRUE FALSE FALSE TRUE ## [1] TRUE FALSE FALSE FALSE ## [1] TRUE TRUE TRUE FALSE ## [1] FALSE TRUE TRUE FALSE Set Operations: With vectors, various set operations can be executed, like determining union, intersection, and differences. Here are some common set operations with vectors: union(x, y): Combines the unique values of two vectors x and y. union(c(1, 2, 3, 4), c(3, 4, 5, 6)) # Outputs: 1, 2, 3, 4, 5, 6 ## [1] 1 2 3 4 5 6 intersect(x, y): Determines common values between two vectors x and y. intersect(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;), c(&quot;c&quot;, &quot;d&quot;, &quot;e&quot;, &quot;f&quot;)) # Outputs: &quot;c&quot;, &quot;d&quot; ## [1] &quot;c&quot; &quot;d&quot; setdiff(x, y): Returns the values in vector x that aren’t in vector y. setdiff(c(1, 2, 3, 4), c(3, 4, 5, 6)) # Outputs: 1, 2 ## [1] 1 2 setequal(x, y): Checks if two vectors contain the same elements. setequal(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;), c(&quot;d&quot;, &quot;c&quot;, &quot;b&quot;, &quot;a&quot;)) # Outputs: TRUE ## [1] TRUE is.element(el, set): Determines if elements in el are in set. is.element(c(1, 5, 12), 1:10) # Outputs: TRUE, TRUE, FALSE ## [1] TRUE TRUE FALSE Conditional Operations: The ifelse() function facilitates vectorized conditional checks. numeric_vector ifelse(test = numeric_vector &gt; 3, yes = &quot;Above 3&quot;, no = &quot;3 or Below&quot;) ## [1] 5 2 3 4 1 ## [1] &quot;Above 3&quot; &quot;3 or Below&quot; &quot;3 or Below&quot; &quot;Above 3&quot; &quot;3 or Below&quot; In ifelse(), the test parameter evaluates a condition or a vector of conditions. The yes parameter defines the output when test is TRUE, and the no parameter specifies the output when test is FALSE. Functions for Numerical Vectors: Numerical vectors hold quantitative values and are foundational for most statistical and mathematical operations in R. The following functions are optimized for manipulating and analyzing numerical vectors: sum(), prod(), max(), min(), and mean(): Computes basic statistics. sum(numeric_vector) # Sum of all elements: 15 prod(numeric_vector) # Product of all elements: 120 max(numeric_vector) # Maximum value: 5 min(numeric_vector) # Minimum value: 1 mean(numeric_vector) # Average value: 3 ## [1] 15 ## [1] 120 ## [1] 5 ## [1] 1 ## [1] 3 cumsum(), cumprod(), cummax(), and cummin(): Calculate cumulative statistics. For example, in a sequence of numbers representing daily sales, the cumulative sum cumsum() would show the total sales up to each day, while the cumulative product cumprod() might represent the compounded growth of an investment for each day. cumsum(numeric_vector) # Cumulative sum cumprod(numeric_vector) # Cumulative product cummax(numeric_vector) # Cumulative maximum cummin(numeric_vector) # Cumulative minimum cumsum(numeric_vector) / seq_along(numeric_vector) # Cumulative mean ## [1] 5 7 10 14 15 ## [1] 5 10 30 120 120 ## [1] 5 5 5 5 5 ## [1] 5 2 2 2 1 ## [1] 5.000000 3.500000 3.333333 3.500000 3.000000 sort(): Sorts the vector. numeric_vector sort(numeric_vector) # Ascending order by default ## [1] 5 2 3 4 1 ## [1] 1 2 3 4 5 order(): Provides sorting indices. numeric_vector order(numeric_vector) # Indices for sorting ## [1] 5 2 3 4 1 ## [1] 5 2 3 4 1 The order() function is particularly useful when dealing with multiple related vectors (or columns in a data frame). # Define related vectors grades &lt;- c(90, 85, 88, 75) names &lt;- c(&quot;Alice&quot;, &quot;Bob&quot;, &quot;Charlie&quot;, &quot;David&quot;) # Get the indices to order grades in ascending order order_indices &lt;- order(grades) order_indices # Arrange names based on grades&#39; order ordered_names &lt;- names[order_indices] ordered_names ## [1] 4 2 3 1 ## [1] &quot;David&quot; &quot;Bob&quot; &quot;Charlie&quot; &quot;Alice&quot; Functions for Logical Vectors: Logical vectors hold boolean values, which are either TRUE or FALSE. They often result from comparisons and are fundamental in R for tasks such as subsetting data based on conditions. The following functions are tailored for working with logical vectors: any() and all(): Test if at least one or all of the elements in a logical vector are TRUE, respectively. logical_vector any(logical_vector) # Returns TRUE if any value is TRUE all(logical_vector) # Returns TRUE only if all values are TRUE ## [1] TRUE FALSE TRUE FALSE ## [1] TRUE ## [1] FALSE which(): Identifies the indices of TRUE values in a logical vector. Useful for subsetting based on conditions. logical_vector which(logical_vector) # Outputs positions of TRUE values ## [1] TRUE FALSE TRUE FALSE ## [1] 1 3 Functions for Character Vectors: When working with textual data in R, specialized functions become indispensable. These tools facilitate tasks such as transforming, searching, and manipulating character vectors: toupper() and tolower(): Convert text elements to uppercase or lowercase, respectively. character_vector toupper(character_vector) # Converts to uppercase tolower(character_vector) # Converts to lowercase ## [1] &quot;apple&quot; &quot;banana&quot; &quot;cherry&quot; ## [1] &quot;APPLE&quot; &quot;BANANA&quot; &quot;CHERRY&quot; ## [1] &quot;apple&quot; &quot;banana&quot; &quot;cherry&quot; substr(): Extracts specific portions of strings based on given start and stop positions. character_vector substr(character_vector, start = 1, stop = 3) # Extract first 3 characters ## [1] &quot;apple&quot; &quot;banana&quot; &quot;cherry&quot; ## [1] &quot;app&quot; &quot;ban&quot; &quot;che&quot; grep(): Searches for patterns within character vectors, returning either the indices or the matched values. character_vector grep(pattern = &quot;apple&quot;, x = character_vector) # Index of matches grep(pattern = &quot;apple&quot;, x = character_vector, value = TRUE) # Matched values ## [1] &quot;apple&quot; &quot;banana&quot; &quot;cherry&quot; ## [1] 1 ## [1] &quot;apple&quot; gsub(): Searches for and replaces all instances of a pattern within a character vector. character_vector gsub(pattern = &quot;a&quot;, replacement = &quot;-A-&quot;, x = character_vector, ignore.case = TRUE) ## [1] &quot;apple&quot; &quot;banana&quot; &quot;cherry&quot; ## [1] &quot;-A-pple&quot; &quot;b-A-n-A-n-A-&quot; &quot;cherry&quot; nchar(): Calculates the number of characters in each element of a character vector. character_vector nchar(character_vector) # Outputs number of characters for each element ## [1] &quot;apple&quot; &quot;banana&quot; &quot;cherry&quot; ## [1] 5 6 6 strsplit(): Dissects elements of a character vector using a specified delimiter. character_vector strsplit(character_vector, split = &quot;a&quot;) # Splits the sentence at each &quot;a&quot; ## [1] &quot;apple&quot; &quot;banana&quot; &quot;cherry&quot; ## [[1]] ## [1] &quot;&quot; &quot;pple&quot; ## ## [[2]] ## [1] &quot;b&quot; &quot;n&quot; &quot;n&quot; ## ## [[3]] ## [1] &quot;cherry&quot; paste() and paste0(): Merges two or more character vectors element-wise. first_names &lt;- c(&quot;John&quot;, &quot;Sarah&quot;) last_names &lt;- c(&quot;Doe&quot;, &quot;Connor&quot;) paste(first_names, last_names) # Merges with a space paste0(first_names, last_names) # Merges without a space paste(last_names, first_names, sep = &quot;, &quot;) # Merges with a comma ## [1] &quot;John Doe&quot; &quot;Sarah Connor&quot; ## [1] &quot;JohnDoe&quot; &quot;SarahConnor&quot; ## [1] &quot;Doe, John&quot; &quot;Connor, Sarah&quot; paste(..., collapse): Combines multiple elements of a character vector into a single string. The collapse parameter defines the character used to link individual strings. character_vector paste(character_vector, collapse = &quot; and &quot;) ## [1] &quot;apple&quot; &quot;banana&quot; &quot;cherry&quot; ## [1] &quot;apple and banana and cherry&quot; Functions for Factors: Factors in R are designed to handle categorical data efficiently. They store data as integers but maintain a separate set of character values (levels) that correspond to these integers. Various functions in R facilitate the creation, inspection, and manipulation of factors: factor(): Converts a character vector into a factor. factor(character_vector) ## [1] apple banana cherry ## Levels: apple banana cherry ordered(): Converts an unordered factor into an ordered factor. ordered(unordered_factor) ## [1] male male female male female ## Levels: male &lt; female levels(): Retrieves the levels of a factor and can also be used to modify them. levels(unordered_factor) levels(ordered_factor) levels(ordered_factor) &lt;- c(&quot;Low&quot;, &quot;Medium&quot;, &quot;High&quot;) print(ordered_factor) ## [1] &quot;male&quot; &quot;female&quot; &quot;other&quot; ## [1] &quot;L&quot; &quot;M&quot; &quot;H&quot; ## [1] Low Low High Low High High Medium Medium High ## Levels: Low &lt; Medium &lt; High as.numeric(): Converts factor levels to their integer representation. as.numeric(unordered_factor) as.numeric(ordered_factor) ## [1] 1 1 2 1 2 ## [1] 1 1 3 1 3 3 2 2 3 relevel(): Adjusts the reference level for an unordered factor, especially useful in regression modeling. relevel(unordered_factor, ref = &quot;female&quot;) ## [1] male male female male female ## Levels: female male other table(): Generates a frequency table of a factor. table(unordered_factor) table(ordered_factor) ## unordered_factor ## male female other ## 3 2 0 ## ordered_factor ## Low Medium High ## 3 2 4 Comparison: Ordered factors allow for relational comparisons. ordered_factor &gt;= &quot;Medium&quot; sort(ordered_factor) order(ordered_factor) ## [1] FALSE FALSE TRUE FALSE TRUE TRUE TRUE TRUE TRUE ## [1] Low Low Low Medium Medium High High High High ## Levels: Low &lt; Medium &lt; High ## [1] 1 2 4 7 8 3 5 6 9 Having a grasp over these vector operations will greatly assist in data manipulation and analytics in R. 3.2.8 Missing Values Handling missing values is a foundational step in data analysis. In R, missing data is denoted by the symbol NA. Recognizing and effectively managing these missing values is key to ensuring reliable and robust analyses. Assign Missing Values: Intentionally include missing values to a vector, simulating missing data points. vec_na &lt;- c(1, NA, 3, NA) print(vec_na) ## [1] 1 NA 3 NA Identify Missing Values: Use the is.na() and anyNA() functions to determine if a value or a set of values in a vector is missing. vec_na is.na(vec_na) # Returns: FALSE, TRUE, FALSE, TRUE anyNA(vec_na) # Returns: TRUE, because there is at least one missing ## [1] 1 NA 3 NA ## [1] FALSE TRUE FALSE TRUE ## [1] TRUE Count Missing Values: To gauge the extent of missing data in your dataset, count the number of missing values. Use the sum() function in tandem with is.na(). vec_na sum(is.na(vec_na)) # Returns 2, indicating two missing values ## [1] 1 NA 3 NA ## [1] 2 Remove Missing Values: You might opt to eliminate missing values. The na.omit() function can be used for this. vec_na na.omit(vec_na) # Removes all NA values ## [1] 1 NA 3 NA ## [1] 1 3 ## attr(,&quot;na.action&quot;) ## [1] 2 4 ## attr(,&quot;class&quot;) ## [1] &quot;omit&quot; The output shows the vector without NA values; the “na.action” and “class” attributes respectively mark the positions of the omitted NAs and classify the action as “omit”. Replace Missing Values: There could be situations where you want to substitute missing values with a specific value, such as the mean or median. The ifelse() function combined with is.na() provides a solution. vec_na ifelse(is.na(vec_na), 0, vec_na) # Replace NA with 0 ## [1] 1 NA 3 NA ## [1] 1 0 3 0 Prevent Operations on Missing Values: Operations on vectors with NA values can produce an NA result. If you wish to exclude NA values during such calculations, several functions include an na.rm argument. # Compute mean including NA values vec_na mean(vec_na) # Returns NA # Compute mean excluding NA values mean(vec_na, na.rm = TRUE) # Returns a number ## [1] 1 NA 3 NA ## [1] NA ## [1] 2 Compare NA values: NA values can be tricky during comparisons. NA == NA # Returns NA is.na(NA) # Returns TRUE ## [1] NA ## [1] TRUE NA == NA returns NA since both values are unknown, we can’t definitively say whether they are equal. is.na(NA) returns TRUE because is.na() is specifically designed to identify missing values in R, returning TRUE for each element that is NA and FALSE otherwise. Moreover, during logical operations: c(NA, NA, NA) &amp; c(NA, TRUE, FALSE) # Returns NA, NA, FALSE c(NA, NA, NA) | c(NA, TRUE, FALSE) # Returns NA, TRUE, NA ## [1] NA NA FALSE ## [1] NA TRUE NA NA &amp; TRUE yields NA. The outcome remains uncertain due to the presence of the unknown (NA) value. If NA is true, then the result is true; but if NA is false, the result is false. Given the ambiguity, R returns NA. NA &amp; FALSE gives FALSE. Regardless of the value of NA, the definite FALSE value ensures the entire expression evaluates to false. NA | TRUE results in TRUE. The definite TRUE value guarantees the expression’s truthfulness irrespective of the NA’s value. NA | FALSE produces NA. The result hinges on the value of NA: if it’s true, the expression is true; if it’s false, it’s false. With the value of NA indeterminate, R returns NA. NA &amp; NA and NA | NA always equate to NA since both values are undetermined. It’s crucial to handle missing values appropriately, as they can influence the conclusions of your studies. 3.2.9 Attributes In R, attributes serve as metadata that provide additional information about an object. These attributes can describe the dimensions of an array, the names of variables, or the class of an object, among other things. Additionally, attributes can be leveraged to store custom metadata like data sources, labels, or units. Access Attributes The attributes() function provides an overview of all attributes of an object: # Define a data object (ordered factor) income_levels &lt;- factor(x = c(Alice = &quot;low&quot;, Bob = &quot;low&quot;, Chris = &quot;high&quot;, David = &quot;medium&quot;, Emily = &quot;high&quot;, Frank = &quot;low&quot;), levels = c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;), ordered = TRUE) # View attributes attributes(income_levels) ## $names ## [1] &quot;Alice&quot; &quot;Bob&quot; &quot;Chris&quot; &quot;David&quot; &quot;Emily&quot; &quot;Frank&quot; ## ## $levels ## [1] &quot;low&quot; &quot;medium&quot; &quot;high&quot; ## ## $class ## [1] &quot;ordered&quot; &quot;factor&quot; This outputs a list of attributes, including element names (“Alice”, “Bob”, etc.), factor levels (“low”, “medium”, “high”), and the object’s class (“ordered”, “factor”). Add Custom Attributes Custom attributes can be added using the attr() function: # Add a custom attribute to specify the data source attr(income_levels, &quot;source&quot;) &lt;- &quot;Generated for demonstration&quot; # View the custom attribute attr(income_levels, &quot;source&quot;) ## [1] &quot;Generated for demonstration&quot; This will show “Generated for demonstration”, indicating the source of the data. Leverage Attributes in Data Analysis When dealing with financial or economic data, attributes can be particularly useful. For instance, when downloading stock price data, you can use attributes to store information about the data source, time span covered, or the API key used. # Assume stock_data is acquired from Yahoo Finance from 1990-2020 attr(stock_data, &quot;source&quot;) &lt;- &quot;Yahoo Finance&quot; attr(stock_data, &quot;time_span&quot;) &lt;- &quot;1990-2020&quot; By doing so, anyone examining this object later can quickly understand its context without having to consult external documentation. Caveats While attributes are useful, they are not always preserved when an object is modified. Some operations and functions might strip away the attributes, so always verify their presence after significant data manipulation tasks. In summary, attributes in R serve as a versatile tool for both describing and augmenting data objects, aiding both in analysis and interpretability. 3.3 Dates and Times In Economics and Finance, handling date and time data efficiently is essential. R offers specialized data types and functions tailored for this purpose. Two primary representations are the POSIXct class for time and the Date class for dates. 3.3.1 Time Class (POSIXct) In R, date-time values are typically stored using POSIXct. POSIXct represents date-time values in a compact way, specifically using Unix time (also known as Epoch time or POSIX time). Unix time counts the seconds elapsed since the Unix Epoch (00:00:00 UTC on January 1, 1970), not accounting for leap seconds. This numeric representation is beneficial for arithmetic operations as it treats date-time values as simple numbers. As an alternative, POSIXlt (as opposed to POSIXct) is a more detailed format that represents date-time as a list with components like year, month, day, hour, etc. To convert a human-readable date-time string like YYYY-MM-DD HH:MM:SS into POSIXct format, use the as.POSIXct() function. # Convert a character vector to POSIXct character_vector &lt;- c(&quot;2023-06-30 18:47:10&quot;, &quot;2023-06-30 19:13:54&quot;) time_vector &lt;- as.POSIXct(x = character_vector, tz = &quot;America/Chicago&quot;) class(time_vector) # Outputs &quot;POSIXct&quot; print(time_vector) # Shows time in YYYY-MM-DD HH:MM:SS format ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; ## [1] &quot;2023-06-30 18:47:10 CDT&quot; &quot;2023-06-30 19:13:54 CDT&quot; In the pattern above, %H is the 24-hour format, %M is the minute, and %S is the second. The tz parameter specifies the time zone— “America/Chicago” is for Central Daylight Time. Modify the tz parameter for your specific time zone. For a complete list of R’s time zones, refer to the OlsonNames() function. To see the Unix time representation of a POSIXct object, use as.numeric(). # Show POSIXct object as Unix time (numeric) unix_vector &lt;- as.numeric(time_vector) class(unix_vector) # Outputs &quot;numeric&quot; print(unix_vector) # Shows seconds since 1970-01-01 00:00:00 UTC diff(unix_vector) # Shows number of seconds between dates diff(unix_vector) / 60 # Shows number of minutes between dates ## [1] &quot;numeric&quot; ## [1] 1688168830 1688170434 ## [1] 1604 ## [1] 26.73333 To revert Unix time back to POSIXct, utilize the as.POSIXct() function. # Revert Unix time to POSIXct, keeping the initial time zone reconverted_time_vector &lt;- as.POSIXct(x = unix_vector, origin = &quot;1970-01-01&quot;, tz = &quot;America/Chicago&quot;) identical(time_vector, reconverted_time_vector) # Checks if identical ## [1] TRUE In the above code, the origin argument is set to “1970-01-01”, which is the starting point for Unix time, and tz sets the time zone. 3.3.2 Date Class (Date) The Date class in R represents dates without time components. Internally, R stores Date objects as the number of days since January 1, 1970 (the Unix Epoch). This is analogous to how POSIXct represents date-time values as Unix time, counting the number of seconds instead of days. Using a numeric representation for dates simplifies arithmetic operations, such as finding the number of days between two dates. The as.Date() function is used to convert a character string, formatted as YYYY-MM-DD, into a Date object: # Create Date vector from character strings character_vector &lt;- c(&quot;2023-06-30&quot;, &quot;2023-07-02&quot;) date_vector &lt;- as.Date(character_vector) class(date_vector) # Returns &quot;Date&quot;. print(date_vector) # Outputs date in YYYY-MM-DD format ## [1] &quot;Date&quot; ## [1] &quot;2023-06-30&quot; &quot;2023-07-02&quot; In this format, %Y represents a four-digit year, %m indicates a two-digit month, and %d stands for a two-digit day. If you’re interested in viewing the underlying numeric representation of a Date object, you can do the following: # Convert Date object to its numeric representation date_numeric &lt;- as.numeric(date_vector) class(date_numeric) # Returns &quot;numeric&quot;. print(date_numeric) # Outputs number of days since 1970-01-01 diff(date_numeric) # Calculates the difference in days between dates diff(date_numeric) / 7 # Calculates the difference in weeks between dates ## [1] &quot;numeric&quot; ## [1] 19538 19540 ## [1] 2 ## [1] 0.2857143 To revert a numeric representation back to the Date format, use the as.Date() function. Ensure you specify the origin as 1970-01-01: # Convert numeric representation back to Date format reconverted_date_vector &lt;- as.Date(date_numeric, origin = &quot;1970-01-01&quot;) identical(date_vector, reconverted_date_vector) # Checks if identical ## [1] TRUE By understanding the internal numeric representation of Date objects, users can perform a range of date-based calculations and manipulations efficiently. 3.3.3 Custom Date and Time Formats There might be cases where the date and time are represented differently in your data. R allows customization using the format parameter. For example, consider a date string: “April 6 – 23”. It can be parsed into a date using a custom format: # Convert any date string into a date variable string_april_6_23 &lt;- &quot;April 6 -- 23&quot; date_april_6_23 &lt;- as.Date(string_april_6_23, format = &quot;%B %d -- %y&quot;) class(date_april_6_23) print(date_april_6_23) ## [1] &quot;Date&quot; ## [1] &quot;2023-04-06&quot; Once a variable is stored as a Date or POSIXct class, you can employ the format() function to display date or time variables in desired formats: # Change the way a date variable is displayed format(date_april_6_23, format = &quot;%b %d, %Y&quot;) ## [1] &quot;Apr 06, 2023&quot; Additionally, the format() function can be used to extract specific components from a date or time object: time &lt;- as.POSIXct(&quot;2023-06-30 18:47:10&quot;) year &lt;- format(time, format = &quot;%Y&quot;) month &lt;- format(time, format = &quot;%m&quot;) day &lt;- format(time, format = &quot;%d&quot;) hour &lt;- format(time, format = &quot;%H&quot;) minute &lt;- format(time, format = &quot;%M&quot;) second &lt;- format(time, format = &quot;%S&quot;) c(year, month, day, hour, minute, second) ## [1] &quot;2023&quot; &quot;06&quot; &quot;30&quot; &quot;18&quot; &quot;47&quot; &quot;10&quot; For a detailed list of format specifications, use the ?strptime command in the R console. This will provide you with various syntax options for different date and time formats. The most commonly used formats are outlined in the table below: Table 3.1: Syntax for Date Format Specification Description Example %a Abbreviated weekday Sun, Thu %A Full weekday Sunday, Thursday %b or %h Abbreviated month May, Jul %B Full month May, July %d Day of the month, 0-31 27, 07 %j Day of the year, 001-366 148, 188 %m Month, 01-12 05, 07 %U Week, 01-53, with Sunday as first day of the week 22, 27 %w Weekday, 0-6, Sunday is 0 0, 4 %W Week, 00-53, with Monday as first day of the week 21, 27 %x Date, locale-specific %y Year without century, 00-99 84, 05 %Y Year with century, on input: 00 to 68 prefixed by 20, 69 to 99 prefixed by 19 1984, 2005 %C Century 19, 20 %D Date formatted %m/%d/%y 5/27/84 %u Weekday, 1-7, Monday is 1 7, 4 %n Newline on output or arbitrary whitespace on input %t Tab on output or arbitrary whitespace on input 3.3.4 Operations with Dates and Times Date and POSIXct objects support various arithmetic operations: # Define example dates date1 &lt;- as.Date(&quot;2023-06-30&quot;) date2 &lt;- as.Date(&quot;2023-01-01&quot;) # Calculate the difference between two dates date1 - date2 ## Time difference of 180 days # Add a certain number of days to a date date1 + 30 ## [1] &quot;2023-07-30&quot; Using the difftime function, you can calculate differences in various units: # Define example dates date1 &lt;- as.Date(&quot;2023-01-01&quot;) date2 &lt;- as.Date(&quot;2023-06-30&quot;) # Difference in seconds, minutes, hours, days, and weeks difftime(date2, date1, units = &quot;secs&quot;) difftime(date2, date1, units = &quot;mins&quot;) difftime(date2, date1, units = &quot;hours&quot;) difftime(date2, date1, units = &quot;days&quot;) difftime(date2, date1, units = &quot;weeks&quot;) # Difference in months and years have to be computed by hand as.numeric(difftime(date2, date1, units = &quot;weeks&quot;) / 4.33) # months as.numeric(difftime(date2, date1, units = &quot;days&quot;) / 365.25) # years ## Time difference of 15552000 secs ## Time difference of 259200 mins ## Time difference of 4320 hours ## Time difference of 180 days ## Time difference of 25.71429 weeks ## [1] 5.938634 ## [1] 0.4928131 You can compare and sort dates: # Define example dates date1 &lt;- as.Date(&quot;2023-07-01&quot;) date2 &lt;- as.Date(&quot;2023-06-15&quot;) # Compare dates date1 &lt; date2 # FALSE because date1 is later than date2 date1 == date2 # FALSE because the dates are different # Sort dates sort(c(date1, date2)) ## [1] FALSE ## [1] FALSE ## [1] &quot;2023-06-15&quot; &quot;2023-07-01&quot; Here’s how to generate a date sequence: # Generate a sequence of dates using 2-day intervals start_date &lt;- as.Date(&quot;2023-06-01&quot;) end_date &lt;- as.Date(&quot;2023-06-21&quot;) sequence_of_dates &lt;- seq(start_date, end_date, by = &quot;2 days&quot;) sequence_of_dates ## [1] &quot;2023-06-01&quot; &quot;2023-06-03&quot; &quot;2023-06-05&quot; &quot;2023-06-07&quot; &quot;2023-06-09&quot; ## [6] &quot;2023-06-11&quot; &quot;2023-06-13&quot; &quot;2023-06-15&quot; &quot;2023-06-17&quot; &quot;2023-06-19&quot; ## [11] &quot;2023-06-21&quot; You can convert time data between different time zones: # Convert time date between time zones time_cdt &lt;- as.POSIXct(&quot;2023-06-30 18:47:10&quot;, tz = &quot;America/Chicago&quot;) time_utc &lt;- time_cdt attr(time_utc, &quot;tzone&quot;) &lt;- &quot;UTC&quot; print(time_cdt) print(time_utc) time_cdt - time_utc # Returns 0 secs. identical(as.numeric(time_cdt), as.numeric(time_utc)) # Identical Unix time. ## [1] &quot;2023-06-30 18:47:10 CDT&quot; ## [1] &quot;2023-06-30 23:47:10 UTC&quot; ## Time difference of 0 secs ## [1] TRUE Modifying the time zone attribute of a POSIXct object doesn’t alter its underlying Unix timestamp. Instead, it reinterprets the same timestamp: “Show me what time it was in the UTC timezone when it was ‘2023-06-30 18:47:10’ in the ‘America/Chicago’ timezone.” If the original time zone was mistakenly set, and you wish to correct it: # Assuming the time was incorrectly set as &quot;America/Chicago&quot; incorrect_time_cdt &lt;- as.POSIXct(&quot;2023-06-30 18:47:10&quot;, tz = &quot;America/Chicago&quot;) # Convert to character, removing the time zone time_str &lt;- format(incorrect_time_cdt, format = &quot;%Y-%m-%d %H:%M:%S&quot;) # Recast with the correct time zone &quot;UTC&quot; correct_time_utc &lt;- as.POSIXct(time_str, tz = &quot;UTC&quot;) # Print the values print(incorrect_time_cdt) print(correct_time_utc) incorrect_time_cdt - correct_time_utc identical(as.numeric(incorrect_time_cdt), as.numeric(correct_time_utc)) ## [1] &quot;2023-06-30 18:47:10 CDT&quot; ## [1] &quot;2023-06-30 18:47:10 UTC&quot; ## Time difference of 5 hours ## [1] FALSE Now, correct_time_utc represents “2023-06-30 18:47:10” in the UTC time zone, but with an adjusted Unix timestamp. The difference between incorrect_time_cdt and correct_time_utc gives the actual time difference between the two time zones. In conclusion, understanding and employing Date and POSIXct operations in R is crucial for precise manipulation and interpretation of date and time data. 3.3.5 Current Date and Time In R, you can retrieve the current system’s date, time, and time zone. The Sys.Date() function provides the current date based on the system’s setting: today &lt;- Sys.Date() class(today) print(today) format(today, format = &quot;The latest update to this book was on %B %d, %Y.&quot;) ## [1] &quot;Date&quot; ## [1] &quot;2024-05-31&quot; ## [1] &quot;The latest update to this book was on May 31, 2024.&quot; To get the current date and time down to the second, you can use the Sys.time() function: current_time &lt;- Sys.time() class(current_time) print(current_time) format(current_time, format = &quot;The update took place at %I:%M %p (%Z time zone).&quot;) ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; ## [1] &quot;2024-05-31 14:04:20 CDT&quot; ## [1] &quot;The update took place at 02:04 PM (CDT time zone).&quot; To determine the time zone set on the system, you can use the Sys.timezone() function: Sys.timezone() ## [1] &quot;America/Chicago&quot; By leveraging these functions, you can easily fetch the current date, time, and time zone settings of your system within R. 3.4 Graphs A graph is a visual representation that displays data points on a coordinate system, typically visualizing the outcomes of a single variable, or the relationship between two or more variables. 3.4.1 Scatter Plot A scatter plot displays points based on two sets of values. Each point on the plot represents a pair of values, one from each set. Imagine you have data on five different countries. For each country, you have the Gross Domestic Product (GDP) per capita and the unemployment rate. Using a scatter plot, you can visualize the relationship between a country’s wealth (represented by GDP per capita) and its unemployment rate. First, let’s look at our data. We have GDP per capita values for five countries in vector_1 and their corresponding unemployment rates in vector_2: # Example data (order matters) countries &lt;- c(&quot;Country A&quot;, &quot;Country B&quot;, &quot;Country C&quot;, &quot;Country D&quot;, &quot;Country E&quot;) vector_1 &lt;- c(40000, 62000, 54000, 42000, 53000) # GDP per capita vector_2 &lt;- c(12, 3, 4, 6, 6.5) # Unemployment rate Now, we’ll plot these values using a scatter plot: # Drawing the scatter plot plot(x = vector_1, y = vector_2, main = &quot;GDP per Capita vs. Unemployment Rate&quot;, xlab = &quot;GDP per Capita&quot;, ylab = &quot;Unemployment Rate (%)&quot;, col = &quot;blue&quot;, pch = 19, xlim = c(39000, 65000), ylim = c(2, 13)) # Adding country labels to the points text(x = vector_1, y = vector_2, labels = countries, pos = 1, cex = 0.8, col = &quot;red&quot;) Figure 3.3: Scatter Plot To explain the code: plot(): This is the primary function that creates the scatter plot. x = vector_1 and y = vector_2: These determine the x and y coordinates for each point on the plot, representing GDP per capita and unemployment rate respectively. main, xlab, and ylab: Provide the title for the graph and label the axes. main specifies the graph’s title while xlab and ylab label the x-axis and y-axis respectively. col and pch: Control the visual appearance of the points. col denotes the color (blue in this instance) and pch sets the shape (19 corresponds to a solid circle). xlim and ylim: These set the bounds for the x and y axes. In this case, we’ve set the x-axis to span from 39,000 to 65,000 and the y-axis from 2 to 13. text(): An auxiliary function used to add text labels to the graph. x = vector_1 and y = vector_2: Indicate the positions at which the text labels (country names) should be placed. labels = countries: Specifies the actual text to be added. Here, we’re adding country names. pos = 1: Defines the position of the text relative to the point, with 1 placing the text below the point. cex = 0.8: Controls the font size, where 0.8 denotes 80% of the default size. col = \"red\": Sets the text color to red. Upon running the code, a scatter plot is produced as depicted in Figure 3.3. Each blue dot corresponds to a country. The x-coordinate of the dot represents the country’s GDP per capita, and the y-coordinate showcases its unemployment rate. The country labels, written in red, help identify each dot. 3.4.2 Line Graph Imagine you’re monitoring the stock price of a company over a week. A line graph can help you visualize how the stock price changes each day. Each day’s stock price becomes a dot on the graph. The line connecting these dots helps us track the rise or fall of the stock price over time. Let’s say you recorded the stock price for a company, Company X, for two weeks in 2023: # Stock prices for Company X across two weeks days &lt;- as.Date(&quot;2023-03-05&quot;) + c(1:5, 8:12) stock_prices &lt;- c(99.5, 99.2, 99.7, 99.9, 98.8, 101.0, 103.2, 98.9, 96.0, 95.8) Now, we’ll plot these stock prices using a line plot: # Plotting the stock price changes plot(x = days, y = stock_prices, type = &quot;o&quot;, main = &quot;Stock Prices in 2023&quot;, xlab = &quot;&quot;, ylab = &quot;Stock Price ($)&quot;, xaxt = &quot;n&quot;, col = &quot;purple&quot;, pch = 19) axis(side = 1, las = 2, at = days, labels = format(days, &quot;%a W%U&quot;)) Figure 3.4: Line Graph Let’s break down the code chunk: Plotting: plot() is the main function used to draw the line graph. x = days and y = stock_prices: These specify the horizontal (days) and vertical (stock_prices) positions of the line plot. type = \"o\" specifies that the plot should have both points (o stands for “overplotted”) and lines. Alternatively, use type = \"l\" to only plot lines without points. main = \"Stock Prices in 2023\" gives the graph its title. xlab = \"\" and ylab = \"Stock Price ($)\" label the x-axis and y-axis respectively, where xlab = \"\" results in no x-axis title. xaxt = \"n\" tells R not to create its own x-axis labels, since we’ll add our custom labels for days of the week in the next step. col = \"purple\" sets the color of both the lines and points to purple. pch = 19 specifies the type of point to be a solid circle. Custom X-Axis: axis(): This function adds a custom x-axis to the plot. side = 1 denotes that we’re customizing the x-axis (x-axis is denoted by 1, y-axis by 2 in R plotting conventions). las = 2 controls the orientation of the axis labels, where 0 is parallel to the axis, 1 is always horizontal, 2 is perpendicular to the axis (vertical for x-axis), and 3 is always vertical. at = days indicates where along the axis the labels should be placed. In this case, we want a label at every date. However, if the axis is too cluttered, one could opt for at = days[seq(1, length(days), 3)] to put a label at every third date. labels = format(days, \"%a W%U\") instructs R to use both the name of the day and the week number from the days vector as labels. Specifically, %a represents the abbreviated weekday name, and %U denotes the week number.” After executing the code, the result is a line graph, depicted in Figure 3.4, that shows the fluctuations in the stock price of Company X over ten days. The blue dots represent daily stock prices, and the connecting lines show the progression of prices through the week. A line graph essentially represents an ordered scatter plot, with the line connecting data points based on their order. Hence, a line graph is only appropriate for ordered data. Consider the example of GDP per capita versus unemployment rate across different countries, as discussed in the scatter plot section; the order of countries doesn’t have any meaning, so the data isn’t “ordered.” In contrast, time series data, like stock prices over specific dates, always maintains an order — a later date follows an earlier one. Thus, the data is ordered and a line graph becomes a fitting choice. If you create a line graph using only a single input, R automatically assumes the x-axis to be a sequence of increasing integers: # Plotting the stock price changes plot(stock_prices, type = &quot;o&quot;, main = &quot;Stock Prices in Weeks 10 &amp; 11 in 2023&quot;, xlab = &quot;Weekday Index&quot;, ylab = &quot;Stock Price ($)&quot;, col = &quot;purple&quot;, pch = 19) Figure 3.5: Line Graph with Single Input This version is shown in Figure 3.5, where the x-axis corresponds to sequential weekdays, labeled as integers from 1 to 10. 3.4.3 Bar Chart In economics and finance, bar charts (or bar graphs) are frequently used to display and compare the values of various items. Consider the annual profits of different companies. Each company’s profit can be represented as a bar, where the height of the bar indicates the profit amount. Let’s illustrate the annual profits of five companies: # Profits of companies in millions companies &lt;- c(&quot;TechCorp&quot;, &quot;HealthInc&quot;, &quot;EduTech&quot;, &quot;GreenPower&quot;, &quot;FinanceFirm&quot;) profits &lt;- c(12, 8, 15, 9, 11) # Drawing the bar graph barplot(profits, main = &quot;Annual Profits of Companies&quot;, xlab = &quot;Company&quot;, ylab = &quot;Profits (in millions)&quot;, names.arg = companies, col = &quot;lightgreen&quot;, border = &quot;black&quot;) Figure 3.6: Bar Chart In the code: profits represent the annual profits of the companies. names.arg gives names to the bars, which are company names in our case. main, xlab, and ylab are used to title the graph and label the axes. col specifies the color of the bars, while border defines the color of the bar edges. 3.4.4 Frequency Bar Chart In economics, frequency bar charts (or count bar charts) are widely used to represent and compare the frequencies or counts of categorical items. Consider you have data about the preferred payment methods of customers in a store. Each payment method’s popularity can be depicted using bars, where the height of the bar indicates the count of customers who prefer that method. For example: # Randomly sampling payment methods based on their relative frequencies random_preferences &lt;- sample(x = c(&quot;Cash&quot;, &quot;Credit Card&quot;, &quot;Mobile Pay&quot;, &quot;Check&quot;), size = 1000, # Total number of customers replace = TRUE, prob = c(0.20, 0.50, 0.25, 0.05)) # Counting the number of customers for each payment method num_customers &lt;- table(random_preferences) # Drawing the bar chart barplot(num_customers, main = &quot;Preferred Payment Methods&quot;, xlab = &quot;Payment Method&quot;, ylab = &quot;Number of Customers&quot;, names.arg = names(num_customers), col = &quot;lightcoral&quot;, border = &quot;black&quot;) Figure 3.7: Frequency Bar Chart In this code: num_customers represents the number of customers for each payment method. names.arg labels the bars with the names of the payment methods. Other attributes like main, xlab, and ylab provide titles and labels to the graph. The resulting bar chart displays the count of customers who prefer each payment method, making it easier to determine the most and least popular payment methods. 3.4.5 Histogram In finance, when analyzing returns of a stock or any financial instrument, it’s helpful to understand how often different return values occur. A histogram provides such insights. It divides the data into bins or intervals and displays how many data points fall into each bin. The height of each bar represents the number of data points in that bin. Imagine you have daily return data for a stock over a year. Let’s visualize how often different return values occurred: # Simulated daily returns of a stock returns &lt;- rnorm(252, mean = 0.0005, sd = 0.02) # Drawing the histogram hist(returns, main = &quot;Histogram of Daily Stock Returns&quot;, xlab = &quot;Daily Return&quot;, ylab = &quot;Number of Days&quot;, col = &quot;lightblue&quot;, border = &quot;black&quot;, breaks = 20) Figure 3.8: Histogram Here’s what the code does: returns simulates daily returns of a stock over a trading year (usually 252 days). breaks determines how many bins or intervals the data should be divided into. Other parameters, like main, xlab, and ylab, give titles and labels. From the resulting histogram, one can gauge how often certain return values occurred throughout the year. 3.5 Functions In R, functions are essential tools designed to execute specific tasks. Think of a function as a small machine: you provide it with certain ingredients (called “arguments” or “inputs”), it processes them, and then outputs a result. 3.5.1 How Functions Operate For instance, let’s discuss the task of calculating an average. R provides a function named mean() to streamline this operation: # Using the mean function to calculate the average of a set of numbers mean(x = c(1, 2, 3, 4, 5, 6)) ## [1] 3.5 In this example, the function is mean(), and the input x is the vector c(1, 2, 3, 4, 5, 6). Functions can accept multiple arguments. Take the function sample(), which draws random samples from a data set. This function can have arguments like x (the data set) and size (number of items to draw). We can provide these arguments by name for clarity: # Using the sample function with named arguments sample(x = 1:10, size = 5) ## [1] 7 1 9 3 5 However, if we’re aware of the default order of the arguments, we might opt to skip naming them: # Using the sample function without named arguments sample(1:10, 5) ## [1] 9 5 4 1 10 Still, for clarity and to prevent unintended mistakes, using named arguments is generally recommended. 3.5.2 Infix Operators In R, infix operators don’t follow the usual pattern of function calls. Unlike traditional functions that have their arguments enclosed in parentheses after the function name, infix operators are positioned between their arguments. Familiar arithmetic operations like +, -, *, and / are examples. Similarly, [...] and [[...]], used for indexing, and the $ operator for extracting elements, are also infix operators. This is in contrast to prefix notation, where the function name comes before its enclosed arguments. However, it’s worth noting that these, too, are technically functions under the hood. In R, everything is a function! You can actually use the + operator or the [ operator in a prefix manner by enclosing them in backticks (`): # Representing 2 + 3 using prefix notation `+`(2, 3) ## [1] 5 # Representing character_vector[2] using prefix notation character_vector &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) `[`(character_vector, 2) ## [1] &quot;b&quot; This reveals the fundamentally functional design of R. 3.5.3 Control-Flow Operators In R, control-flow operators guide the sequence of execution based on conditions or iterations. They’re instrumental in shaping the logic of a program or script. Here’s a breakdown of these constructs: if(cond) expr: The if statement tests a condition, represented by cond. If the condition is TRUE, it executes expr. x &lt;- 5 if (x &gt; 3) print(&quot;x is greater than 3&quot;) ## [1] &quot;x is greater than 3&quot; if(cond) cons.expr else alt.expr: This expands on the basic if statement by adding an else clause. If cond is TRUE, cons.expr is executed; otherwise, alt.expr is executed. x &lt;- 2 if (x &gt; 3) { print(&quot;x is greater than 3&quot;) } else { print(&quot;x is less than or equal to 3&quot;) } ## [1] &quot;x is less than or equal to 3&quot; for(var in seq) expr: The for loop iterates over elements in a sequence (seq). In each iteration, var takes on a value from the sequence, and expr is executed. for (i in 1:5) { print(i^2) # prints the square of i } ## [1] 1 ## [1] 4 ## [1] 9 ## [1] 16 ## [1] 25 while(cond) expr: The while loop repeatedly executes expr as long as cond remains TRUE. x &lt;- 1 while (x &lt; 5) { print(x) x &lt;- x + 1 # increment x } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 repeat expr: The repeat loop indefinitely executes expr until a break statement is encountered. x &lt;- 1 repeat { print(x) x &lt;- x + 1 if (x &gt; 5) break # break out of the loop when x exceeds 5 } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 break: The break statement exits the current loop immediately, moving the flow of control to the statement following the loop. next: The next statement skips the rest of the current iteration and proceeds to the next iteration of the loop. for (i in 1:5) { if (i == 3) next # skip when i is 3 print(i) } ## [1] 1 ## [1] 2 ## [1] 4 ## [1] 5 These loops help perform actions multiple times, depending on specific conditions. This adaptability is key in data analysis, especially for repetitive tasks. 3.5.4 Help and Documentation R comes with a help system for when you need clarification on a function’s details. By prefixing the function’s name with ?, you can summon its official documentation. For example, to delve deeper into the sample() function, you’d input: ?sample Executing this in R or RStudio brings up the function’s documentation, detailing its purpose, its arguments, usage guidelines, and illustrative examples. In RStudio, this information is displayed in the bottom-right window. The usual approach to accessing help files, such as with ?sample, won’t directly work with infix operators like + or [ or control-flow operators like for or while. To retrieve information about these operations, you need to enclose them in backticks (`): # Accessing help for the addition operation ?`+` # Accessing help for the selection operation ?`[` # Accessing help for control-flow operators ?`for` To conclude, a sound understanding of functions and their arguments is pivotal for effective R programming. With the ? operator at your fingertips, you’re ensured quick access to any function’s intricacies. 3.5.5 Default Inputs Functions in R often come with default input values, providing flexibility and ease of use. These defaults are set by the function’s designer to ensure the function works out-of-the-box without requiring every parameter to be specified. However, these defaults may not be suitable for every application. Local Override Consider the mean() function. It has a default argument na.rm = FALSE, which determines whether or not to remove NA values before calculating the mean. If we want to remove NA values, we need to override the default: # Using the default mean function mean(c(1, 2, 3, NA, 5, 6)) ## [1] NA # Using the mean function with na.rm set to TRUE mean(c(1, 2, 3, NA, 5, 6), na.rm = TRUE) ## [1] 3.4 Similarly, the print() function has a default argument digits = 7, meaning that it print no more than 7 digits. To sample with replacement, you would do: # Using the default print function print(1.3947655619) print(10000 * 1.3947655619) ## [1] 1.394766 ## [1] 13947.66 # Using the print function with digits set to 10 print(1.3947655619, digits = 10) print(10000 * 1.3947655619, digits = 10) ## [1] 1.394765562 ## [1] 13947.65562 Understanding default inputs is crucial for effective function usage. To find out what the default arguments are for a given function, you can consult the function’s documentation. For instance, typing ?mean in the R console will display all the details about the mean() function, including its default inputs. Global Override If you find yourself consistently needing to change the defaults, consider using options() to set global options, affecting the behavior of functions and outputs throughout your R session. To retrieve the current value of an option, you can use getOption(). Here’s how you can change and query the default number of digits displayed in numerical output: # Show the current default number of digits current_digits &lt;- getOption(&quot;digits&quot;) current_digits ## [1] 7 # Change the default number of digits to 2 options(digits = 2) # Confirm that the option has been changed getOption(&quot;digits&quot;) ## [1] 2 # This will affect how numbers are printed print(1.3947655619) print(10000 * 1.3947655619) ## [1] 1.4 ## [1] 13948 # Resetting the option to its original value options(digits = current_digits) The options() setting will remain in effect for the duration of the R session or until you change it again. Note that the default inputs for most functions are local to those functions and not global options. For instance, the na.rm = FALSE default in the mean() function is specific to that function: # Attempt to show the default behavior for handling NA in mean() getOption(&quot;na.rm&quot;) ## NULL Setting options(na.rm = TRUE) would change this default at a global level, affecting other functions that also use an na.rm argument. This could introduce unintended behaviors and is generally not recommended. In such cases, it’s advisable to override the default value locally within the function call itself. 3.5.6 Custom Functions In R, you can create your own functions to perform specific tasks, allowing for code reusability and organization. A custom function is defined using the function keyword, followed by a set of parentheses, which can house any inputs the function might need. Here’s the basic structure: # Define a custom function function_name &lt;- function(inputs) { # Function body: operations to be performed return(result) # The value to be returned by the function } The return statement is optional. If it’s not included, the function will return the value of the last expression evaluated. For example, let’s create a function that computes the square of a number: # Define a custom function that squares the input square &lt;- function(x) { return(x * x) } # Testing the function square(4) # Should return 16 ## [1] 16 Functions can also be more complex, accepting multiple parameters and performing multiple operations: # Define a custom function that computes the area of a rectangle rectangle_area &lt;- function(length, width) { area &lt;- length * width return(area) } # Testing the function rectangle_area(5, 4) # Should return 20 ## [1] 20 When calling a function, R matches the provided arguments to the function’s parameters by order, unless specified by name. This means you can use named arguments for clarity: # Execute custom function using input names rectangle_area(length = 5, width = 4) ## [1] 20 You can set default values for function parameters when defining your custom functions. This provides flexibility by allowing the function to run without requiring every argument to be explicitly passed. To assign default values to parameters, you use the assignment operator = within the function definition. For example, let’s modify the rectangle_area function to have default values for length and width: # Define a custom function with default parameters rectangle_area &lt;- function(length = 1, width = 1) { area &lt;- length * width return(area) } # Testing the function with default parameters rectangle_area() # Should return 1 (1*1) ## [1] 1 # Testing the function with one default parameter rectangle_area(length = 5) # Should return 5 (5*1) ## [1] 5 # Testing the function with no default parameters rectangle_area(length = 5, width = 4) # Should return 20 (5*4) ## [1] 20 As shown, the function will use the default values unless you provide new values when calling the function. 3.5.7 Functions from R Packages In R, there’s no need to always create custom functions for every task. R packages, elaborated further in Chapter 3.6, are reservoirs of pre-defined functions developed by experts from various domains. Some packages are part of base R and are loaded automatically every time you start an R session. This means you can directly use the functions from these packages without any additional steps. Specifically, the loaded packages are base, datasets, graphics, grDevices, methods, stats, and utils by the R Core Team (2023). To employ functions from an R package, such as the xts package written by Ryan and Ulrich (2024b), you initially need to install the package using install.packages(\"xts\"). Subsequently, by adding library(\"xts\") at the top of your R script, the package’s functions can be used. An alternative to using library(\"xts\") is to prefix the desired function with the package’s name followed by the :: operator, like xts::first(). This approach can be particularly handy in cases where multiple packages offer functions with identical names. By specifying xts::first(), you ensure that the first() function from the xts package is the one being invoked. While the double colon :: operator is a way to directly call exported functions from a package, R also provides a triple colon ::: operator to access non-exported functions. These are internal functions intended to support the exported functions and aren’t directly accessible even when the package is loaded. 3.5.8 Indirect Functions Unlike typical functions that work directly on the provided inputs, indirect functions or higher-order functions operate on expressions, commands, or lists of arguments to be passed to other functions. In this section, we will delve into some of the most frequently used indirect functions, notably parse(), eval(), call(), and do.call(). The parse() and eval() Function In R, an expression is a language construct that, when evaluated, produces a result. That is, an expression can be thought of as stored R code that is not immediately evaluated. They can be evaluated later using the eval() function. This unique characteristic of delaying evaluation and programmatically generating and manipulating code makes expressions an invaluable tool for meta-programming. The parse() function plays a central role in the creation of expressions. It translates a character string into an R expression. In essence, it turns readable text that represents code (i.e., character strings) into a form that R can understand and evaluate. Given a character string as its argument, parse() returns an expression object. This object can be stored, manipulated, and subsequently evaluated. # Parsing a character string to produce an expression expr &lt;- parse(text = &quot;3 + 4&quot;) # Displaying the expression object expr # To evaluate this expression, use the eval() function eval(expr) ## expression(3 + 4) ## [1] 7 The call() Function While eval() is used to evaluate expressions, the call() function is used to construct function calls from a given name and arguments. The key distinction here is that call() does not execute the function immediately. Instead, it returns a callable expression that can later be evaluated using eval(). # Creating a callable expression for the sum of 2 and 3 sum_call &lt;- call(&quot;sum&quot;, 2, 3) # Evaluating the callable expression eval(sum_call) # Returns 5 # Other ways to achieve the same result eval(call(&quot;sum&quot;, 2, 3)) # Constructing and evaluating in one step eval(parse(text = &quot;sum(2, 3)&quot;)) # Using parse() and eval() directly sum(2, 3) # Directly calling the sum function ## [1] 5 ## [1] 5 ## [1] 5 ## [1] 5 In essence, expressions let developers create code dynamically, hold off on running it, and handle it like data. Though beneficial, one should use indirect functions cautiously to prevent unnecessary complexities. The do.call() Function The do.call() function can be thought of as a combination of call() and eval(). It takes a function and a list of arguments, and then calls the function with those arguments. In this sense, it’s like creating a callable expression using call() and immediately evaluating it with the eval() function. # Direct function execution sum(2, 3, 4, 5) # Using call() and eval() sum_expr = call(&quot;sum&quot;, 2, 3, 4, 5) eval(sum_expr) # Utilizing do.call() do.call(&quot;sum&quot;, list(2, 3, 4, 5)) ## [1] 14 ## [1] 14 ## [1] 14 In the above examples, both approaches achieve the same result, but do.call() does it in a more concise manner. Notably, do.call() becomes indispensable when the argument count is unpredictable because it accepts them in a list format. Consider you possess multiple character vectors that need to be sequentially linked: # Vector list initialization vec1 = c(&quot;apple&quot;, &quot;banana&quot;) vec2 = c(&quot;cherry&quot;, &quot;date&quot;, &quot;elderberry&quot;) vec3 = c(&quot;fig&quot;, &quot;grape&quot;) vectors_list = list(vec1, vec2, vec3) Applying do.call() in tandem with the c() function: # Direct function invocation c(vec1, vec2, vec3) # Indirect function invocation do.call(c, vectors_list) ## [1] &quot;apple&quot; &quot;banana&quot; &quot;cherry&quot; &quot;date&quot; &quot;elderberry&quot; ## [6] &quot;fig&quot; &quot;grape&quot; ## [1] &quot;apple&quot; &quot;banana&quot; &quot;cherry&quot; &quot;date&quot; &quot;elderberry&quot; ## [6] &quot;fig&quot; &quot;grape&quot; Effectively, the do.call() function “unpacks” the vector list, considering each vector as a distinct argument for the c() function. Thus, in contrast to directly invoking c(), do.call() facilitates the merging of all vectors within a list, regardless of the vector count or their individual names. 3.5.9 Apply Functions In R, while loops are commonly used for repetitive operations, an alternative that offers efficiency and conciseness are apply functions. This set includes apply(), lapply(), sapply(), vapply(), mapply(), replicate(), and more. These functions, as highlighted in Section 3.5.8, work as indirect functions: they accept a function as an argument and apply it across different data structures. Iteration over a Single Variable Consider an example where we want to square each element in a vector: # Using a loop to square elements of the vector numeric_vector &lt;- c(1, 3, 4, 12, 9) for(i in numeric_vector) { print(i^2) } ## [1] 1 ## [1] 9 ## [1] 16 ## [1] 144 ## [1] 81 This repetitive task can be succinctly performed using sapply: # Alternative approach using sapply sapply(X = numeric_vector, FUN = function(i) print(i^2)) ## [1] 1 ## [1] 9 ## [1] 16 ## [1] 144 ## [1] 81 ## [1] 1 9 16 144 81 The sapply function’s parameters are: X: This parameter represents the data on which the function will act. For the example provided, X = numeric_vector signifies a numeric vector with values c(1, 3, 4, 12, 9). The sapply function will then apply the specified function (indicated by FUN) to each element within this vector. FUN: This parameter indicates the function that sapply will apply to every element of X. In this instance, FUN = function(i) print(i^2) is a function designed to take an individual element i from numeric_vector and process it. Specifically, for each element i, this function computes its square (i^2) and subsequently prints the squared value. Hence, when you run the sapply function with these inputs, it will square each number in numeric_vector and print the squared values. It’s worth noting, however, that for operations as simple as squaring each element of a vector, we can apply the square operator directly to the numeric vector: # Using direct vectorized operation print(numeric_vector^2) ## [1] 1 9 16 144 81 Such vectorized methods tend to be more efficient than both loops and apply functions. However, for more intricate operations, apply functions demonstrate their value. Iteration over Multiple Variables For operations involving two or more arrays of data, the mapply() function is particularly handy. It can be seen as a multivariate version of sapply(). Using the task of raising the elements of one vector to the powers of another vector as an example: # Using mapply for operations on two vectors bases &lt;- c(10, 11, 12) powers &lt;- c(1, 2, 3) mapply(FUN = function(base, power) base^power, base = bases, power = powers) ## [1] 10 121 1728 The primary parameters for mapply() are: FUN: This is the function that mapply will apply element-wise to the arguments specified next. Here, FUN = function(base, power) base^power denotes a function that raises each base number to its corresponding power. Subsequent named arguments: These represent the data sets over which the function will be applied. For this example, base = bases and power = powers are the two vectors we’re working with. The function in FUN will be applied to them in a pairwise manner. So, the first element of bases will be raised to the power of the first element of powers, and so on. As with the sapply() example, a direct vectorized operation exists for the mapply() example that is more efficient: # Using direct vectorized operation print(bases^powers) ## [1] 10 121 1728 It’s only for more intricate operations that the apply family’s utility truly shines. Iteration for Specific R Objects Although we showcased these functions with vectors, there are other specialized “apply”-type functions tailored for different data structures. For example, lapply() is designed for lists and apply() for matrices. An exploration of these functions, along with the data structures they handle, is available in Chapter 4. Replication Another function from the apply family is replicate(). It serves as a convenient wrapper around the sapply() function for repeated evaluations of an expression. Using sapply(): sapply(X = 1:3, FUN = function(x) &quot;Hello&quot;) ## [1] &quot;Hello&quot; &quot;Hello&quot; &quot;Hello&quot; Using replicate() for identical operation: replicate(n = 3, expr = &quot;Hello&quot;) ## [1] &quot;Hello&quot; &quot;Hello&quot; &quot;Hello&quot; In essence, replicate() offers a more concise and readable way to execute repeated evaluations when the iteration doesn’t depend on the input sequence. 3.5.10 Advanced Higher-Order Functions While the apply functions described in Section 3.5.9 stand as quintessential higher-order functions in R, the language comes with other tools that also utilize this principle. Higher-order functions inherently accept other functions as arguments, applying them to various data operations and often eliminating the necessity for explicit looping (as touched upon in Section 3.5.8). Notable functions within this category include Reduce(), Filter(), Find(), Map(), Negate(), and Position(). The Reduce() Function The Reduce() function allows for cumulative operations over a list or vector, applying a given binary function (i.e., a function with two arguments) successively to the elements. For instance, consider the operation of summing a vector of numbers: # Summing numbers from 1 to 5 Reduce(f = `+`, x = 1:5) # returns 1 + 2 + 3 + 4 + 5 = 15 ## [1] 15 You can also use the init argument to provide a starting value: # Summing with an initial value Reduce(f = `+`, x = 1:5, init = 10) # returns 10 + 1 + 2 + 3 + 4 + 5 = 25 ## [1] 25 If you use the right argument, Reduce() processes the elements from right to left: # Subtract numbers from left to right Reduce(f = `-`, x = 1:5) # equivalent to: ((((1 - 2) - 3) - 4) - 5) # Subtract numbers from right to left Reduce(f = `-`, x = 1:5, right = TRUE) # equivalent to: 1 - (2 - (3 - (4 - 5))) ## [1] -13 ## [1] 3 And with accumulate, you can retain intermediate results: # Cumulative sums Reduce(f = `+`, x = 1:5, accumulate = TRUE) ## [1] 1 3 6 10 15 The Filter() Function The Filter() function retains the elements of a list or vector based on a given predicate function: # Keep only the even numbers Filter(f = function(x) x %% 2 == 0, x = 1:10) ## [1] 2 4 6 8 10 The Find() and Position() Functions Find() returns the first element of a list or vector for which a predicate function yields TRUE: # Discover the first number greater than 6 Find(f = function(x) x &gt; 6, x = c(1, 3, 5, 7, 9, 12)) ## [1] 7 Position() is similar to Find(), but instead of returning the value, it provides the position or index: # Position of the first number greater than 6 Position(f = function(x) x &gt; 6, x = c(1, 3, 5, 7, 9, 12)) ## [1] 4 The Map() Function Map() is essentially a wrapper for the base R function mapply() with SIMPLIFY = FALSE as its default, ensuring results are always returned as a list. # Add two vectors element by element Map(f = `+`, 1:3, 4:6) # returns list(5, 7, 9) ## [[1]] ## [1] 5 ## ## [[2]] ## [1] 7 ## ## [[3]] ## [1] 9 The Negate() Function Negate() returns a function that is the logical negation of a given function: # Define original function is.odd &lt;- function(x) x %% 2 == 1 # Negate original function is.even &lt;- Negate(f = is.odd) # Apply negated function to vector Filter(f = is.even, x = 1:10) ## [1] 2 4 6 8 10 These higher-order functions form the foundation of functional programming in R, offering concise and expressive means to operate on data structures without explicitly writing loops or iterative constructs. 3.5.11 Wrapper Functions In R, certain functions adapt their behavior based on the input’s data type or structure. The plot() function, for instance, produces a dot plot when applied to a numeric vector, but generates a bar plot for a factor vector. # Construct vectors with the same content but different classes example_vector_numeric &lt;- c(2, 1, 3, 3, 3, 3, 2, 3, 2, 1) example_vector_factor &lt;- factor(example_vector_numeric) print(example_vector_numeric) print(example_vector_factor) ## [1] 2 1 3 3 3 3 2 3 2 1 ## [1] 2 1 3 3 3 3 2 3 2 1 ## Levels: 1 2 3 # Put both plots on the same figure par(mfrow = c(1, 2)) # plot() for a numeric vector plot(example_vector_numeric, main = &quot;Numeric Vector&quot;) # plot() for a factor vector plot(example_vector_factor, main = &quot;Factor Vector&quot;) Figure 3.9: Plots Using a Wrapper Function Such functions that adjust their behavior based on the input’s class are known as wrapper functions or generic functions. When one of these functions is called: R identifies the class of the given object. It subsequently searches for a method, also known as an object-specific function, that corresponds to that class. If a suitable method exists, it’s applied to the object. Examples of other wrapper functions include mean(), print(), head(), tail(), summary(), str(), and many infix operators like +, -, [...], [[...]], and $. The methods() function lists all methods associated with a given wrapper function: # View available methods for plot methods(plot) ## [1] plot,ANY-method plot,color-method plot.acf* ## [4] plot.chobTA* plot.data.frame* plot.decomposed.ts* ## [7] plot.default plot.dendrogram* plot.density* ## [10] plot.ecdf plot.factor* plot.formula* ## [13] plot.function plot.ggplot* plot.gtable* ## [16] plot.hcl_palettes* plot.hclust* plot.histogram* ## [19] plot.HoltWinters* plot.isoreg* plot.lm* ## [22] plot.medpolish* plot.mlm* plot.ppr* ## [25] plot.prcomp* plot.princomp* plot.profile.nls* ## [28] plot.quantmod* plot.quantmodResults* plot.R6* ## [31] plot.raster* plot.replot* plot.replot_xts* ## [34] plot.shingle* plot.spec* plot.stepfun ## [37] plot.stl* plot.table* plot.transform* ## [40] plot.trellis* plot.ts plot.tskernel* ## [43] plot.TukeyHSD* plot.xts plot.zoo ## see &#39;?methods&#39; for accessing help and source code This reveals methods linked to the plot() function. Typically, each method is named in the format: &lt;wrapper_function_name&gt;.&lt;class_name&gt;() or &lt;wrapper_function_name&gt;.default(). For instance, plot.factor() gets activated for factor inputs, yielding a bar plot, while plot.default() runs for numeric inputs, rendering a dot plot. An asterisk * next to a function denotes a non-exported function, accessible only via the ::: operator, e.g., graphics:::plot.factor(). Calling these object-specific functions directly removes any uncertainty regarding which method will be executed. # Put both plots on the same figure par(mfrow = c(1, 2)) # plot.factor() creates a bar plot for any class graphics:::plot.factor(example_vector_numeric, main = &quot;Bar Plot of Numeric Vector&quot;) # plot.default() creates a line plot for any class plot.default(example_vector_factor, main = &quot;Line Plot of Factor Vector&quot;) Figure 3.10: Plots Using Object-Specific Functions When defining a new class in R, you can create custom methods for that class. For example, for a class named “fruit”, you might design a unique plot method as: # Define a custom method for the &quot;fruit&quot; class plot.fruit &lt;- function(x) { print(&quot;Hello fruit!&quot;) } # Modify the class of the numeric vector to &quot;fruit&quot; class(example_vector_numeric) &lt;- &quot;fruit&quot; # Use the print() function on the &quot;fruit&quot; object plot(example_vector_numeric) ## [1] &quot;Hello fruit!&quot; This adaptive behavior in programming, where functions vary based on object class, is a hallmark of object-oriented programming (OOP). In OOP, data and functions are “objects” that interact with one another. This differs from functional programming, where functions behave in a consistent manner regardless of the type of data they’re given. 3.6 R Packages R packages are collections of functions, datasets, and accompanying documentation bundled together. These packages enhance the functionality of R by providing additional tools not included in the base R distribution. The strength of R largely resides in the expansive ecosystem of available packages, which tackle a wide range of statistical, graphical, and data manipulation tasks. 3.6.1 CRAN CRAN, standing for the Comprehensive R Archive Network, is the central repository for R packages. It’s a global network of web servers hosting standardized and vetted R packages. CRAN ensures that these packages meet specific quality and documentation standards, making it a trusted source for the R community. Visit CRAN at cran.r-project.org. 3.6.2 Load and Update Packages R packages can be found on CRAN or other repositories. To install a package from CRAN: install.packages(&quot;name_of_the_package&quot;) Once installed, you need to load a package to make its functions, datasets, and other resources available in your current R session. Think of installing a package as adding a new book to your bookshelf, while loading it is like opening that book to use and read its contents. Without loading the package, R won’t recognize the commands or functions associated with it, even if it’s already installed. Load a package with: library(&quot;name_of_the_package&quot;) To get a comprehensive list of a package’s functions: ls(&quot;name_of_the_package&quot;, all.names = TRUE) For each package, CRAN provides extensive documentation: Reference Manual: A detailed guide listing all the package’s functions, datasets, and objects. Found on the package’s CRAN page under the ‘Reference manual’ link. Vignettes: Comprehensive guides offering a narrative on the package’s capabilities, complete with examples. Accessible from the package’s CRAN page under ‘Vignettes’ or in R using vignette(). Function Help: Specific information about a function can be retrieved using the ? prefix in R, e.g., ?function_name. Over time, package authors release updates to add new features, fix bugs, or improve performance. It’s a good practice to regularly update your packages. You can do this using: update.packages() This function will go through all installed packages and check if there are newer versions available. 3.6.3 Noteworthy Packages Below are some noteworthy R packages: xts (Ryan and Ulrich 2024b): Designed for handling time series data. It provides an extensible time series class that can manage ordered observations. quantmod (Ryan and Ulrich 2024a): Aimed at managing, modeling, and analyzing quantitative financial modeling. It’s a rapid prototyping environment that makes modeling easier. Quandl (Raymond McTaggart, Gergely Daroczi, and Clement Leung 2021): An interface to the Quandl databases. It allows direct access to financial and economic datasets from within R. knitr (Xie 2023): An engine for dynamic report generation. It allows for embedding R code in documents to produce analyses that are both reproducible and dynamic. It also comes with the kable() function used to convert data objects into professional-looking tables. kableExtra (Zhu 2024): Enhances the functionality of kable(), a table-generating function. It provides a suite of functions to fine-tune table designs. ggplot2 (Wickham et al. 2024): A system for creating graphics based on the grammar of graphics. It provides a flexible and consistent base for producing almost any kind of statistical graphic. tibble (Müller and Wickham 2023): An evolution of data frames. Tibbles are a modern take on data frames, making them easier to work with. dplyr (Wickham et al. 2023): A grammar for data manipulation. It provides verbs to help manage and analyze datasets, especially within the Tidyverse framework. tidyverse (Wickham 2023c): An ensemble of packages tailored for data science tasks. It encompasses tools for data handling, visualization, and modeling, to name a few. Further details can be found in the Tidyverse section 3.6.7. readr (Wickham, Hester, and Bryan 2024): Part of the Tidyverse, it provides a fast and friendly way to read rectangular data (like CSV, TSV, and FWF). readxl (Wickham and Bryan 2023): Used for reading Excel files (.xls and .xlsx). It does not require any external dependencies, making it easy to install. data.table (Barrett et al. 2024): Provides an enhanced version of a data frame that allows for fast aggregation of large datasets, as well as other operations. Each of these packages is tailored for specific tasks, and they represent just a glimpse into the vast ecosystem of R packages available. Before diving into any package, it’s recommended to check its documentation and vignettes to understand its capabilities and usage, as discussed in the subsequent section 3.6.4. 3.6.4 Package Documentation When working with R packages, understanding the package’s capabilities and its functions is crucial. Every package comes with documentation for each function, and many also come with vignettes which are long-form guides to the package. Here’s how you can access them: Function Documentation: To access the documentation of a particular function, prepend the function name with a ?. If you want to understand the filter() function from the dplyr package, input: ?dplyr::filter This command displays the help file for the function, detailing its purpose, input parameters, and other relevant information. The example() function can also be invoked to view practical applications of a function. For instance: library(&quot;dplyr&quot;) example(filter) Vignettes: Vignettes are detailed guides provided by package authors. They offer an in-depth look into package functionalities, complete with examples and use-cases. To see the list of available vignettes for a package, you can use: vignette(package = &quot;dplyr&quot;) This doesn’t directly load a vignette but shows a list of available vignettes for the package. To access a specific vignette by its name: vignette(&quot;programming&quot;, package = &quot;dplyr&quot;) Demo and Examples: Some packages also come with demos, which are scripts showcasing how to use the package’s functions. To see available demos: demo(package = &quot;quantmod&quot;) To run a specific demo: demo(topic = &quot;chartSeries&quot;, package = &quot;quantmod&quot;) Always ensure that the package is installed and loaded into your R session using library() before trying to access its vignettes or documentation. 3.6.5 Function Name Conflicts In R, the order in which packages are loaded can influence which function is used when multiple packages have functions with the same name. For instance, both dplyr and xts include a first() function. If dplyr is loaded before xts, i.e. library(\"dplyr\") is executed before library(\"xts\"), then calling first() will use the dplyr version by default. To specify the xts version, one can use the :: operator, like so: xts::first(). This operator ensures the correct function is called and can also invoke a function without loading the entire package, meaning xts::first() works even without running library(\"xts\"). 3.6.6 Non-Exported Functions While the double colon :: operator allows you to directly call exported functions from a package, R provides a triple colon ::: operator to access non-exported functions. Non-exported functions are internal functions designed to support the primary, exported functions. They aren’t meant for regular use and aren’t directly accessible, even if the package is loaded. For instance, if there’s a non-exported function named hiddenFunction in the dplyr package, you can access it using dplyr:::hiddenFunction(). Be cautious when using non-exported functions, as they are internal for a reason. Their functionality or behavior may change in future package updates without warning, and they might not be as thoroughly tested or documented as exported functions. To list all the functions and objects of a package, including the non-exported ones: ls(getNamespace(&quot;name_of_the_package&quot;), all.names = TRUE) If you’re interested specifically in the non-exported functions of a package, such as graphics, you can differentiate between its exported and non-exported functions: all_functions &lt;- ls(getNamespace(&quot;graphics&quot;), all.names = TRUE) exported_functions &lt;- ls(&quot;package:graphics&quot;, all.names = TRUE) non_exported_functions &lt;- setdiff(all_functions, exported_functions) tail(non_exported_functions, 20) ## [1] &quot;pairs.formula&quot; &quot;persp.default&quot; &quot;piechart&quot; ## [4] &quot;plot.data.frame&quot; &quot;plot.factor&quot; &quot;plot.formula&quot; ## [7] &quot;plot.histogram&quot; &quot;plot.raster&quot; &quot;plot.table&quot; ## [10] &quot;plotHclust&quot; &quot;points.formula&quot; &quot;points.table&quot; ## [13] &quot;RunregisterBase&quot; &quot;spineplot.default&quot; &quot;spineplot.formula&quot; ## [16] &quot;stripchart.default&quot; &quot;stripchart.formula&quot; &quot;sunflowerplot.default&quot; ## [19] &quot;sunflowerplot.formula&quot; &quot;text.formula&quot; 3.6.7 The Tidyverse The tidyverse (Wickham 2023c) is a collection of R packages designed to simplify data science tasks. These packages share an underlying philosophy and common APIs, centered around the principles of tidy data. By adhering to a consistent syntax and methodology, the tidyverse ensures a streamlined workflow and an easier learning curve for new users. Packages within the Tidyverse # Load all tidyverse packages library(&quot;tidyverse&quot;) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.4 ✔ readr 2.1.5 ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ ggplot2 3.5.1 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.3 ✔ tidyr 1.3.1 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::first() masks xts::first() ## ✖ dplyr::group_rows() masks kableExtra::group_rows() ## ✖ dplyr::lag() masks stats::lag() ## ✖ dplyr::last() masks xts::last() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors ggplot2 (Wickham et al. 2024): An implementation of the Grammar of Graphics, this is a powerful tool for creating complex graphics with fine control over visual aesthetics. dplyr (Wickham et al. 2023): A grammar for data manipulation, offering a set of verbs that help you solve the most common data manipulation challenges. tidyr (Wickham, Vaughan, and Girlich 2024): Aids in tidying data. Tidy data has a specific structure which makes it easy to visualize, model, and analyze. readr (Wickham, Hester, and Bryan 2024): Provides functions to read rectangular data quickly. purrr (Wickham and Henry 2023): A functional programming toolkit, enhancing R’s existing sapply(), lapply(), etc. functional capabilities. stringr (Wickham 2023b): Simplifies the types of operations that can be performed on strings. forcats (Wickham 2023a): Tools for working with factor variables (categorical data), especially useful for reordering factor levels or changing the representation of a categorical variable. lubridate (Spinu, Grolemund, and Wickham 2023): Simplifies working with date-times in R. It has functions to extract components of a date-time, such as year or month, and to perform common date arithmetic. tibble (Müller and Wickham 2023): Provides a modern rethinking of the data frame, keeping what time has shown to be effective, and throwing out what it has not. Pipe Operator (%&gt;%) The pipe operator, %&gt;%, is a key feature of the tidyverse. Originating from the magrittr package by Bache and Wickham (2022), and popularized by the Tidyverse, this operator makes Tidyverse-style code look much different from traditional R code. The operator sends the result of one function into another, leading to a clearer left-to-right sequence and reducing nested functions and temporary variables. Suppose we’re computing the mean growth rate of a series of numbers: # Series of numbers x &lt;- c(100, 101, 103, 105, 112) # Compute mean growth rate with traditional R approach round(mean(100 * diff(log(x))), 2) # Compute mean growth rate with pipe operator x %&gt;% log() %&gt;% diff() %&gt;% `*`(100) %&gt;% mean() %&gt;% round(2) ## [1] 2.83 ## [1] 2.83 Let’s break it down step by step. The traditional R approach uses nested functions. Starting from the inside: log(x) - Computes the natural logarithm of each element in x. diff(log(x)) - Calculates the difference between successive log values, essentially capturing the growth rate. 100 * diff(log(x)) - Scales the growth rate to a percentage. mean(100 * diff(log(x))) - Calculates the average of these growth rates. round(mean(100 * diff(log(x))), 2) - Finally, rounds the mean growth rate to two decimal places. The pipe operator allows us to pass the result of one function directly as input to the next function. Instead of nesting functions inside each other, we can chain them from left to right: x %&gt;% log() - Takes x and passes it to log(), computing the logarithm. x %&gt;% log() %&gt;% diff() - Then passes the log values to diff(). x %&gt;% log() %&gt;% diff() %&gt;% `*`(100) - Multiplies the differences by 100 to get percentages. x %&gt;% log() %&gt;% diff() %&gt;% `*`(100) %&gt;% mean() - Computes the mean of the growth rates. x %&gt;% log() %&gt;% diff() %&gt;% `*`(100) %&gt;% mean() %&gt;% round(2) - Finally, rounds the mean to two decimal places. With the pipe operator, you can clearly see the sequence of operations. It often makes the code more readable as it follows a logical, left-to-right flow, mirroring the order of operations you would verbalize if explaining the process out loud. Additional Resources To delve deeper into the Tidyverse, explore their official website: www.tidyverse.org. Another resource is the R-Bootcamp, available at r-bootcamp.netlify.app. Additionally, DataCamp provides a comprehensive skill track devoted to the Tidyverse, named Tidyverse Fundamentals with R. 3.6.8 Citing R Packages R packages are written by a diverse group of contributors, including professional statisticians, data scientists, researchers, hobbyists, and students. These individuals often craft packages to: Address specific challenges in their domain. Introduce new methodologies. Simplify and automate repetitive tasks. Reasons for developing and sharing packages include: Community Contribution: Giving back to the R community by providing solutions to common problems. Standardization: Promoting best practices and standard implementations for specific tasks. Academic Recognition: Researchers might release a package alongside academic papers, integrating theory with practical tools. Feedback and Improvement: A public release allows for community feedback, leading to refinements and enhancements. When using R packages in your research or any scholarly work, it’s imperative to give credit to the authors and maintainers who have contributed to the development of these tools. Citing packages correctly not only shows respect to the developers but also ensures transparency in your methodologies and allows others to reproduce your work. Steps to Cite an R Package: Find the Citation Information: Most R packages come with built-in citation information. To retrieve this, you can use the citation() function followed by the package name in quotes. For example: citation(&quot;quantmod&quot;) ## To cite package &#39;quantmod&#39; in publications use: ## ## Ryan JA, Ulrich JM (2024). _quantmod: Quantitative Financial ## Modelling Framework_. R package version 0.4.26, ## &lt;https://CRAN.R-project.org/package=quantmod&gt;. ## ## A BibTeX entry for LaTeX users is ## ## @Manual{, ## title = {quantmod: Quantitative Financial Modelling Framework}, ## author = {Jeffrey A. Ryan and Joshua M. Ulrich}, ## year = {2024}, ## note = {R package version 0.4.26}, ## url = {https://CRAN.R-project.org/package=quantmod}, ## } Include Package Version: Given that R packages are frequently updated, mentioning the version you used can be vital for reproducibility. This can be found using the packageVersion() function. For example: packageVersion(&quot;quantmod&quot;) ## [1] &#39;0.4.26&#39; Mention All Relevant Packages: If your work relies on multiple R packages, ensure you cite each one of them. If you’re using a suite of packages, like the tidyverse, consider citing the individual packages within the suite that were essential to your analysis. Citing Programs and Languages: Alongside referencing individual packages, it’s also recommended to acknowledge all the languages and programs used. This practice ensures that the creators and maintainers of these tools receive due recognition for their contributions. For example, if you’re producing dynamic PDF documents with RStudio, referencing the tools discussed in Chapter 6 becomes crucial: You should cite the R language (R Core Team 2023) itself as it provides the core statistical and graphical functionalities. Acknowledge the RStudio IDE (Posit Team 2023), which serves as a comprehensive platform for developing and executing R scripts. Recognize the LaTeX markup language (Lamport 1986). If you’re utilizing TinyTeX (Xie 2024b), it’s vital to mention it together with TeX Live (TeX Users Group 1996), which is the foundation upon which it is built. Mention the Markdown language (Gruber 2004), a straightforward formatting language that aids in the creation of content. The R Markdown package (Allaire et al. 2024) deserves a citation as it’s instrumental in integrating narrative text and R code. It collaborates with tools like knitr and pandoc to transform the markdown document with embedded results into the desired output format, whether PDF, HTML, or other formats. Credit the knitr package (Xie 2023). As the primary engine for R Markdown, it seamlessly integrates R code with the narrative. It takes on the role of executing the R code chunks within the R Markdown document, embedding results, whether they are tables, figures, or text, into the final markdown output. Acknowledge Pandoc (MacFarlane 2023), a versatile document converter that’s indispensable for changing R Markdown files into various output forms. And, of course, ensure to cite any other R packages or tools that played a significant role in creating your R Markdown content. For other software or tools you utilized, be sure to consult their documentation or websites for appropriate citation information. To retrieve the citation details for both R and RStudio within RStudio, you can execute the following: # Retrieving the citation for the R language citation() ## To cite R in publications use: ## ## R Core Team (2023). _R: A Language and Environment for Statistical ## Computing_. R Foundation for Statistical Computing, Vienna, Austria. ## &lt;https://www.R-project.org/&gt;. ## ## A BibTeX entry for LaTeX users is ## ## @Manual{, ## title = {R: A Language and Environment for Statistical Computing}, ## author = {{R Core Team}}, ## organization = {R Foundation for Statistical Computing}, ## address = {Vienna, Austria}, ## year = {2023}, ## url = {https://www.R-project.org/}, ## } ## ## We have invested a lot of time and effort in creating R, please cite it ## when using it for data analysis. See also &#39;citation(&quot;pkgname&quot;)&#39; for ## citing R packages. # Retrieving the citation for the RStudio program RStudio.Version()$citation Remember, it’s always a good practice to give credit where it’s due, and citing the tools you’ve used is a crucial part of academic and professional integrity. 3.7 File Management R can do more than just process its scripts; it can communicate directly with your computer. By connecting to the system’s terminal, R can manage files, download files from the web, and work with other software, like sending emails or crafting Word documents. This chapter focuses on key R functions for accessing and saving files. 3.7.1 Find Folder Path To utilize R for accessing, modifying, and storing files on your computer, like economic data, it’s essential to know their exact locations. Each file exists at a distinct location, possibly embedded within several folders. This hierarchical structure helps in organizing the vast amounts of data that computers store. The exact location of a file is called its “folder path” or simply “path”. A folder path is like an address for a file, guiding you or software applications to its precise location. It starts from a root directory and moves through each nested folder until it reaches the desired file. For example, a path might look like C:\\Users\\John\\Documents\\myfile.txt on a Windows system, or /Users/John/Documents/myfile.txt on a Mac. To find out the folder path for a specific file or folder on your computer, you can follow these steps: For Windows: Open File Explorer and navigate to the file you want. Once the file is located, click on the address bar at the top of the File Explorer window. This will reveal the file’s full path. Copy the path by right-clicking the highlighted address and selecting ‘Copy’, or by pressing Ctrl + C. A folder path on Windows looks like this: C:\\Users\\YourName\\Documents. When recording this file path in R, paste the path with a right-click followed by ‘Paste’ or by pressing Ctrl + V. Encapsulate this in quotation marks to mark it as a string. Modify the backslashes (\\) to forward slashes (/) or double them (e.g., \\\\) due to R’s interpretation of a singular backslash as an escape character rather than as a division between folders. # Example of a file name and its folder path: my_file &lt;- &quot;data.csv&quot; my_folder_path &lt;- &quot;C:/Users/YourName/Documents&quot; For MacOS: Open Finder and navigate to your desired file. Once you locate the file, click on it to select it. While the file is selected, press Command + Option + C. This action copies the file path to your clipboard. Another approach is to Control-click (or employ a mouse’s right-click) on the file, selecting “Get Info”, and subsequently copying the folder path listed under “Where”. A folder path on Mac looks like this: /Users/YourName/Documents. To input this file path into R, use a right-click followed by ‘Paste’ or press Cmd + V, and wrap the path in quotation marks. # Example of a file name and its folder path my_file &lt;- &quot;data.csv&quot; my_folder_path &lt;- &quot;/Users/YourName/Documents&quot; To direct R to a specific file, you must combine the folder path and the file name into one cohesive string separated by a backslash (\\). You can manually assemble these using the paste0() function or employ the file.path() function which automatically uses the appropriate slash: # Assemble file path using paste0() paste0(my_folder_path, &quot;/&quot;, my_file) ## [1] &quot;/Users/YourName/Documents/data.csv&quot; # Formulate file path using file.path() file.path(my_folder_path, my_file) ## [1] &quot;/Users/YourName/Documents/data.csv&quot; For instance, if you have a data.csv file, you can read it into R with the command read.csv(\"/Users/YourName/Documents/data.csv\"). Alternatively, if you have previously defined the my_folder_path and my_file variables, you can use read.csv(file.path(my_folder_path, my_file)). 3.7.2 Set Working Directory The working directory in R is the folder where R starts when it’s looking for files to read or write. If you’re not sure where your current working directory is, you can use the getwd() (get working directory) command in R to find out: getwd() ## [1] &quot;/Users/julianludwig/Library/CloudStorage/Dropbox/Economics/teaching/eda/textbook&quot; To change your working directory, use the setwd() (set working directory) function: setwd(&quot;your/folder/path&quot;) Be sure to replace \"your/folder/path\" with the actual path to your folder. When your files are stored in the same directory as your working directory, defined using the setwd() function, you can directly access these files by their names. For instance, read.csv(\"data.csv\") will successfully read the file if “data.csv” is in the working directory. If the file is located in a subfolder within the working directory, for example a folder named files, you would need to specify the folder in the file path when calling the file: read.csv(\"files/data.csv\"). Remember, when setting the working directory in R, you need to use forward slashes (/) in the folder path, even on Windows where the convention is to use backslashes (\\). 3.7.3 Download Files Starting any data analysis requires first securing the desired data on your local machine, ideally within a designated directory. This can be achieved manually by creating a new directory using the File Explorer in Windows or Finder in Mac, followed by a download from the website. However, R offers the convenience of performing both tasks directly. To create a folder within R, utilize the dir.create() function. Insert the desired directory path: # Setting up a directory in the working environment for data storage dir.create(path = &quot;/Users/YourName/Documents/files&quot;) By executing this code, a new folder named “files” will be established within the existing “Documents” directory located at “/Users/YourName/Documents”. You can then use this “files” folder to store and manage your datasets or any related files. If the directory “files” is already present, the function will issue a warning. To avoid this, include the showWarnings = FALSE parameter. If you’re looking to create a directory inside the present working directory, defined using setwd(), simply state the directory’s name: # Establishing a directory within the current working space for data storage dir.create(path = &quot;files&quot;, showWarnings = FALSE) Suppose you wish to analyze Bitcoin prices from the Bitstamp exchange. This data is available via CryptoDataDownload, a platform for cryptocurrency research. They offer various cryptocurrency historical datasets in CSV format. To obtain the minute-frequency data for Bitcoin to USD (Bitstamp 2024), go to www.cryptodatadownload.com, choose “Historical Data”, select “US &amp; UK Exchanges”, click on “Bitstamp”, and find the “Bitstamp minute data broken down by year” section. Here, opt for “BTC/USD 2023 minute” and click “Download CSV”. After downloading, move the file to your ‘files’ directory and rename it BTCUSD_minute.csv. For a streamlined download within R, right-click on “Download CSV” and select “Copy Link Address”. Subsequently, use this copied link address as the url input for the download.file() function: # Direct download of Bitcoin price data in R download.file( url = &quot;https://www.cryptodatadownload.com/cdd/Bitstamp_BTCUSD_2023_minute.csv&quot;, destfile = &quot;files/BTCUSD_minute.csv&quot;) It’s crucial to acknowledge the sources of your data. The Bitcoin prices are sourced from the Bitstamp exchange and distributed by CryptoDataDownload. If you’ve accessed the data from CryptoDataDownload, both entities merit citation. As an example, if the data was accessed on September 20, 2023, you would cite it as: Bitstamp. 2023. “BTC/USD 2023 Minute Data.” https://www.cryptodatadownload.com/cdd/Bitstamp_BTCUSD_2023_minute.csv (September 20, 2023). Retrieved from CryptoDataDownload. Remember to be ethical when downloading data in R. Servers have finite resources, and excessive downloading can strain these systems, potentially making the data inaccessible to others. Overloading the server not only diminishes the user experience for others but could lead to additional costs for the hosting entity. As a best practice, limit your downloads to a few times per day and save the data on your device. This way, you reduce the need for repeated downloads. By being considerate in your downloading habits, you help ensure that everyone can access and benefit from the resources provided. 3.7.4 Import Files When beginning any data analysis, the first step is to bring your data into the software environment. Let’s consider the scenario where you are interested in analyzing Bitcoin prices, which is stored as a CSV file under files/BTCUSD_minute.csv. The read.csv() function in R is tailored to read comma-separated values (CSV) files. By specifying the skip argument, we can ignore unnecessary rows at the start of our file. Moreover, using nrows allows us to limit the number of rows we import. # Import CSV file with Bitcoin prices btcusd &lt;- read.csv(&quot;files/BTCUSD_minute.csv&quot;, skip = 1, nrows = 1000) # Display the first 6 rows (head) of columns 1 to 7 for an overview head(btcusd[, 1:7]) ## unix date symbol open high low close ## 1 1697413380 2023-10-15 23:43:00 BTC/USD 27114 27119 27107 27119 ## 2 1697413320 2023-10-15 23:42:00 BTC/USD 27114 27120 27114 27120 ## 3 1697413260 2023-10-15 23:41:00 BTC/USD 27115 27119 27115 27119 ## 4 1697413200 2023-10-15 23:40:00 BTC/USD 27106 27120 27106 27116 ## 5 1697413140 2023-10-15 23:39:00 BTC/USD 27105 27105 27103 27104 ## 6 1697413080 2023-10-15 23:38:00 BTC/USD 27101 27104 27097 27104 This loaded data is stored in a structure known as a data frame (data.frame). The specifics and utility of this structure will be discussed in Chapter 4. Chapter 5 elaborates on importing various file formats besides CSV, including TSV files, Excel spreadsheets, and data embedded within websites. 3.7.5 Export Files After refining data in R, there might be a need to save the cleansed data or even the visual representations of your analyses. One way to keep organized is by setting aside a specific directory for your outputs, such as exported-files: # Create a directory for exported data and visuals dir.create(path = &quot;exported-files&quot;, showWarnings = FALSE) To illustrate, consider you’re analyzing Bitcoin’s price trends, transitioning from minute to hourly observations. The transformation steps below will be expounded upon in Chapter 5. For now, the focus is on exporting the transformed data. # Date-time adjustments and data aggregation btcusd$date &lt;- as.POSIXct(btcusd$date) btcusd$date_hour &lt;- as.POSIXct(format(btcusd$date, &quot;%Y-%m-%d %H:00:00 CDT&quot;)) btcusd_hour &lt;- aggregate(list(close = btcusd$close), by = list(date_hour = btcusd$date_hour), FUN = mean) # Assign a custom attribute attr(btcusd_hour, &quot;source&quot;) &lt;- &quot;www.cryptodatadownload.com/cdd&quot; The saveRDS() function allows for exporting R objects, which can be re-imported via readRDS(): # Export and re-import to verify saveRDS(btcusd_hour, &quot;exported-files/BTCUSD_hour.rds&quot;) btcusd_hour_imported &lt;- readRDS(&quot;exported-files/BTCUSD_hour.rds&quot;) # Verify if the original and reloaded files are the same identical(btcusd_hour, btcusd_hour_imported) ## [1] TRUE For broader compatibility, it’s practical to save the transformed data as CSV: # Export transformed data as CSV file write.csv(btcusd_hour, &quot;exported-files/BTCUSD_hour.csv&quot;) btcusd_hour_imported &lt;- read.csv(&quot;exported-files/BTCUSD_hour.csv&quot;) # Verify if the original and reloaded CSV files are the same identical(btcusd_hour, btcusd_hour_imported) ## [1] FALSE Note that exporting to CSV does not preserve all information, such as attributes. Moreover, upon import, the data types may not be accurately recognized. As a result, the original and reloaded files may not be identical when using CSV. Visual outputs can be saved as well. The example below illustrates Bitcoin’s recent price trajectory: # Plot data plot(x = btcusd$date, y = btcusd$close, type = &quot;l&quot;, main = &quot;Bitcoin Prices During 1,000 Trading Minutes&quot;, ylab = &quot;USD&quot;, xlab = &quot;Time&quot;) Figure 3.11: Bitcoin Prices During 1,000 Trading Minutes Delve into Chapter 5 to learn more about data visualization in R. Saving the above plot as a PDF is done via the pdf() function: # Plot data and save the figure as a PDF file pdf(&quot;exported-files/my_plot.pdf&quot;) plot(x = btcusd$date, y = btcusd$close, type = &quot;l&quot;, main = &quot;Bitcoin Prices&quot;, ylab = &quot;USD&quot;, xlab = &quot;Time&quot;) dev.off() The code provided generates a PDF of Figure 3.11 and stores it in the exported-files folder under the name my_plot.pdf. 3.7.6 Delete Files Post analysis, for tidiness or to manage storage, you might decide to remove some files or even entire folders: # Remove individual files file.remove(&quot;exported-files/BTCUSD_hour.csv&quot;, &quot;exported-files/BTCUSD_hour.rds&quot;, &quot;exported-files/my_plot.pdf&quot;) # Eliminate the entire folder unlink(&quot;exported-files&quot;, recursive = TRUE) Note: Be cautious when deleting files or folders, as this action is often irreversible. 3.8 Optimal Workflow To use RStudio effectively, it’s essential to adopt good habits in your workflow. 3.8.1 Execute as You Write R is a dynamic language, which means you can and should execute code as you write it. This interactive approach provides immediate feedback, enabling you to: Identify and rectify errors early. Monitor the effects of your code. Understand the results of a particular function or operation instantly. Recommendation: Every time you write a new line or block of code, execute it and inspect the outcome. To execute only certain parts of your code in the R script: Select within a Single Line: Start by positioning your cursor at the beginning of the code segment you wish to evaluate. By pressing and holding the left mouse button, drag to the end of the segment. Span Selection across Lines: Click at the start point of your desired selection. While holding down the Shift key, click at your end point, highlighting the entire code segment in between. Run the Highlighted Segment: With your chosen code segment highlighted, press Ctrl + Enter (Windows/Linux) or Command + Enter (Mac) to execute. The result will be displayed in the console immediately. Execute Entire Line: If no segment is highlighted, then Ctrl + Enter (Windows/Linux) or Command + Enter (Mac) will execute the entire line where the cursor is positioned. 3.8.2 Key Shortcuts Keyboard shortcuts can significantly streamline your coding process. Here are some essential shortcuts for RStudio: Execute Current Line/Selection: Ctrl + Enter (or Cmd + Enter on Mac) Execute All Code in R Script: Ctrl + Shift + Enter (or Cmd + Shift + Enter) Execute an R Markdown Script (“Knit”): Ctrl + Shift + K (or Cmd + Shift + K) Clear Console: Ctrl + L Jump to Line Number: Ctrl + Alt + Shift + G (or Cmd + Option + Shift + G) Find and Replace: Ctrl + F (or Cmd + F on Mac) Comment/Uncomment Line: Ctrl + Shift + C (or Cmd + Shift + C) Open a New Script: Ctrl + Shift + N (or Cmd + Shift + N) Save the Script: Ctrl + S (or Cmd + S) Zoom In: Ctrl + + (or Cmd + + on Mac) Zoom Out: Ctrl + - (or Cmd + - on Mac) I suggest regularly consulting RStudio’s built-in shortcut list to reinforce your memory. Navigate to Help &gt; Keyboard Shortcuts Help or use the shortcut Alt + Shift + K (or Option + Shift + K on Mac) for quick access. Remember, mastering shortcuts requires practice. Over time, these combinations will become second nature, leading to a smoother and faster coding experience. 3.8.3 Separate Window for R Script Personally, I prefer to have the R script in a separate window, as depicted in Figure 3.2 in Chapter 3.1. To do this, click on the icon in the top-left corner in RStudio. This setup grants a broader perspective of your R code, making full use of your screen space instead of just the limited top-left section of RStudio. To swiftly alternate between RStudio and the detached RScript, press Alt + Tab on Windows or Command + ` on Mac. Additionally, for switching between different applications, like toggling between RStudio and Chrome, press Ctrl + Tab on Windows or Command + Tab on Mac. I find this shortcut indispensable in my daily tasks. 3.8.4 Conclusion Crafting an efficient workflow in RStudio revolves around understanding the nature of R as a dynamic language, using keyboard shortcuts, and customizing the IDE layout to suit your preferences. Combining these practices will increase your productivity. References "],["data-structures-in-r.html", "Chapter 4 Data Structures in R 4.1 Scalar 4.2 Vector 4.3 Matrix (matrix) 4.4 List (list) 4.5 Data Frame (data.frame) 4.6 Tibble (tbl_df) 4.7 Data Table (data.table) 4.8 Extensible Time Series (xts)", " Chapter 4 Data Structures in R This chapter delves into the following data structures and their associated R functions: Scalar: A singular data point, such as a string or a number. Vector: A one-dimensional array that contains elements of the same type. Matrix (matrix): A two-dimensional array that contains elements of the same type. List (list): A one-dimensional array capable of storing various data types. Data Frame (data.frame): A two-dimensional array that can accommodate columns of different types. Tibble (tbl_df): Introduced by the tibble package (Müller and Wickham 2023), a tibble is a modern take on the data frame. As a component of the tidyverse (Wickham 2023c), tibbles offer improved features for better usability. Data Table (data.table): A high-performance extension of data frames by the data.table package (Barrett et al. 2024), crafted for efficient operations on large datasets. Extensible Time Series (xts): A specialized data frame offered by the xts package (Ryan and Ulrich 2024b), designed explicitly for time series data. Understanding the data structure of variables is crucial because it determines the operations and functions that can be applied to them. 4.1 Scalar Scalars in R are variables holding single objects, such as a number, a string, a logical value, or a date. # Numeric (a.k.a. Double) w &lt;- 5.5 # w is a decimal number. class(w) # Returns &quot;numeric&quot;. # Integer x &lt;- 10L # The L tells R to store x as an integer instead of a decimal number. class(x) # Returns &quot;integer&quot;. # Complex u &lt;- 3 + 4i # u is a complex number, where 3 is real and 4 is imaginary. class(u) # Returns &quot;complex&quot;. # Character y &lt;- &quot;Hello, World!&quot; # y is a character string. class(y) # Returns &quot;character&quot;. # Logical z &lt;- TRUE # z is a logical value. class(z) # Returns &quot;logical&quot;. # Date z &lt;- as.Date(&quot;2022-08-12&quot;) # z is a Date value. class(z) # Returns &quot;Date&quot;. # Time z &lt;- as.POSIXct(&quot;2022-08-12 22:30:12&quot;, tz = &quot;America/Chicago&quot;) # z is a time value. class(z) # Returns &quot;POSIXct&quot;. ## [1] &quot;numeric&quot; ## [1] &quot;integer&quot; ## [1] &quot;complex&quot; ## [1] &quot;character&quot; ## [1] &quot;logical&quot; ## [1] &quot;Date&quot; ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; Further discussions on functions pertaining to these data types can be found in Chapters 3.2 and 3.3. 4.2 Vector In R, a vector is a homogeneous sequence of elements, meaning they must all be of the same data type. As such, a vector can hold multiple numbers, but it cannot mix types, such as having both numbers and words. The function c() (for combine) can be used to create a vector. # Numeric vector numeric_vector &lt;- c(5, 2, 3, 4, 1) class(numeric_vector) # Returns &quot;numeric&quot;. # Character vector character_vector &lt;- c(&quot;Hello&quot;, &quot;World&quot;, &quot;!&quot;) class(character_vector) # Returns &quot;character&quot;. # Logical vector logical_vector &lt;- c(TRUE, FALSE, TRUE) class(logical_vector) # Returns &quot;logical&quot;. # Date vector date_vector &lt;- as.Date(c(&quot;2022-08-12&quot;, &quot;2022-08-30&quot;, &quot;2022-09-03&quot;)) class(date_vector) # Returns &quot;Date&quot;. # Unordered factor unordered_factor &lt;- factor(x = c(&quot;male&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;), levels = c(&quot;male&quot;, &quot;female&quot;, &quot;other&quot;), ordered = FALSE) class(unordered_factor) # Returns &quot;factor&quot;. # Ordered factor ordered_factor &lt;- factor(x = c(&quot;L&quot;, &quot;L&quot;, &quot;H&quot;, &quot;L&quot;, &quot;M&quot;, &quot;H&quot;, &quot;M&quot;, &quot;M&quot;, &quot;H&quot;), levels = c(&quot;L&quot;, &quot;M&quot;, &quot;H&quot;), ordered = TRUE) class(ordered_factor) # Returns &quot;ordered&quot; &quot;factor&quot;. ## [1] &quot;numeric&quot; ## [1] &quot;character&quot; ## [1] &quot;logical&quot; ## [1] &quot;Date&quot; ## [1] &quot;factor&quot; ## [1] &quot;ordered&quot; &quot;factor&quot; In R, the class() function labels both scalars and vectors by their data type, not by their dimension. Consequently, a single integer and a vector of integers are both labeled as “integer”. This perspective aligns with the idea that a scalar is essentially a vector with only one element. For an overview of vector operations, see Chapters 3.2.7 and 3.2.8. 4.3 Matrix (matrix) A matrix in R is a two-dimensional array comprising both rows and columns. Every element within the matrix must belong to the same data type, be it numeric, character, or otherwise. 4.3.1 Create a Matrix Matrices can be formed using the matrix() function, and its important inputs include: data: The elements that constitute the matrix. nrow and ncol: Specify the number of rows and columns, respectively. byrow: A logical value. If set to TRUE, the matrix is filled by rows. If FALSE (the default), it’s filled by columns. # Create a 3x3 numeric matrix, column-wise (default behavior) numeric_matrix &lt;- matrix(data = 1:9, ncol = 3) print(numeric_matrix) ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 # Create a 2x3 character matrix, row-wise character_matrix &lt;- matrix(data = letters[1:6], ncol = 3, byrow = TRUE) print(character_matrix) ## [,1] [,2] [,3] ## [1,] &quot;a&quot; &quot;b&quot; &quot;c&quot; ## [2,] &quot;d&quot; &quot;e&quot; &quot;f&quot; 4.3.2 Inspect a Matrix To gain an understanding of the type and structure of a matrix, utilize the following functions: # Investigate numeric matrix class(numeric_matrix) # Outputs &quot;matrix&quot;. is.matrix(numeric_matrix) # Outputs TRUE. typeof(numeric_matrix) # Outputs &quot;integer&quot;. ## [1] &quot;matrix&quot; &quot;array&quot; ## [1] TRUE ## [1] &quot;integer&quot; # Investigate character matrix class(character_matrix) # Outputs &quot;matrix&quot;. is.matrix(character_matrix) # Outputs TRUE. typeof(character_matrix) # Outputs &quot;character&quot;. ## [1] &quot;matrix&quot; &quot;array&quot; ## [1] TRUE ## [1] &quot;character&quot; To ascertain the dimensions of a matrix, one can employ the nrow(), ncol(), and dim() functions: # Retrieve row count (rows_count &lt;- nrow(character_matrix)) ## [1] 2 # Retrieve column count (columns_count &lt;- ncol(character_matrix)) ## [1] 3 # Obtain overall dimensions dim(character_matrix) # Returns a vector: c(rows_count, columns_count) ## [1] 2 3 The head() and tail() functions retrieve the initial and final n rows of a matrix, respectively, with a default value of n = 6. These functions are handy when the matrix has a substantial number of rows, making it difficult to display the entire matrix in the R console. # Extract the first two rows head(numeric_matrix, n = 2) ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 # Extract the last row tail(character_matrix, n = 1) ## [,1] [,2] [,3] ## [2,] &quot;d&quot; &quot;e&quot; &quot;f&quot; If a matrix has a large number of columns, making it challenging to display even with head() or tail(), RStudio offers the View() function, which presents the matrix in a spreadsheet format. Note that View() is specific to RStudio and isn’t part of base R. # Display the matrix in a spreadsheet-like viewer View(character_matrix) Moreover, the summary() function delivers a collection of statistical measures for every column in a numeric matrix. This function is invaluable as it offers insights that might not be immediately evident from looking at the data. # Generate statistical summaries for the columns of a numeric matrix summary(numeric_matrix) ## V1 V2 V3 ## Min. :1.0 Min. :4.0 Min. :7.0 ## 1st Qu.:1.5 1st Qu.:4.5 1st Qu.:7.5 ## Median :2.0 Median :5.0 Median :8.0 ## Mean :2.0 Mean :5.0 Mean :8.0 ## 3rd Qu.:2.5 3rd Qu.:5.5 3rd Qu.:8.5 ## Max. :3.0 Max. :6.0 Max. :9.0 4.3.3 Select and Modify You can select and modify specific elements, rows, or columns using indexing: # Access the element in the 2nd row and 1st column character_matrix[2, 1] ## [1] &quot;d&quot; # Access the entire second row character_matrix[2, ] ## [1] &quot;d&quot; &quot;e&quot; &quot;f&quot; # Access the second row but retain the matrix structure character_matrix[2, , drop = FALSE] ## [,1] [,2] [,3] ## [1,] &quot;d&quot; &quot;e&quot; &quot;f&quot; # Assign a specific value to an element numeric_matrix[2, 1] &lt;- 99 print(numeric_matrix) ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 99 5 8 ## [3,] 3 6 9 4.3.4 Add Labels For better clarity, rows and columns can be named: # Constructing a matrix with labeled rows and columns labeled_matrix &lt;- matrix(data = 1:4, ncol = 2, dimnames = list(c(&quot;Row1&quot;, &quot;Row2&quot;), c(&quot;Col1&quot;, &quot;Col2&quot;))) print(labeled_matrix) ## Col1 Col2 ## Row1 1 3 ## Row2 2 4 # Retrieving and modifying labels colnames(labeled_matrix) ## [1] &quot;Col1&quot; &quot;Col2&quot; rownames(labeled_matrix) &lt;- c(&quot;First_Row&quot;, &quot;Second_Row&quot;) print(labeled_matrix) ## Col1 Col2 ## First_Row 1 3 ## Second_Row 2 4 4.3.5 Handle Missing Values Matrices in R can also contain missing values, represented by NA: # Matrix with NA values mat_with_na &lt;- matrix(data = c(1, 2, NA, 4, 5, 6, 7, 8, 9), nrow = 3) print(mat_with_na) ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] NA 6 9 # Identify NA values is.na(mat_with_na) ## [,1] [,2] [,3] ## [1,] FALSE FALSE FALSE ## [2,] FALSE FALSE FALSE ## [3,] TRUE FALSE FALSE # Replace NA with a specified value mat_replaced_na &lt;- mat_with_na mat_replaced_na[is.na(mat_replaced_na)] &lt;- 0 # Remove entire rows with NA values cleaned_mat &lt;- na.omit(mat_with_na) print(cleaned_mat) ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## attr(,&quot;na.action&quot;) ## [1] 3 ## attr(,&quot;class&quot;) ## [1] &quot;omit&quot; 4.3.6 Element-Wise Operations In R, matrices support a variety of operations that act on their elements. When you use typical arithmetic operators like *, +, -, and / with matrices, they operate element-wise. This means that the operation is applied to each corresponding pair of elements from the two matrices. # Define matrix A (A &lt;- matrix(1:4, nrow = 2)) ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 # Define matrix B of equal size than A (B &lt;- matrix(5:8, nrow = 2)) ## [,1] [,2] ## [1,] 5 7 ## [2,] 6 8 # Element-wise multiplication A * B ## [,1] [,2] ## [1,] 5 21 ## [2,] 12 32 # Element-wise addition A + B ## [,1] [,2] ## [1,] 6 10 ## [2,] 8 12 Note: For element-wise operations, the matrices should have the same dimensions. If they differ, R will attempt to recycle values, which can result in unexpected behavior. 4.3.7 Linear Algebra Operations Beyond element-wise operations, R provides a variety of functions and operators to perform linear algebra operations on matrices. Transpose t(): The transpose of a matrix \\(A\\), denoted as \\(A^{\\prime}\\), is obtained by flipping it over its diagonal. This interchanges its rows and columns. (A &lt;- matrix(1:6, nrow = 2)) # Matrix A ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 t(A) # Transpose of matrix A ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 ## [3,] 5 6 Matrix Multiplication %*%: Matrix multiplication, unlike element-wise multiplication *, uses the %*% operator. It performs the standard matrix multiplication operation, where each element of the resulting matrix is the sum of the products of elements from the corresponding rows of the first matrix (A) and the corresponding columns of the second matrix (B). For example, if \\(A = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix}\\) and \\(B = \\begin{bmatrix} w &amp; x \\\\ y &amp; z \\end{bmatrix}\\), then the result of element-wise multiplication A * B will be: \\(A \\odot B = \\begin{bmatrix} a \\cdot w &amp; b \\cdot x \\\\ c \\cdot y &amp; d \\cdot z \\end{bmatrix}\\), whereas the result of matrix multiplication A %*% B will be: \\(A B = \\begin{bmatrix} (a \\cdot w + b \\cdot y) &amp; (a \\cdot x + b \\cdot z) \\\\ (c \\cdot w + d \\cdot y) &amp; (c \\cdot x + d \\cdot z) \\end{bmatrix}\\). (A &lt;- matrix(1:4, nrow = 2)) # Matrix A (B &lt;- matrix(5:8, nrow = 2)) # Matrix B A * B # Element-wise multiplication A %*% B # Matrix multiplication AB ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 ## [,1] [,2] ## [1,] 5 7 ## [2,] 6 8 ## [,1] [,2] ## [1,] 5 21 ## [2,] 12 32 ## [,1] [,2] ## [1,] 23 31 ## [2,] 34 46 Crossproduct crossprod(A, B): Computes the matrix product \\(A&#39;B\\), where \\(A&#39;\\) is the transpose of matrix \\(A\\). It’s essentially a faster version of computing the transpose of \\(A\\) followed by the regular matrix multiplication with \\(B\\). crossprod(A, B) # Equivalent to t(A) %*% B ## [,1] [,2] ## [1,] 17 23 ## [2,] 39 53 Kronecker Product kronecker(A, B): The Kronecker product, often denoted by \\(\\otimes\\), is an operation that takes two matrices and produces a block matrix. In R, it’s computed using the function kronecker(). For two matrices \\(A\\) and \\(B\\), the Kronecker product \\(A \\otimes B\\) will have the matrix \\(A\\) scaled by each element of matrix \\(B\\). For example, if \\(A = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix}\\) and \\(B = \\begin{bmatrix} w &amp; x \\\\ y &amp; z \\end{bmatrix}\\), then \\(A \\otimes B = \\begin{bmatrix} a \\cdot w &amp; a \\cdot x &amp; b \\cdot w &amp; b \\cdot x \\\\ a \\cdot y &amp; a \\cdot z &amp; b \\cdot y &amp; b \\cdot z \\\\ c \\cdot w &amp; c \\cdot x &amp; d \\cdot w &amp; d \\cdot x \\\\ c \\cdot y &amp; c \\cdot z &amp; d \\cdot y &amp; d \\cdot z \\end{bmatrix}\\). kronecker(A, B) ## [,1] [,2] [,3] [,4] ## [1,] 5 7 15 21 ## [2,] 6 8 18 24 ## [3,] 10 14 20 28 ## [4,] 12 16 24 32 Vec Operator: The vec operator is used to stack the columns of a matrix on top of each other, creating a single column vector. In R, you can achieve this by using the c() function or the as.vector() function to concatenate the columns of a matrix into a single vector, and then using the matrix() function to specify that the resulting vector is a column vector. For example, if \\(A = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix}\\), then \\(\\text{vec}(A) = \\begin{bmatrix} a \\\\ c \\\\ b \\\\ d \\end{bmatrix}\\). # Use the vec operator to stack the columns of matrix A matrix(c(A), ncol = 1) ## [,1] ## [1,] 1 ## [2,] 2 ## [3,] 3 ## [4,] 4 In this example, vec(A) stacks the columns of matrix_A on top of each other to create a single column vector. Matrix Inversion solve(): The inverse of a matrix \\(A\\) (if it exists), denoted as \\(A^{-1}\\), is a matrix \\(B\\) such that the product of \\(A\\) and \\(B\\) (in that order) is the identity matrix: \\(AB=I\\). Use solve() to compute the inverse. solve(A) # Inverse of matrix A ## [,1] [,2] ## [1,] -2 1.5 ## [2,] 1 -0.5 Solving Linear Systems with solve(A, B): If you have a matrix equation of the form \\(A X = B\\), you can solve for matrix \\(X\\) using solve(A, B). This approach is more efficient than computing solve(B) %*% A. solve(A, B) # Determines X in the equation AX = B ## [,1] [,2] ## [1,] -1 -2 ## [2,] 2 3 Matrix Exponentiation: Raising a matrix to a power (\\(n\\)) is not as straightforward as using the ^ operator. The %^% operator from the expm package (Maechler, Dutang, and Goulet 2024) facilitates matrix exponentiation in accordance with linear algebra rules. library(expm) A ^ 2 # Element-wise squaring of A ## [,1] [,2] ## [1,] 1 9 ## [2,] 4 16 A %^% 2 # Matrix squaring of A, equivalent to A %*% A ## [,1] [,2] ## [1,] 7 15 ## [2,] 10 22 A %^% 3 # Computes A %*% A %*% A ## [,1] [,2] ## [1,] 37 81 ## [2,] 54 118 Eigenvalues and Eigenvectors eigen(): The eigen() function is used to compute the eigenvalues and eigenvectors of a matrix. eigen(A) ## eigen() decomposition ## $values ## [1] 5.3722813 -0.3722813 ## ## $vectors ## [,1] [,2] ## [1,] -0.5657675 -0.9093767 ## [2,] -0.8245648 0.4159736 Determinant det(): You can compute the determinant of a matrix using the det() function. det(A) # Determinant of matrix A ## [1] -2 Matrix Rank qr(): The rank of a matrix, which is the dimension of the column space, can be determined using the qr() function. qr(A)$rank # Rank of matrix A ## [1] 2 Singular Value Decomposition svd(): Computes the singular value decomposition of a matrix. svd_result &lt;- svd(A) svd_result$u # Left singular vectors ## [,1] [,2] ## [1,] -0.5760484 -0.8174156 ## [2,] -0.8174156 0.5760484 svd_result$d # Singular values ## [1] 5.4649857 0.3659662 svd_result$v # Right singular vectors ## [,1] [,2] ## [1,] -0.4045536 0.9145143 ## [2,] -0.9145143 -0.4045536 Cholesky Decomposition chol(): Performs the Cholesky decomposition on a positive-definite square matrix. # Define positive definite symmetric matrix C (C &lt;- matrix(c(4, 2, 2, 3), nrow = 2)) ## [,1] [,2] ## [1,] 4 2 ## [2,] 2 3 # Compute Cholesky decomposition chol(C) ## [,1] [,2] ## [1,] 2 1.000000 ## [2,] 0 1.414214 LU Decomposition Matrix::lu(): The lu() function from the Matrix package (Bates, Maechler, and Jagan 2024) decomposes a matrix into a product of a lower triangular and an upper triangular matrix. library(&quot;Matrix&quot;) lu(A) ## LU factorization of Formal class &#39;denseLU&#39; [package &quot;Matrix&quot;] with 4 slots ## ..@ x : num [1:4] 2 0.5 4 1 ## ..@ perm : int [1:2] 2 2 ## ..@ Dim : int [1:2] 2 2 ## ..@ Dimnames:List of 2 ## .. ..$ : NULL ## .. ..$ : NULL QR Decomposition qr(): Decomposes a matrix into a product of an orthogonal and a triangular matrix. qr(A) ## $qr ## [,1] [,2] ## [1,] -2.2360680 -4.9193496 ## [2,] 0.8944272 -0.8944272 ## ## $rank ## [1] 2 ## ## $qraux ## [1] 1.4472136 0.8944272 ## ## $pivot ## [1] 1 2 ## ## attr(,&quot;class&quot;) ## [1] &quot;qr&quot; Condition Number kappa(): Estimates the condition number of a matrix, which provides insight into the stability of matrix computations. kappa(A) ## [1] 18.77778 Matrix Norm Matrix::norm(): The norm() function from the Matrix package (Bates, Maechler, and Jagan 2024) computes various matrix norms. library(&quot;Matrix&quot;) norm(A, type = &quot;F&quot;) # Frobenius norm ## [1] 5.477226 Remember to always ensure that the matrices you’re using with these functions meet the necessary preconditions (e.g., being square, positive-definite, etc.) required for each operation. 4.3.8 Combine Matrices Matrices in R can be joined together using various functions. The most straightforward methods are rbind() for row-wise binding and cbind() for column-wise binding: # Create matrices with identical column and row numbers x &lt;- matrix(data = 1:4, ncol = 2) y &lt;- matrix(data = 101:104, ncol = 2) # Row-wise combination of matrices with same column number rbind(x, y) ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 ## [3,] 101 103 ## [4,] 102 104 # Column-wise combination of matrices with same row number cbind(x, y) ## [,1] [,2] [,3] [,4] ## [1,] 1 3 101 103 ## [2,] 2 4 102 104 Additionally, when combining multiple matrices, the Reduce() function can be quite handy: # Create additional matrix with identical column number z &lt;- matrix(data = 201:206, ncol = 2) # Use Reduce with rbind to combine multiple matrices row-wise Reduce(f = rbind, x = list(x, y, z)) ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 ## [3,] 101 103 ## [4,] 102 104 ## [5,] 201 204 ## [6,] 202 205 ## [7,] 203 206 The Reduce() function, detailed in Chapter 3.5.10, is a prominent example of advanced higher-order functions. It facilitates consecutive operations over a list or vector. In the given context, it employs the rbind() function repeatedly to merge matrices. This technique becomes particularly beneficial when dealing with a variable or large number of matrices. For example, to compute powers of matrices, you can use the Reduce() function to create a custom operation instead of relying on the %^% function from the expm package discussed in Chapter 4.3.7: # Define a custom function for matrix exponentiation `%**%` &lt;- function(MAT, n) Reduce(f = `%*%`, x = replicate(n = n, expr = MAT, simplify = FALSE)) # Demonstrate the sixth power of matrix x (contrast with element-wise exponentiation) x%**%6 x^6 ## [,1] [,2] ## [1,] 5743 12555 ## [2,] 8370 18298 ## [,1] [,2] ## [1,] 1 729 ## [2,] 64 4096 In this revised version, the custom function %**% employs the Reduce() function to successively multiply the matrix with itself, achieving matrix exponentiation. 4.3.9 Apply Family Apply functions are instrumental in conducting operations on matrices efficiently, offering an optimized alternative to loops (see Chapter 3.5.9). While the sapply function discussed in Chapter 3.5.9 applies a function to each element of a vector, the apply function is specialized for matrices and applies a function to each row or column of that matrix. # Define a numeric matrix (A &lt;- matrix(1:9, ncol = 3)) ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 # Calculating the maximum of each row apply(X = A, MARGIN = 1, FUN = function(r) max(r)) ## [1] 7 8 9 # Calculating the sum of each column (using compact FUN notation) apply(X = A, MARGIN = 2, FUN = max) ## [1] 3 6 9 The inputs for the apply function include: X: Specifies the matrix or array. MARGIN: Designates 1 for operations across rows and 2 for columns. FUN: The function to be executed (examples include sum, mean). For those seeking expedited calculations for specific functions like sum() or mean(), R offers faster alternatives such as rowSums(), colSums(), colMeans(), and rowMeans(). # Summation across rows apply(X = A, MARGIN = 1, FUN = sum) rowSums(A) # A faster alternative ## [1] 12 15 18 ## [1] 12 15 18 # Averaging across columns apply(X = A, MARGIN = 2, FUN = mean) colMeans(A) # A faster alternative ## [1] 2 5 8 ## [1] 2 5 8 The sweep() function allows for efficient array manipulations, such as subtraction or division, across rows or columns using summary statistics. Specifically, it combines the capabilities of apply() and rep() to operate element-wise on a matrix. This is particularly useful when performing operations that require using a summary statistic—such as a mean—across all elements of a matrix. For example, one could use sweep() to demean a matrix by subtracting the mean of each column from every element in that respective column. # Define a numeric matrix (B &lt;- matrix(c(1, 4, 6, 8, 109, 107, 104, 105), ncol = 2)) ## [,1] [,2] ## [1,] 1 109 ## [2,] 4 107 ## [3,] 6 104 ## [4,] 8 105 # Calculate mean of each column (col_means &lt;- colMeans(B)) ## [1] 4.75 106.25 # Use sweep to subtract each column&#39;s mean from each element in that column sweep(x = B, MARGIN = 2, STATS = col_means, FUN = &quot;-&quot;) ## [,1] [,2] ## [1,] -3.75 2.75 ## [2,] -0.75 0.75 ## [3,] 1.25 -2.25 ## [4,] 3.25 -1.25 Manually achieving the same effect would involve a combination of apply(), rep(), and element-wise operations: # Replicate the means to match the dimensions of the original matrix (rep_means &lt;- matrix(rep(col_means, each = nrow(B)), nrow = nrow(B))) ## [,1] [,2] ## [1,] 4.75 106.25 ## [2,] 4.75 106.25 ## [3,] 4.75 106.25 ## [4,] 4.75 106.25 # Subtract the replicated means from the original matrix element-wise B - rep_means ## [,1] [,2] ## [1,] -3.75 2.75 ## [2,] -0.75 0.75 ## [3,] 1.25 -2.25 ## [4,] 3.25 -1.25 In conclusion, matrices form an integral component of R, especially for multivariate analysis and linear algebra tasks. A comprehensive understanding of matrix operations in R can substantially elevate your data manipulation and analysis capabilities. 4.4 List (list) A list (list) in R serve as an ordered collection of objects. In contrast to vectors, elements within a list are not required to be of the same type. Moreover, some list elements may store multiple sub-elements, allowing for complex nested structures. For instance, a single element of a list might itself be a matrix or another list. 4.4.1 Create a List Lists can be formed using the list() function: # Constructing a list list_sample &lt;- list(name = &quot;John&quot;, age = 25, is_student = TRUE, scores = c(88, 90, 78, 92), grade = &quot;B&quot;, relationships = list(friends = c(&quot;Marc&quot;, &quot;Victor&quot;, &quot;Peter&quot;), parents = c(&quot;Valerie&quot;, &quot;Bob&quot;), partner = NA)) print(list_sample) ## $name ## [1] &quot;John&quot; ## ## $age ## [1] 25 ## ## $is_student ## [1] TRUE ## ## $scores ## [1] 88 90 78 92 ## ## $grade ## [1] &quot;B&quot; ## ## $relationships ## $relationships$friends ## [1] &quot;Marc&quot; &quot;Victor&quot; &quot;Peter&quot; ## ## $relationships$parents ## [1] &quot;Valerie&quot; &quot;Bob&quot; ## ## $relationships$partner ## [1] NA The elements within the list (list_sample) encompass diverse data types: character strings (name and grade), numeric values (age), logical indicators (is_student), vectors (scores), and nested lists (relationships). This shows the versatility of lists in R, capable of storing various data types and structures. You can also convert other objects (e.g., vectors) into lists using the as.list() function: # Transform a numeric vector into a list converted_list &lt;- as.list(1:3) # Revert the list back to a vector unlist(converted_list) ## [1] 1 2 3 4.4.2 Inspect a List To identify the type and components of a list, the following functions can be utilized: # Identify the data structure of the list class(list_sample) # Outputs &quot;list&quot;. is.list(list_sample) # Outputs TRUE. # Identify the data type of each element within the list sapply(list_sample, class) ## [1] &quot;list&quot; ## [1] TRUE ## name age is_student scores grade ## &quot;character&quot; &quot;numeric&quot; &quot;logical&quot; &quot;numeric&quot; &quot;character&quot; ## relationships ## &quot;list&quot; To determine the total count of items in a list, employ the length() function: # Ascertain the count of items in the list length(list_sample) ## [1] 6 To gain a comprehensive view of the list’s content and structure, the str() and summary() functions are valuable: # Delve into the list&#39;s structure str(list_sample) ## List of 6 ## $ name : chr &quot;John&quot; ## $ age : num 25 ## $ is_student : logi TRUE ## $ scores : num [1:4] 88 90 78 92 ## $ grade : chr &quot;B&quot; ## $ relationships:List of 3 ## ..$ friends: chr [1:3] &quot;Marc&quot; &quot;Victor&quot; &quot;Peter&quot; ## ..$ parents: chr [1:2] &quot;Valerie&quot; &quot;Bob&quot; ## ..$ partner: logi NA # Summarize list elements summary(list_sample) ## Length Class Mode ## name 1 -none- character ## age 1 -none- numeric ## is_student 1 -none- logical ## scores 4 -none- numeric ## grade 1 -none- character ## relationships 3 -none- list 4.4.3 Select and Modify You can select and modify specific elements using indexing: # Retrieving elements by index using double square brackets list_sample[[2]] ## [1] 25 # Fetching elements by name with the dollar sign list_sample$age ## [1] 25 # Another method for name-based access using double square brackets list_sample[[&quot;age&quot;]] ## [1] 25 # Accessing multiple elements using an index via single square bracket list_sample[c(1, 3)] ## $name ## [1] &quot;John&quot; ## ## $is_student ## [1] TRUE # Nested lists: retrieving &#39;Marc&#39; from the &#39;friends&#39; sublist list_sample$relationships$friends[1] ## [1] &quot;Marc&quot; # Introducing a new element list_sample$address &lt;- &quot;123 Main St&quot; # Adjusting an existing element list_sample$name &lt;- &quot;John Travolta&quot; # Deleting an element list_sample$relationships &lt;- NULL 4.4.4 Add Labels You can examine and modify the names of the list elements: # Checking the names of the elements names(list_sample) ## [1] &quot;name&quot; &quot;age&quot; &quot;is_student&quot; &quot;scores&quot; &quot;grade&quot; ## [6] &quot;address&quot; # Modifying the element names names(list_sample) &lt;- c(&quot;Full Name&quot;, &quot;Age&quot;, &quot;Is Student&quot;, &quot;Scores&quot;, &quot;Grade&quot;, &quot;Address&quot;) 4.4.5 Combine Lists You can merge lists with the c() function: # Create two sample lists list_1 &lt;- list(name = &quot;Alice&quot;, age = 30, is_female = TRUE) list_2 &lt;- list(job = &quot;Engineer&quot;, city = &quot;New York&quot;) # Merge the lists merged_list &lt;- c(list_1, list_2) print(merged_list) ## $name ## [1] &quot;Alice&quot; ## ## $age ## [1] 30 ## ## $is_female ## [1] TRUE ## ## $job ## [1] &quot;Engineer&quot; ## ## $city ## [1] &quot;New York&quot; For more control over the merging position, use the append() function. It offers an after argument to specify where the second list should be inserted: # Merge the lists, placing the second list after the second element merged_at_position &lt;- append(list_1, list_2, after = 2) print(merged_at_position) ## $name ## [1] &quot;Alice&quot; ## ## $age ## [1] 30 ## ## $job ## [1] &quot;Engineer&quot; ## ## $city ## [1] &quot;New York&quot; ## ## $is_female ## [1] TRUE Here, the append() function inserts the elements of list_2 immediately after the second element of list_1. To reverse the order of a list’s elements, apply the rev() function: # Reverse the order of list elements reversed_list &lt;- rev(list_sample) print(reversed_list) ## $Address ## [1] &quot;123 Main St&quot; ## ## $Grade ## [1] &quot;B&quot; ## ## $Scores ## [1] 88 90 78 92 ## ## $`Is Student` ## [1] TRUE ## ## $Age ## [1] 25 ## ## $`Full Name` ## [1] &quot;John Travolta&quot; 4.4.6 Apply Family The apply functions are not limited to vectors and matrices; they can be applied to lists, offering a more concise and optimized alternative to loops. This section delves into the use of apply functions with lists. lapply: The lapply() (list-apply) function is tailored specifically for lists. It applies a given function to each element of a list and consistently returns results in list format. # Define a sample list of numeric vectors sample_list &lt;- list( a = c(2, 4, 6), b = c(3, 6, 9, 12), c = c(4, 8) ) # Use lapply to calculate the mean of each list element lapply(X = sample_list, FUN = mean) ## $a ## [1] 4 ## ## $b ## [1] 7.5 ## ## $c ## [1] 6 sapply: The sapply() function (simplify-apply) aims to simplify the output, defaulting to vectors whenever feasible. Thus, unlike the lapply() function, its return type is not always a list. # Compute the mean of each list element and return a vector if possible sapply(X = sample_list, FUN = mean) ## a b c ## 4.0 7.5 6.0 vapply: The vapply() function (value-apply) mirrors sapply() but with an additional provision: you can designate the expected return type. This feature not only ensures consistent output format but can also boost speed in specific cases. # Compute the mean (numeric) of each list element vapply(X = sample_list, FUN = mean, FUN.VALUE = numeric(1)) ## a b c ## 4.0 7.5 6.0 # Compute the mean (numeric) and type (character) of each list element vapply(X = sample_list, FUN = function(vec) list(mean_value = mean(vec), type = class(vec)), FUN.VALUE = list(mean_value = numeric(1), type = character(1))) ## a b c ## mean_value 4 7.5 6 ## type &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; In summary, lists are inherently flexible and, due to this characteristic, play a pivotal role in many R tasks, from data manipulation to functional programming. 4.5 Data Frame (data.frame) A data frame (data.frame) in R resembles a matrix in its two-dimensional, rectangular structure. However, unlike a matrix, a data frame allows each column to contain a different data type. Therefore, within each column (or vector), the elements must be homogeneous, but different columns can accommodate distinct types. Typically, when importing data into R, the default object type used is a data frame. 4.5.1 Create a Data Frame Data frames can be formed using the data.frame() function: # Create vectors for the data frame names &lt;- c(&quot;Anna&quot;, &quot;Ella&quot;, &quot;Sophia&quot;) ages &lt;- c(23, NA, 26) female &lt;- c(TRUE, TRUE, TRUE) grades &lt;- factor(c(&quot;A&quot;, &quot;B&quot;, &quot;A&quot;), levels = rev(LETTERS[1:6]), ordered = TRUE) major &lt;- c(&quot;Math&quot;, &quot;Biology&quot;, &quot;Physics&quot;) # Construct the data frame students_df &lt;- data.frame(name = names, age = ages, female = female, grade = grades, major = major) print(students_df) ## name age female grade major ## 1 Anna 23 TRUE A Math ## 2 Ella NA TRUE B Biology ## 3 Sophia 26 TRUE A Physics In the above code chunk, we observe that like a matrix, every column in a data frame possesses an identical length (3 rows). However, the first and fifth columns are composed of character data (name, major), while the second column comprises numeric data (age), the third column consists of logical values (female), and the fourth column is an ordered factor (grade). This capacity to host varied data types in separate columns is what sets data frames apart from matrices. 4.5.2 Inspect a Data Frame To identify the type and components of a data frame, the following functions can be utilized: # Identify the data structure of the data frame class(students_df) # Outputs &quot;data.frame&quot;. is.data.frame(students_df) # Outputs TRUE ## [1] &quot;data.frame&quot; ## [1] TRUE # Identify the data type of each column within the data frame sapply(students_df, class) ## $name ## [1] &quot;character&quot; ## ## $age ## [1] &quot;numeric&quot; ## ## $female ## [1] &quot;logical&quot; ## ## $grade ## [1] &quot;ordered&quot; &quot;factor&quot; ## ## $major ## [1] &quot;character&quot; Many matrix operations are compatible with data frames. For instance, to determine the dimensions of a data frame, the functions nrow(), ncol(), and dim() can be used, analogous to their applications with matrices discussed in Chapter 4.3: # Retrieve row count (rows_count &lt;- nrow(students_df)) ## [1] 3 # Retrieve column count (columns_count &lt;- ncol(students_df)) ## [1] 5 # Obtain overall dimensions dim(students_df) # Returns a vector: c(rows_count, columns_count) ## [1] 3 5 The head() and tail() functions are employed to extract the first and last n rows of a data frame, respectively, where n is defaulted to 6. They are particularly beneficial when working with large data frames, as displaying the entirety in the R console becomes unwieldy. # Retrieve the first two rows head(students_df, n = 2) ## name age female grade major ## 1 Anna 23 TRUE A Math ## 2 Ella NA TRUE B Biology # Obtain the second-last row head(tail(students_df, n = 2), n = 1) ## name age female grade major ## 2 Ella NA TRUE B Biology For data frames with many columns, which can be cumbersome to display fully using head() or tail(), RStudio’s View() function is beneficial. It displays the data frame in a spreadsheet style. It’s worth noting that the View() function is exclusive to RStudio and is not a feature of base R. # Open the data frame in RStudio&#39;s spreadsheet-style viewer View(students_df) Furthermore, the summary() and str() functions provide detailed insights into a data frame’s columns in terms of their composition and characteristics. These tools are essential for understanding facets of the data that might not be immediately apparent from a cursory glance. # Inspect the data frame&#39;s structure str(students_df) ## &#39;data.frame&#39;: 3 obs. of 5 variables: ## $ name : chr &quot;Anna&quot; &quot;Ella&quot; &quot;Sophia&quot; ## $ age : num 23 NA 26 ## $ female: logi TRUE TRUE TRUE ## $ grade : Ord.factor w/ 6 levels &quot;F&quot;&lt;&quot;E&quot;&lt;&quot;D&quot;&lt;&quot;C&quot;&lt;..: 6 5 6 ## $ major : chr &quot;Math&quot; &quot;Biology&quot; &quot;Physics&quot; # Gather a comprehensive overview of the data frame summary(students_df) ## name age female grade major ## Length:3 Min. :23.00 Mode:logical F:0 Length:3 ## Class :character 1st Qu.:23.75 TRUE:3 E:0 Class :character ## Mode :character Median :24.50 D:0 Mode :character ## Mean :24.50 C:0 ## 3rd Qu.:25.25 B:1 ## Max. :26.00 A:2 ## NA&#39;s :1 4.5.3 Select and Modify Elements within a data frame can be accessed, modified, or created using various indexing methods: # Retrieve specific cell: row 2, column 4 students_df[2, 4] ## [1] B ## Levels: F &lt; E &lt; D &lt; C &lt; B &lt; A # Obtain an entire column by its name students_df$major # Another method for name-based column access students_df[[&quot;major&quot;]] # Yet another method for name-based column access students_df[, &quot;major&quot;] ## [1] &quot;Math&quot; &quot;Biology&quot; &quot;Physics&quot; ## [1] &quot;Math&quot; &quot;Biology&quot; &quot;Physics&quot; ## [1] &quot;Math&quot; &quot;Biology&quot; &quot;Physics&quot; # Obtain an entire column while retaining the data frame structure students_df[&quot;major&quot;] ## major ## 1 Math ## 2 Biology ## 3 Physics # Access multiple columns by their names students_df[, c(&quot;name&quot;, &quot;grade&quot;)] # Another method for name-based multiple column access students_df[c(&quot;name&quot;, &quot;grade&quot;)] ## name grade ## 1 Anna A ## 2 Ella B ## 3 Sophia A ## name grade ## 1 Anna A ## 2 Ella B ## 3 Sophia A # Extract rows based on specific criteria selected_rows = students_df$age &gt; 24 &amp; students_df$grade == &quot;A&quot; students_df[selected_rows, ] ## name age female grade major ## 3 Sophia 26 TRUE A Physics # Introduce a new column (ensuring the vector&#39;s length matches the number of rows) students_df$gpa &lt;- c(3.7, 3.4, 3.9) print(students_df) ## name age female grade major gpa ## 1 Anna 23 TRUE A Math 3.7 ## 2 Ella NA TRUE B Biology 3.4 ## 3 Sophia 26 TRUE A Physics 3.9 # Delete a column students_df$female &lt;- NULL print(students_df) ## name age grade major gpa ## 1 Anna 23 A Math 3.7 ## 2 Ella NA B Biology 3.4 ## 3 Sophia 26 A Physics 3.9 The with() Function The with() function is an indirect function that simplifies operations on data frame columns by eliminating the need for constant data frame referencing. For instance, when working with a data frame like students_df, the conventional way to concatenate columns name and major would require referencing the data frame explicitly using students_df$name and students_df$major: # Traditional approach: operation with repeated data frame reference paste0(students_df$name, &quot; studies &quot;, students_df$major, &quot;.&quot;) ## [1] &quot;Anna studies Math.&quot; &quot;Ella studies Biology.&quot; ## [3] &quot;Sophia studies Physics.&quot; Using with(), you can perform the same operation without the repeated references: # Using with(): operation without repeated data frame reference with(students_df, paste0(name, &quot; studies &quot;, major, &quot;.&quot;)) ## [1] &quot;Anna studies Math.&quot; &quot;Ella studies Biology.&quot; ## [3] &quot;Sophia studies Physics.&quot; Here are some more illustrative examples: # Traditional approach: operations with repeated data frame reference sum(students_df$gpa &gt;= 3.5) / length(students_df$gpa) * 100 # Cum Laude Share students_df[students_df$age &gt; 24 &amp; students_df$grade == &quot;A&quot;, ] # Select rows # Using with(): operations without repeated data frame reference with(students_df, sum(gpa &gt;= 3.5) / length(gpa) * 100) # Cum Laude Share students_df[with(students_df, age &gt; 24 &amp; grade == &quot;A&quot;), ] # Select rows ## [1] 66.66667 ## name age grade major gpa ## 3 Sophia 26 A Physics 3.9 ## [1] 66.66667 ## name age grade major gpa ## 3 Sophia 26 A Physics 3.9 In summary, the with() function streamlines operations involving data frames by setting a temporary environment for computations. This can make the code more concise and reduce the likelihood of errors due to repeated data object references. 4.5.4 Add Labels You can examine and modify the names of the data frame rows and columns: # Names of columns and rows colnames(students_df) ## [1] &quot;name&quot; &quot;age&quot; &quot;grade&quot; &quot;major&quot; &quot;gpa&quot; rownames(students_df) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; # Change column names colnames(students_df) &lt;- c(&quot;Name&quot;, &quot;Age&quot;, &quot;Grade&quot;, &quot;Major&quot;, &quot;GPA&quot;) # Change the name of a specific column colnames(students_df)[colnames(students_df) == &quot;Major&quot;] &lt;- &quot;Field of Study&quot; students_df ## Name Age Grade Field of Study GPA ## 1 Anna 23 A Math 3.7 ## 2 Ella NA B Biology 3.4 ## 3 Sophia 26 A Physics 3.9 4.5.5 Handle Missing Values Data frames in R can also contain missing values, represented by NA: # Number of missing values in each column colSums(is.na(students_df)) ## Name Age Grade Field of Study GPA ## 0 1 0 0 0 # Remove all rows that contain missing values na.omit(students_df) ## Name Age Grade Field of Study GPA ## 1 Anna 23 A Math 3.7 ## 3 Sophia 26 A Physics 3.9 # Replace missing values in &#39;Age&#39; column with the mean age students_df$Age[is.na(students_df$Age)] &lt;- mean(students_df$Age, na.rm = TRUE) print(students_df) ## Name Age Grade Field of Study GPA ## 1 Anna 23.0 A Math 3.7 ## 2 Ella 24.5 B Biology 3.4 ## 3 Sophia 26.0 A Physics 3.9 4.5.6 Combine Data Frames When handling data in R, there are instances where combining data from different sources or merging tables is necessary. In this context, functions like merge(), rbind(), and cbind() are invaluable. Let’s explore these functions with examples: rbind(): This function allows you to concatenate data frames vertically, stacking one on top of the other. It is essential that the columns of both data frames match in both name and order: # Create data frames with identical column names (students_2022 &lt;- data.frame(Name = c(&quot;Alice&quot;, &quot;Bob&quot;), Age = c(21, 22))) (students_2023 &lt;- data.frame(Name = c(&quot;Charlie&quot;, &quot;David&quot;), Age = c(23, 24))) ## Name Age ## 1 Alice 21 ## 2 Bob 22 ## Name Age ## 1 Charlie 23 ## 2 David 24 # Combine data frames by rows rbind(students_2022, students_2023) ## Name Age ## 1 Alice 21 ## 2 Bob 22 ## 3 Charlie 23 ## 4 David 24 cbind(): This function combines data frames horizontally, side by side. It’s essential the data frames have the same number of rows, and the rows must have the same order. In the provided example, the first score of 91.1 corresponds with “Alice” from the first row, while the second score of 85.3 aligns with “Bob” from the second row: # Create a data frame with the same number of rows (scores_2022 &lt;- data.frame(Score = c(91.1, 85.3))) ## Score ## 1 91.1 ## 2 85.3 # Combine data frames by columns cbind(students_2022, scores_2022) ## Name Age Score ## 1 Alice 21 91.1 ## 2 Bob 22 85.3 merge(): The merge() function comes into play when data frames don’t necessarily have rows in the same order, but possess a shared identifier (such as “Name”). This function aligns the data frames by the shared identifier, determined using the by argument (such as by = \"Name\"): # Create a data frame with a common column &quot;Name&quot; (scores_2022 &lt;- data.frame(Name = c(&quot;Bob&quot;, &quot;Alice&quot;), Score = c(85.3, 91.1))) ## Name Score ## 1 Bob 85.3 ## 2 Alice 91.1 # Merge data frames by common column &quot;Name&quot; merge(students_2022, scores_2022, by = &quot;Name&quot;) ## Name Age Score ## 1 Alice 21 91.1 ## 2 Bob 22 85.3 merge() with Different Merge Types: Sometimes, two data frames intended for merging may have varying row counts. This can occur because one data frame contains extra entries not found in the other. In these cases, you can specify the type of merge to perform: Inner Merge (all = FALSE): Retains only the entries found in both data frames. Outer Merge (all = TRUE): Retains all unique entries from both data frames. Left Merge (all.x = TRUE): Keeps all entries from the first data frame, regardless of whether they have a match in the second data frame. Right Merge (all.y = TRUE): Keeps all entries from the second data frame. If an entry is present in one data frame but absent in the other, the missing columns for that entry are populated with NA values: # Construct a data frame with a partially shared &quot;Name&quot; column (scores_2022 &lt;- data.frame(Name = c(&quot;Bob&quot;, &quot;Eva&quot;), Score = c(85.3, 78.3))) ## Name Score ## 1 Bob 85.3 ## 2 Eva 78.3 # Inner-merge retaining common &#39;Name&#39; entries (benchmark) merge(students_2022, scores_2022, by = &quot;Name&quot;, all = FALSE) ## Name Age Score ## 1 Bob 22 85.3 # Outer-merge retaining all unique &#39;Name&#39; entries from both data frames merge(students_2022, scores_2022, by = &quot;Name&quot;, all = TRUE) ## Name Age Score ## 1 Alice 21 NA ## 2 Bob 22 85.3 ## 3 Eva NA 78.3 # Left-merge to keep all rows from &#39;students_2022&#39; merge(students_2022, scores_2022, by = &quot;Name&quot;, all.x = TRUE) ## Name Age Score ## 1 Alice 21 NA ## 2 Bob 22 85.3 # Right-merge to retain all rows from &#39;scores_2022&#39; merge(students_2022, scores_2022, by = &quot;Name&quot;, all.y = TRUE) ## Name Age Score ## 1 Bob 22 85.3 ## 2 Eva NA 78.3 do.call() + rbind(), cbind(), or merge(): When you use the indirect function do.call() in combination with functions like rbind(), cbind(), or merge() and supply a list of data frames, it effectively binds them together. For example, if you have a list of data frames and you want to bind them all together into one data frame by stacking them vertically: # Create data frames for different months jan &lt;- data.frame(Name = c(&quot;Eve&quot;, &quot;Frank&quot;), Age = c(25, 26)) feb &lt;- data.frame(Name = c(&quot;Grace&quot;, &quot;Harry&quot;), Age = c(27, 28)) mar &lt;- data.frame(Name = c(&quot;Irene&quot;), Age = c(29)) # Save all data frames in a list list_of_dfs &lt;- list(jan, feb, mar) # Combine all data frames by rows (combined_df &lt;- do.call(rbind, list_of_dfs)) ## Name Age ## 1 Eve 25 ## 2 Frank 26 ## 3 Grace 27 ## 4 Harry 28 ## 5 Irene 29 What do.call() does here is similar to iteratively binding each data frame in the list. The resulting combined_df stacks jan, feb, and mar on top of each other. The advantage of utilizing do.call(rbind, list_of_dfs) over the direct rbind(jan, feb, mar) approach is its flexibility: it can merge any number of data frames within a list without needing to know their individual names or the total count. This makes do.call() particularly valuable when the number of data frames is unpredictable, a scenario frequently encountered in data repositories. In such repositories, data might be segmented into separate data frames for each year. Using the conventional rbind() method would necessitate yearly adjustments to accommodate the varying number of data frames. In contrast, the do.call() method operates smoothly without such annual adjustments. When combining multiple data frames, always check for consistency in column names and data types to prevent unexpected results. 4.5.7 Apply Family In R, the apply functions execute repeated operations without the need for writing explicit loops. While initially tailored for matrices, they also offer great utility with data frames. This chapter elucidates the application of the apply family on data frames. Apply by Columns and Rows apply(): While initially intended for matrices, apply() can be used with data frames, treating them as lists of columns. Its core function is to process a function over rows or columns: # Sample data frame df &lt;- data.frame(a = 1:4, b = 5:8) # Mean values across columns (resultant is a vector) apply(X = df, MARGIN = 2, FUN = mean) ## a b ## 2.5 6.5 lapply(): Primarily for lists, lapply() works seamlessly with data frames, which are inherently lists of vectors (columns). It processes a function over each column, producing a list: # Derive mean for each column (output is a list) lapply(X = df, FUN = mean) ## $a ## [1] 2.5 ## ## $b ## [1] 6.5 sapply(): A more concise form of lapply(), sapply() tries to yield a simplified output, outputting vectors, matrices, or lists based on the scenario: # Determine mean for each column (output is a vector) sapply(X = df, FUN = mean) ## a b ## 2.5 6.5 vapply(): Resembling sapply(), with vapply() you declare the return value’s type, ensuring a uniform output: # Derive mean for each column while stating the output type vapply(X = df, FUN = mean, FUN.VALUE = numeric(1)) ## a b ## 2.5 6.5 mapply(): This is the “multivariate” version of apply. It applies a function across multiple input lists on an element-to-element basis: # Sample data sets df &lt;- data.frame(x = 1:3, y = 4:6, z = 9:11) # Apply function with(df, mapply(FUN = function(x, y, z) (y - z)^x, x, y, z)) ## [1] -5 25 -125 Apply by Groups When dealing with grouped or categorized data, the goal often becomes performing operations within these distinct groups rather than across entire rows or columns. Typically, a specific column in the data frame denotes these groups, categorizing each data point. R provides several functions, part of the apply family and beyond, that allow for such “group-wise” computation. split(): Before diving into the apply-by-group functions, understanding split() is crucial. It partitions a data frame based on the levels of a factor, producing a list of data frames. This facilitates the application of functions to each subset individually. # Sample grouped data frame df_grouped &lt;- data.frame(group = c(&quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;), value = c(10, 20, 30, 40, 50)) print(df_grouped) ## group value ## 1 A 10 ## 2 A 20 ## 3 B 30 ## 4 B 40 ## 5 B 50 # Splitting the data frame by &#39;group&#39; split_data &lt;- split(x = df_grouped, f = df_grouped$group) print(split_data) ## $A ## group value ## 1 A 10 ## 2 A 20 ## ## $B ## group value ## 3 B 30 ## 4 B 40 ## 5 B 50 Split-Apply-Combine strategy with split() + lapply() + do.call() + c(): The split-apply-combine strategy is foundational in R. First, data is split into subsets based on some criteria (often a factor). Next, a function is applied to each subset independently. Finally, results are combined back into a useful data structure. # Split: Dividing the data based on &#39;group&#39; (split_data &lt;- split(x = df_grouped$value, f = df_grouped$group)) ## $A ## [1] 10 20 ## ## $B ## [1] 30 40 50 # Apply: Summing the &#39;value&#39; within each split group (applied_data &lt;- lapply(X = split_data, FUN = sum)) ## $A ## [1] 30 ## ## $B ## [1] 120 # Combine: combines results into a named vector (combined_data &lt;- do.call(what = c, args = applied_data)) ## A B ## 30 120 tapply(): The tapply() function stands for “table-apply”. It quickly implements the split-apply-combine approach, using its INDEX parameter to define the grouping. # Summing &#39;value&#39; based on &#39;group&#39; tapply(X = df_grouped$value, INDEX = df_grouped$group, FUN = sum) ## A B ## 30 120 aggregate(): The aggregate() function employs the split-apply-combine approach, returning a data frame that combines group names with computed statistics for each group. # Using aggregate() to compute the sum for each group df_agg &lt;- aggregate(x = value ~ group, data = df_grouped, FUN = sum) # Alternative: Using aggregate() with lists df_agg &lt;- aggregate(x = list(value = df_grouped$value), by = list(group = df_grouped$group), FUN = sum) print(df_agg) ## group value ## 1 A 30 ## 2 B 120 The formula x = value ~ group indicates that the function should summarize the value variable based on the group variable. The data = df_grouped argument specifies the dataset to use. The FUN = sum argument tells the function to compute the sum for each group. Alternatively, using lists: the x argument takes a list that specifies the variables to be aggregated, and the by argument provides a list that determines how the dataset is grouped. The result is a summary data frame where each unique level of the group variable has a corresponding sum of the value variable. aggregate() with multiple variables: The aggregate() function can handle scenarios that involve multiple group and value variables. When you have a dataset with more than one group and value column, you can use the aggregate() function to group by multiple columns and compute summaries over several value columns simultaneously. # Sample dataset with multiple groups and values df_advanced &lt;- data.frame( group1 = c(&quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;), group2 = c(&quot;X&quot;, &quot;X&quot;, &quot;Y&quot;, &quot;X&quot;, &quot;Y&quot;, &quot;Y&quot;), v1 = c(10, 20, 30, 40, 50, 60), v2 = c(5, 10, 15, 20, 25, 30) ) # Using aggregate() to compute the mean for each group1 &amp; group2 combination df_agg &lt;- aggregate(x = cbind(v1, v2) ~ group1 + group2, data = df_advanced, FUN = mean) # Alternative: Using aggregate() with lists df_agg &lt;- aggregate(x = df_advanced[c(&quot;v1&quot;, &quot;v2&quot;)], by = df_advanced[c(&quot;group1&quot;, &quot;group2&quot;)], FUN = sum) print(df_agg) ## group1 group2 v1 v2 ## 1 A X 30 15 ## 2 B X 40 20 ## 3 A Y 30 15 ## 4 B Y 110 55 The formula cbind(v1, v2) ~ group1 + group2 instructs R to group by both group1 and group2, and then summarize both v1 and v2. The data = df_advanced argument specifies the dataset to use. The FUN = mean tells the function to calculate the mean for each grouped set of data. In the list-based approach, the x argument provides the data columns to aggregate, while the by argument specifies the grouping columns. The resulting output showcases the average values for v1 and v2 for each unique combination of group1 and group2. This approach offers a concise way to produce summaries for complex datasets with multiple grouping variables. In essence, group-wise computation is a cornerstone in many analyses. Knowing how to efficiently split, process, and combine data is pivotal. Functions in R, especially within the apply family, provide the tools to handle such computations with ease, making the data analysis process streamlined and robust. 4.5.8 Reshape Data Frames Working with different data formats is essential for a seamless data analysis experience. In R, the two primary data structures are the wide and long formats: Wide Format: In this configuration, each row corresponds to a unique observation, with all its associated measurements or characteristics spread across distinct columns. To illustrate, consider an individual named John. In the wide format, John would occupy a single row. Attributes such as age, gender, and IQ would each have their own columns. For John, these might be represented as separate columns with values 38, Male, and 120, respectively. Table 4.1: Wide Format Example Name Age Gender IQ John 38 Male 120 Marie 29 Female 121 Long Format: Contrary to the wide format, in the long format, each row stands for just one characteristic or measurement of an observation. As a result, a single entity might be represented across several rows. Using John as an example again, he would be spread across three rows, one for each attribute. The dataset would typically have three columns: one indicating the individual (John, John, John), one specifying the type of attribute (age, gender, IQ), and the last one containing the corresponding values (38, Male, 120). Table 4.2: Long Format Example Name Attribute Value John Age 38 John Gender Male John IQ 120 Marie Age 29 Marie Gender Female Marie IQ 121 The reshape() function offers a robust method to toggle between these two formats. # Sample data in wide format wide_data &lt;- data.frame( name = c(&quot;John&quot;, &quot;Marie&quot;), age = c(38, 29), gender = c(&quot;Male&quot;, &quot;Female&quot;), iq = c(120, 121) ) print(wide_data) ## name age gender iq ## 1 John 38 Male 120 ## 2 Marie 29 Female 121 # Convert to long format long_data &lt;- reshape( data = wide_data, direction = &quot;long&quot;, varying = list(attribute = c(&quot;age&quot;, &quot;gender&quot;, &quot;iq&quot;)), times = c(&quot;Age&quot;, &quot;Gender&quot;, &quot;IQ&quot;), timevar = &quot;attribute&quot;, v.names = &quot;value&quot;, idvar = &quot;name&quot; ) rownames(long_data) &lt;- NULL print(long_data) ## name attribute value ## 1 John Age 38 ## 2 Marie Age 29 ## 3 John Gender Male ## 4 Marie Gender Female ## 5 John IQ 120 ## 6 Marie IQ 121 # Convert long format back to wide format reshaped_wide_data &lt;- reshape( data = long_data, direction = &quot;wide&quot;, timevar = &quot;attribute&quot;, v.names = &quot;value&quot;, idvar = &quot;name&quot;, sep = &quot;_&quot; ) print(reshaped_wide_data) ## name value_Age value_Gender value_IQ ## 1 John 38 Male 120 ## 2 Marie 29 Female 121 In the reshape function: data: This specifies the data frame you intend to reshape. direction: Determines if you’re going from ‘wide’ to ‘long’ format or vice-versa. varying: Lists columns that you’ll be reshaping. times: This denotes unique times or measurements in the reshaped data. timevar: This names the column in the reshaped data that will contain the unique times identifiers. v.names: The name of the column in the reshaped data that will contain the data values. idvar: Specifies the identifier variable, which will remain the same between reshaped versions. sep: Used in converting from long to wide format, it defines the separator between the identifier and the measurement variables. For beginners, the reshape() function might seem intricate due to its numerous parameters. Yet, with practice, it becomes a valuable tool in a data scientist’s toolkit. Always refer to R’s built-in documentation with ?reshape for additional details, or read the vignette available through vignette(\"reshape\"). 4.6 Tibble (tbl_df) A tibble (tbl_df) refines the conventional data frame, offering a more user-friendly alternative. It is part of the tibble package by Müller and Wickham (2023), which is in the tidyverse collection of R packages (Wickham 2023c). For an introduction into the Tidyverse, consult Chapter 3.6.7. To use tibbles, you need to install the tibble package by executing install.packages(\"tibble\") in your console. Don’t forget to include library(\"tibble\") at the beginning of your R script. If you’re already using the Tidyverse suite, a simple library(\"tidyverse\") will suffice, as it internally loads the tibble package. Despite their modern touch, tibbles remain data frames at their core. This duality is evident when the class() function, applied to a tibble, returns both \"tbl_df\" and \"data.frame\". Therefore, the operations and functions elucidated in the data frame section @ref(data.frame) are entirely compatible with tibbles. Building on this compatibility, this chapter explores the Tidyverse way of manipulating tibbles, leveraging user-friendly functions from the dplyr package by Wickham et al. (2023), rather than solely relying on base functions. The dplyr package is one of the core packages of the tidyverse collection and specializes in data manipulation. The package offers a series of verbs (functions) for the most common data manipulation tasks. Let’s explore some of these functions and see how they can be applied to tibbles. 4.6.1 Create a Tibble Tibbles can be formed using the tibble() function from the tibble package: # Load tibble package library(&quot;tibble&quot;) # Create vectors for the tibble names &lt;- c(&quot;Anna&quot;, &quot;Ella&quot;, &quot;Sophia&quot;) ages &lt;- c(23, NA, 26) female &lt;- c(TRUE, TRUE, TRUE) grades &lt;- factor(c(&quot;A&quot;, &quot;B&quot;, &quot;A&quot;), levels = rev(LETTERS[1:6]), ordered = TRUE) major &lt;- c(&quot;Math&quot;, &quot;Biology&quot;, &quot;Physics&quot;) # Construct the tibble students_tbl &lt;- tibble(name = names, age = ages, female = female, grade = grades, major = major) print(students_tbl) ## # A tibble: 3 × 5 ## name age female grade major ## &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;ord&gt; &lt;chr&gt; ## 1 Anna 23 TRUE A Math ## 2 Ella NA TRUE B Biology ## 3 Sophia 26 TRUE A Physics Moreover, tibbles support incremental construction by referencing a previously established tibble and appending additional columns: # Initiate the tibble students_tbl &lt;- tibble(name = names, age = ages, female = female) # Expand the tibble by adding columns students_tbl &lt;- tibble(students_tbl, grade = grades, major = major) print(students_tbl) ## # A tibble: 3 × 5 ## name age female grade major ## &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;ord&gt; &lt;chr&gt; ## 1 Anna 23 TRUE A Math ## 2 Ella NA TRUE B Biology ## 3 Sophia 26 TRUE A Physics Unlike regular data frames, tibbles allow non-standard column names. You can use special characters or numbers as column names. Here’s an example: # Construct a tibble with unconventional column names tibble(`:)` = &quot;smile&quot;, ` ` = &quot;space&quot;, `2000` = &quot;number&quot;) ## # A tibble: 1 × 3 ## `:)` ` ` `2000` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 smile space number Another way to create a tibble is with the tribble() function. It allows you to define column headings using formulas starting with ~ and separate entries with commas. Here’s an example: tribble( ~x, ~y, ~z, &quot;a&quot;, 2, 3.6, &quot;b&quot;, 1, 8.5 ) ## # A tibble: 2 × 3 ## x y z ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 a 2 3.6 ## 2 b 1 8.5 4.6.2 Inspect a Tibble To discern the type and components of a tibble, the following functions can be employed: # Identify the data structure of the tibble class(students_tbl) # Outputs &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot;. is_tibble(students_tbl) # Outputs TRUE. is.data.frame(students_tbl) # Outputs TRUE. is.matrix(students_tbl) # Outputs FALSE. ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; ## [1] TRUE ## [1] TRUE ## [1] FALSE # Identify the data type of each column within the tibble sapply(students_tbl, class) ## $name ## [1] &quot;character&quot; ## ## $age ## [1] &quot;numeric&quot; ## ## $female ## [1] &quot;logical&quot; ## ## $grade ## [1] &quot;ordered&quot; &quot;factor&quot; ## ## $major ## [1] &quot;character&quot; Inspecting tibbles can be achieved using functions like nrow(), ncol(), dim(), head(), tail(), str(), summarize(), and View(), much like their use for data frames as outlined in Chapter 4.5.2. However, when printing a tibble in R, the default behavior differs from that of a data frame. Only the first 10 rows and the columns that fit on the screen are displayed, accompanied by a message noting the additional rows and columns present. This makes the head() and tail() functions less necessary, as printing a tibble already provides a concise overview. For a more concise structural overview of a tibble, instead of using str(), one might prefer the glimpse() function from the tibble package: # Examine the tibble&#39;s structure glimpse(students_tbl) ## Rows: 3 ## Columns: 5 ## $ name &lt;chr&gt; &quot;Anna&quot;, &quot;Ella&quot;, &quot;Sophia&quot; ## $ age &lt;dbl&gt; 23, NA, 26 ## $ female &lt;lgl&gt; TRUE, TRUE, TRUE ## $ grade &lt;ord&gt; A, B, A ## $ major &lt;chr&gt; &quot;Math&quot;, &quot;Biology&quot;, &quot;Physics&quot; 4.6.3 Select and Modify The dplyr package (Wickham et al. 2023) from the Tidyverse provides an array of functions tailored for data manipulation with tibbles. # Load the dplyr package library(&quot;dplyr&quot;) Select and Order Columns Using select() to retrieve specific columns: # Base R method students_tbl[c(&quot;name&quot;, &quot;grade&quot;)] ## # A tibble: 3 × 2 ## name grade ## &lt;chr&gt; &lt;ord&gt; ## 1 Anna A ## 2 Ella B ## 3 Sophia A # With dplyr select(students_tbl, name, grade) ## # A tibble: 3 × 2 ## name grade ## &lt;chr&gt; &lt;ord&gt; ## 1 Anna A ## 2 Ella B ## 3 Sophia A Using select() to order columns alphabetically: # Base R method students_tbl[sort(colnames(students_tbl))] ## # A tibble: 3 × 5 ## age female grade major name ## &lt;dbl&gt; &lt;lgl&gt; &lt;ord&gt; &lt;chr&gt; &lt;chr&gt; ## 1 23 TRUE A Math Anna ## 2 NA TRUE B Biology Ella ## 3 26 TRUE A Physics Sophia # With dplyr select(students_tbl, sort(colnames(students_tbl))) ## # A tibble: 3 × 5 ## age female grade major name ## &lt;dbl&gt; &lt;lgl&gt; &lt;ord&gt; &lt;chr&gt; &lt;chr&gt; ## 1 23 TRUE A Math Anna ## 2 NA TRUE B Biology Ella ## 3 26 TRUE A Physics Sophia Filter and Order Rows Use filter() to extract rows based on conditions: # Base R method students_tbl[students_tbl$age &gt; 24 &amp; students_tbl$grade == &quot;A&quot;, ] ## # A tibble: 1 × 5 ## name age female grade major ## &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;ord&gt; &lt;chr&gt; ## 1 Sophia 26 TRUE A Physics # With dplyr filter(students_tbl, age &gt; 24 &amp; grade == &quot;A&quot;) ## # A tibble: 1 × 5 ## name age female grade major ## &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;ord&gt; &lt;chr&gt; ## 1 Sophia 26 TRUE A Physics The rows can be ordered with the arrange() function: # Base R method students_tbl[order(students_tbl$age, decreasing = TRUE), ] ## # A tibble: 3 × 5 ## name age female grade major ## &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;ord&gt; &lt;chr&gt; ## 1 Sophia 26 TRUE A Physics ## 2 Anna 23 TRUE A Math ## 3 Ella NA TRUE B Biology # With dplyr arrange(students_tbl, desc(age)) ## # A tibble: 3 × 5 ## name age female grade major ## &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;ord&gt; &lt;chr&gt; ## 1 Sophia 26 TRUE A Physics ## 2 Anna 23 TRUE A Math ## 3 Ella NA TRUE B Biology In the dplyr approach, the desc() function is utilized to order values in descending order. Add and Remove Columns To create a new column, employ mutate(): # Base R method students_tbl$gpa &lt;- c(3.7, 3.4, 3.9) # With dplyr students_tbl &lt;- mutate(students_tbl, gpa = c(3.7, 3.4, 3.9)) print(students_tbl) ## # A tibble: 3 × 6 ## name age female grade major gpa ## &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;ord&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Anna 23 TRUE A Math 3.7 ## 2 Ella NA TRUE B Biology 3.4 ## 3 Sophia 26 TRUE A Physics 3.9 Columns can be removed using select(): # Base R method students_tbl$female &lt;- NULL # With dplyr students_tbl &lt;- select(students_tbl, -female) print(students_tbl) ## # A tibble: 3 × 5 ## name age grade major gpa ## &lt;chr&gt; &lt;dbl&gt; &lt;ord&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Anna 23 A Math 3.7 ## 2 Ella NA B Biology 3.4 ## 3 Sophia 26 A Physics 3.9 Manipulate Columns To construct new values for each row based on existing columns, you can utilize the reframe() function: # Base R method with(students_tbl, paste0(name, &quot; studies &quot;, major, &quot;.&quot;)) ## [1] &quot;Anna studies Math.&quot; &quot;Ella studies Biology.&quot; ## [3] &quot;Sophia studies Physics.&quot; # With dplyr reframe(students_tbl, paste0(name, &quot; studies &quot;, major, &quot;.&quot;)) ## # A tibble: 3 × 1 ## `paste0(name, &quot; studies &quot;, major, &quot;.&quot;)` ## &lt;chr&gt; ## 1 Anna studies Math. ## 2 Ella studies Biology. ## 3 Sophia studies Physics. The key difference between mutate() and reframe() is that mutate() returns the full tibble, while reframe() only returns the newly computed column: # Using mutate() for comparison mutate(students_tbl, new_description = paste0(name, &quot; studies &quot;, major, &quot;.&quot;)) ## # A tibble: 3 × 6 ## name age grade major gpa new_description ## &lt;chr&gt; &lt;dbl&gt; &lt;ord&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Anna 23 A Math 3.7 Anna studies Math. ## 2 Ella NA B Biology 3.4 Ella studies Biology. ## 3 Sophia 26 A Physics 3.9 Sophia studies Physics. To compute summary statistics over all rows, the summarize() or summarise() function can be employed: # Base R method with(students_tbl, mean(gpa)) ## [1] 3.666667 # With dplyr summarize(students_tbl, mean(gpa)) ## # A tibble: 1 × 1 ## `mean(gpa)` ## &lt;dbl&gt; ## 1 3.67 The difference between mutate() and summarize() is that mutate() keeps the tibble format, thus, in this case, mutate() creates a new column with the average values repeated multiple times: # Using mutate() instead of summarize() mutate(students_tbl, mean_gpa = mean(gpa)) ## # A tibble: 3 × 6 ## name age grade major gpa mean_gpa ## &lt;chr&gt; &lt;dbl&gt; &lt;ord&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Anna 23 A Math 3.7 3.67 ## 2 Ella NA B Biology 3.4 3.67 ## 3 Sophia 26 A Physics 3.9 3.67 Note that while base R returns a scalar, reframe() and summarize() keep the tibble structure. To extract just the value, use the pull() function: # Pull to retrieve a vector pull(summarize(students_tbl, mean_gpa = mean(gpa)), mean_gpa) ## [1] 3.666667 Use the Pipe Operator The Tidyverse introduces the pipe operator, %&gt;%. This operator, detailed in Chapter 3.6.7, allows for sequential execution of functions: # Without the pipe operator mutate(select(students_tbl, -age, -grade), is_cum_laude = gpa &gt;= 3.5) ## # A tibble: 3 × 4 ## name major gpa is_cum_laude ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 Anna Math 3.7 TRUE ## 2 Ella Biology 3.4 FALSE ## 3 Sophia Physics 3.9 TRUE # Employ the pipe operator students_tbl %&gt;% select(-age, -grade) %&gt;% mutate(is_cum_laude = gpa &gt;= 3.5) ## # A tibble: 3 × 4 ## name major gpa is_cum_laude ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 Anna Math 3.7 TRUE ## 2 Ella Biology 3.4 FALSE ## 3 Sophia Physics 3.9 TRUE With the pipe operator, operations become more readable and straightforward, promoting cleaner code structuring. 4.6.4 Add Labels To modify the names of the columns in a tibble, you can use the rename() function from the dplyr package: # Base R method colnames(students_tbl) &lt;- c(&quot;Name&quot;, &quot;Age&quot;, &quot;Grade&quot;, &quot;Major&quot;, &quot;GPA&quot;) # With dplyr students_tbl &lt;- students_tbl %&gt;% rename(Name = name, Age = age, Grade = grade, Major = major, GPA = gpa) print(students_tbl) ## # A tibble: 3 × 5 ## Name Age Grade Major GPA ## &lt;chr&gt; &lt;dbl&gt; &lt;ord&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Anna 23 A Math 3.7 ## 2 Ella NA B Biology 3.4 ## 3 Sophia 26 A Physics 3.9 The rename() function is particularly useful when modifying only a subset of column: # Base R method colnames(students_tbl)[colnames(students_tbl) == &quot;Major&quot;] &lt;- &quot;Field of Study&quot; # With dplyr students_tbl &lt;- students_tbl %&gt;% rename(`Field of Study` = Major) print(students_tbl) ## # A tibble: 3 × 5 ## Name Age Grade `Field of Study` GPA ## &lt;chr&gt; &lt;dbl&gt; &lt;ord&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Anna 23 A Math 3.7 ## 2 Ella NA B Biology 3.4 ## 3 Sophia 26 A Physics 3.9 4.6.5 Handle Missing Values Tibbles can contain missing values (NA) just like data frames. To manage these missing values, you can use the drop_na() function from the tidyr package by Wickham, Vaughan, and Girlich (2024), part of the Tidyverse. # Load the tidyr package library(&quot;tidyr&quot;) # Remove all rows containing missing values # --&gt; Base R method na.omit(students_tbl) # --&gt; With tidyr students_tbl %&gt;% drop_na() ## # A tibble: 2 × 5 ## Name Age Grade `Field of Study` GPA ## &lt;chr&gt; &lt;dbl&gt; &lt;ord&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Anna 23 A Math 3.7 ## 2 Sophia 26 A Physics 3.9 The drop_na() function offers the flexibility to target specific columns for NA checking, e.g. drop_na(Age), ensuring that less critical columns with numerous NA values don’t substantially reduce the dataset. For more details, consult the function documentation by executing ?drop_na. To replace missing values, you can use functions from the dplyr package: # Replace missing &#39;Age&#39; values with mean age # --&gt; Base R method students_tbl$Age[is.na(students_tbl$Age)] &lt;- mean(students_tbl$Age, na.rm = TRUE) # --&gt; With dplyr students_tbl &lt;- students_tbl %&gt;% mutate(mean_age = mean(students_tbl$Age, na.rm = TRUE)) %&gt;% mutate(Age = ifelse(is.na(Age), mean_age, Age)) %&gt;% select(-mean_age) print(students_tbl) ## # A tibble: 3 × 5 ## Name Age Grade `Field of Study` GPA ## &lt;chr&gt; &lt;dbl&gt; &lt;ord&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Anna 23 A Math 3.7 ## 2 Ella 24.5 B Biology 3.4 ## 3 Sophia 26 A Physics 3.9 Here, the mutate() function replaces NA values in the ‘Age’ column with the mean age, calculated with na.rm = TRUE to ignore NAs in the calculation. 4.6.6 Combine Tibbles When dealing with multiple tibbles, the Tidyverse provides powerful tools for combination and merging. Specifically, the dplyr package by Wickham et al. (2023) offers functions such as bind_rows(), bind_cols(), and various *_join() methods like inner_join(). Additionally, the purrr package by Wickham and Henry (2023) introduces the map_*() functions, including map_dfr(). Here’s how they work: bind_rows(): This function is the Tidyverse equivalent of rbind() and stacks tibbles vertically. It requires that the columns have matching names: # Create tibbles with identical column names (students_2022 &lt;- tibble(Name = c(&quot;Alice&quot;, &quot;Bob&quot;), Age = c(21, 22))) (students_2023 &lt;- tibble(Name = c(&quot;Charlie&quot;, &quot;David&quot;), Age = c(23, 24))) ## # A tibble: 2 × 2 ## Name Age ## &lt;chr&gt; &lt;dbl&gt; ## 1 Alice 21 ## 2 Bob 22 ## # A tibble: 2 × 2 ## Name Age ## &lt;chr&gt; &lt;dbl&gt; ## 1 Charlie 23 ## 2 David 24 # Combine tibbles by rows bind_rows(students_2022, students_2023) ## # A tibble: 4 × 2 ## Name Age ## &lt;chr&gt; &lt;dbl&gt; ## 1 Alice 21 ## 2 Bob 22 ## 3 Charlie 23 ## 4 David 24 bind_cols(): This function is the Tidyverse equivalent of cbind() and aligns tibbles horizontally. Ensure the tibbles have the same number of rows: # Create a tibble with the same number of rows (scores_2022 &lt;- tibble(Score = c(91.1, 85.3))) ## # A tibble: 2 × 1 ## Score ## &lt;dbl&gt; ## 1 91.1 ## 2 85.3 # Combine tibbles by columns bind_cols(students_2022, scores_2022) ## # A tibble: 2 × 3 ## Name Age Score ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alice 21 91.1 ## 2 Bob 22 85.3 inner_join(): Similar to merge(), inner_join() matches tibbles based on a shared identifier specified by the by argument: # Create a tibble with a common column &quot;Name&quot; (scores_2022 &lt;- tibble(Name = c(&quot;Bob&quot;, &quot;Alice&quot;), Score = c(85.3, 91.1))) ## # A tibble: 2 × 2 ## Name Score ## &lt;chr&gt; &lt;dbl&gt; ## 1 Bob 85.3 ## 2 Alice 91.1 # Merge tibbles by common column &quot;Name&quot; inner_join(students_2022, scores_2022, by = &quot;Name&quot;) ## # A tibble: 2 × 3 ## Name Age Score ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alice 21 91.1 ## 2 Bob 22 85.3 Different Types of Joins with dplyr: With dplyr, there are specialized functions for each type of merge/join operation, making it more intuitive: Inner Join: Retained entries are only those found in both tibbles. Full (Outer) Join: Keeps all entries from both tibbles. Left Join: Keeps all entries from the first tibble. Right Join: Retains all entries from the second tibble. When an entry exists in one tibble but not the other, the missing columns for that entry will be filled with NA values: # Construct a tibble with a partially shared &quot;Name&quot; column (scores_2022 &lt;- tibble(Name = c(&quot;Bob&quot;, &quot;Eva&quot;), Score = c(85.3, 78.3))) ## # A tibble: 2 × 2 ## Name Score ## &lt;chr&gt; &lt;dbl&gt; ## 1 Bob 85.3 ## 2 Eva 78.3 # Inner join to keep common &#39;Name&#39; entries inner_join(students_2022, scores_2022, by = &quot;Name&quot;) ## # A tibble: 1 × 3 ## Name Age Score ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Bob 22 85.3 # Full (outer) join to retain all unique &#39;Name&#39; entries from both tibbles full_join(students_2022, scores_2022, by = &quot;Name&quot;) ## # A tibble: 3 × 3 ## Name Age Score ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alice 21 NA ## 2 Bob 22 85.3 ## 3 Eva NA 78.3 # Left join to keep all rows from &#39;students_2022&#39; left_join(students_2022, scores_2022, by = &quot;Name&quot;) ## # A tibble: 2 × 3 ## Name Age Score ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alice 21 NA ## 2 Bob 22 85.3 # Right join to keep all rows from &#39;scores_2022&#39; right_join(students_2022, scores_2022, by = &quot;Name&quot;) ## # A tibble: 2 × 3 ## Name Age Score ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Bob 22 85.3 ## 2 Eva NA 78.3 map_dfr() and map_dfc(): For combining multiple tibbles stored in a list, the purrr package provides the map_dfr() and map_dfc() functions, which are more flexible alternatives to using do.call() in combination with rbind(), or cbind(): # Load purrr package library(&quot;purrr&quot;) # Create tibbles for different months jan &lt;- tibble(Name = c(&quot;Eve&quot;, &quot;Frank&quot;), Age = c(25, 26)) feb &lt;- tibble(Name = c(&quot;Grace&quot;, &quot;Harry&quot;), Age = c(27, 28)) mar &lt;- tibble(Name = c(&quot;Irene&quot;), Age = c(29)) # Store all tibbles in a list list_of_tibbles &lt;- list(jan, feb, mar) # Combine all tibbles by rows (combined_tibble &lt;- map_dfr(.x = list_of_tibbles, .f = ~.x)) ## # A tibble: 5 × 2 ## Name Age ## &lt;chr&gt; &lt;dbl&gt; ## 1 Eve 25 ## 2 Frank 26 ## 3 Grace 27 ## 4 Harry 28 ## 5 Irene 29 With this approach, map_dfr() sequentially combines each tibble in the list by rows. This method is particularly useful when the number of tibbles to be combined is unknown or variable, offering more flexibility than calling functions like bind_rows(jan, feb, mar) directly. The map_dfc() function operates similarly but combines the tibbles column-wise instead of row-wise. Within the map_*() functions, the argument .f = ~.x is a shorthand formula notation which translates to .f = function(x) x. This means that for each tibble in the list, the function takes the tibble as it is and returns it without any transformation, making it an efficient way to combine them row- or column-wise. reduce() + *_join(): When tasked with merging multiple tibbles stored in a list based on a common column, the combination of purrr::reduce() and dplyr::*_join() functions such as dplyr::full_join() offers a more flexible alternative to using do.call() in combination with merge(): # Construct a tibble with a partially shared &quot;Name&quot; column (subject_2022 &lt;- tibble(Name = c(&quot;Bob&quot;, &quot;Pete&quot;), Subject = c(&quot;History&quot;, &quot;Economics&quot;))) ## # A tibble: 2 × 2 ## Name Subject ## &lt;chr&gt; &lt;chr&gt; ## 1 Bob History ## 2 Pete Economics # A list comprising the tibbles to be merged list_of_tibbles &lt;- list(students_2022, scores_2022, subject_2022) # Execute an outer-merge using reduce() and full_join() (merged_tibble &lt;- reduce(list_of_tibbles, .f = ~full_join(.x, .y, by = &quot;Name&quot;))) ## # A tibble: 4 × 4 ## Name Age Score Subject ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Alice 21 NA &lt;NA&gt; ## 2 Bob 22 85.3 History ## 3 Eva NA 78.3 &lt;NA&gt; ## 4 Pete NA NA Economics 4.6.7 Apply Family In R, the Tidyverse collection of packages offers functions for manipulating tibbles in a more readable and consistent manner. This chapter focuses on these functionalities that replace the traditional apply family used with data frames. Apply by Columns and Rows map(): Instead of apply(), use the purrr::map() function to process each column of a tibble: # Sample tibble tb &lt;- tibble(a = 1:4, b = 5:8) # Mean values across columns (resultant is a list) map(tb, mean) ## $a ## [1] 2.5 ## ## $b ## [1] 6.5 map_dbl(): Similar to lapply(), the purrr::map_dbl() function processes each column and returns a double vector: # Mean values for each column (output is a double vector) map_dbl(tb, mean) ## a b ## 2.5 6.5 Apply by Groups When dealing with grouped data, you can perform operations within these distinct groups using Tidyverse functions. group_by() + summarize(): This pair of functions from dplyr replaces the need for split() and lapply(): # Sample grouped tibble tb_grouped &lt;- tibble(group = c(&quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;), value = c(10, 20, 30, 40, 50)) # Summarizing &#39;value&#39; based on &#39;group&#39; tb_grouped %&gt;% group_by(group) %&gt;% summarize(sum_value = sum(value)) ## # A tibble: 2 × 2 ## group sum_value ## &lt;chr&gt; &lt;dbl&gt; ## 1 A 30 ## 2 B 120 group_by() + summarize() for Multiple Groups and Variables: The dplyr functions can handle multiple group and value variables: # Sample tibble with multiple groups and values tb_advanced &lt;- tibble( group1 = c(&quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;), group2 = c(&quot;X&quot;, &quot;X&quot;, &quot;Y&quot;, &quot;X&quot;, &quot;Y&quot;, &quot;Y&quot;), v1 = c(10, 20, 30, 40, 50, 60), v2 = c(5, 10, 15, 20, 25, 30) ) # Summarize for each group1 &amp; group2 combination tb_advanced %&gt;% group_by(group1, group2) %&gt;% summarize(mean_v1 = mean(v1), mean_v2 = mean(v2)) ## # A tibble: 4 × 4 ## # Groups: group1 [2] ## group1 group2 mean_v1 mean_v2 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A X 15 7.5 ## 2 A Y 30 15 ## 3 B X 40 20 ## 4 B Y 55 27.5 4.6.8 Reshape Tibbles As discussed in Chapter 4.5.8, data can be organized in either a wide or long format: Wide Format: The wide format has each row representing a unique observation, with various attributes detailed across columns. Long Format: Contrary to the wide format, the long format has each row representing a single attribute or measure of an observation, resulting in multiple rows for each observation. The tidyr package by Wickham, Vaughan, and Girlich (2024) provides two key functions for reshaping data: pivot_longer() and pivot_wider(): # Load tidyr package library(&quot;tidyr&quot;) # Create tibble using wide format wide_data &lt;- tibble( Name = c(&quot;John&quot;, &quot;Marie&quot;), Age = c(38, 29), Gender = c(&quot;Male&quot;, &quot;Female&quot;), IQ = c(120, 121) ) print(wide_data) ## # A tibble: 2 × 4 ## Name Age Gender IQ ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 John 38 Male 120 ## 2 Marie 29 Female 121 # Coerce all varying attributes to character type wide_data &lt;- wide_data %&gt;% mutate(across(where(is.numeric), as.character)) # Pivot to long format long_data &lt;- wide_data %&gt;% pivot_longer(cols = -Name, names_to = &quot;Attribute&quot;, values_to = &quot;Value&quot;) print(long_data) ## # A tibble: 6 × 3 ## Name Attribute Value ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 John Age 38 ## 2 John Gender Male ## 3 John IQ 120 ## 4 Marie Age 29 ## 5 Marie Gender Female ## 6 Marie IQ 121 # Convert long format back to wide format reshaped_wide_data &lt;- long_data %&gt;% pivot_wider(names_from = Attribute, values_from = Value) print(reshaped_wide_data) ## # A tibble: 2 × 4 ## Name Age Gender IQ ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 John 38 Male 120 ## 2 Marie 29 Female 121 In pivot_longer() and pivot_wider(): cols: Specifies the columns to pivot into longer or wider format. names_to: Names the column in the reshaped data containing the unique identifiers. values_to: Names the column in the reshaped data that will hold the data values. names_from: Indicates the column that will provide names in the widened data. values_from: Indicates the column that will provide values in the widened data. For those new to tibbles and tidyverse, these functions may initially seem complicated. However, they offer a straightforward way to reshape data in R. You can always refer to the official documentation (?pivot_longer, ?pivot_wider) or read available vignettes for further insights. 4.7 Data Table (data.table) A data table (data.table) is similar to a data frame but with more advanced features for data manipulation. Credited to the data.table package by Barrett et al. (2024), data tables are known for their high-speed operations, particularly beneficial for large datasets. This sets them apart from tibbles which, although more user-friendly, may not be as optimized for speed. Consequently, data.table and tibble can be seen as competitors, each improving upon the basic data frame in their unique ways. Like tibble, data.table is not a part of base R. It requires the installation of the data.table package via install.packages(\"data.table\"), followed by library(\"data.table\") at the beginning of your script. The syntax of data.table differs significantly from the syntax used in the Tidyverse, making the R codes appear almost as distinct languages. While my research often employs data.table for its efficiency, in this book, I have chosen to emphasize tibble and the associated Tidyverse syntax. This decision is primarily due to the more intuitive and user-friendly nature of the Tidyverse. For those who wish to explore the data.table package in more depth, I suggest starting with its vignette. You can access it in R using: vignette(&quot;datatable-intro&quot;, package = &quot;data.table&quot;) If you’re unsure about the specific vignettes available, list all vignettes associated with data.table: vignette(package = &quot;data.table&quot;) Then, based on your interest, you can select a vignette and view it with the vignette() function, as shown earlier. Moreover, for structured learning on data.table, DataCamp offers comprehensive courses including: Data Manipulation with data.table in R Joining Data with data.table in R Time Series with data.table in R These courses provide an in-depth exploration, from basic operations to advanced manipulations using data.table. 4.8 Extensible Time Series (xts) The extensible time series (xts) object of the xts package by Ryan and Ulrich (2024b) pairs a matrix with a time index: Figure 4.1: xts Object as a Matrix with Time Index In xts, each row of a matrix is marked with a unique timestamp, typically formatted as a Date or POSIXct. Details on creating Date or POSIXct time indices can be gathered from Chapter 3.3. It’s important to note that an xts object’s base is a matrix, not a data frame or tibble, which mandates that all columns share the same data type. This means an xts object contains exclusively numbers or characters but not a mix of the two. While most matrix-related functions from Chapter 4.3 are compatible with xts objects, xts offers an additional set of specialized functions tailored for handling time series data. By design, xts objects always sort data chronologically, from the earliest to the latest observation. If you have use-cases that demand arranging time series data based on other criteria (e.g., size of stock returns), it’s necessary to first revert the xts object back to a matrix or a data frame. xts objects are fundamentally based on zoo objects from the zoo package by Zeileis, Grothendieck, and Ryan (2023), named after Zeileis’ ordered observations. These, too, are time-indexed data structures, but the xts package introduces additional functionalities. To integrate xts into your workflow, it needs to be installed first using install.packages(\"xts\"). Subsequently, invoking library(\"xts\") at the beginning of your script loads the package. 4.8.1 Create an xts Object An xts object can be created using the xts() function. This function binds data with its respective time index (order.by = time_index): # Load xts package library(&quot;xts&quot;) # Create a data matrix data &lt;- matrix(1:12, ncol = 2, dimnames = list(NULL, c(&quot;a&quot;, &quot;b&quot;))) print(data) ## a b ## [1,] 1 7 ## [2,] 2 8 ## [3,] 3 9 ## [4,] 4 10 ## [5,] 5 11 ## [6,] 6 12 # Create a time index of the same length as the data matrix time_index &lt;- seq(as.Date(&quot;1995-11-01&quot;), as.Date(&quot;1996-04-01&quot;), by = &quot;month&quot;) print(time_index) ## [1] &quot;1995-11-01&quot; &quot;1995-12-01&quot; &quot;1996-01-01&quot; &quot;1996-02-01&quot; &quot;1996-03-01&quot; ## [6] &quot;1996-04-01&quot; # Create an xts object based on the data matrix its time index dxts &lt;- xts(x = data, order.by = time_index) print(dxts) ## a b ## 1995-11-01 1 7 ## 1995-12-01 2 8 ## 1996-01-01 3 9 ## 1996-02-01 4 10 ## 1996-03-01 5 11 ## 1996-04-01 6 12 4.8.2 Inspect an xts Object Before delving into the specifics of an xts object, it’s helpful to grasp its overall structure and the type of time index it utilizes. # Check data structure and time index type class(dxts) # Returns &quot;xts&quot; and &quot;zoo&quot;, but it&#39;s also a &quot;matrix&quot;. typeof(dxts) # Returns &quot;integer&quot;. tclass(dxts) # Returns &quot;Date&quot;. is.xts(dxts) # Returns TRUE. is.zoo(dxts) # Returns TRUE. is.matrix(dxts) # Returns TRUE. is.data.frame(dxts) # Returns FALSE. ## [1] &quot;xts&quot; &quot;zoo&quot; ## [1] &quot;integer&quot; ## [1] &quot;Date&quot; ## [1] TRUE ## [1] TRUE ## [1] TRUE ## [1] FALSE To inspect xts objects, functions like nrow(), ncol(), dim(), head(), tail(), summarize(), and View() can be employed. Their application is identical to their use with matrices, as detailed in Chapter 4.3.2. Sometimes you might want to separately view the core data (matrix) and the associated time index (Date or POSIXct vector): # Extract the time index from the xts object index(dxts) ## [1] &quot;1995-11-01&quot; &quot;1995-12-01&quot; &quot;1996-01-01&quot; &quot;1996-02-01&quot; &quot;1996-03-01&quot; ## [6] &quot;1996-04-01&quot; # Retrieve the core data as a matrix from the xts object coredata(dxts) ## a b ## [1,] 1 7 ## [2,] 2 8 ## [3,] 3 9 ## [4,] 4 10 ## [5,] 5 11 ## [6,] 6 12 # Retrieve the core data as a numeric vector from the xts object as.numeric(dxts) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 The xts package provides a range of functions tailored for extracting detailed information about the time index: # Extract specific time index information cbind(Date = index(dxts), Year = 1900 + .indexyear(dxts), # -&gt; format(index(dxts), &quot;%Y&quot;) Month = 1 + .indexmon(dxts), # -&gt; format(index(dxts), &quot;%m&quot;) Weekday = .indexwday(dxts), # -&gt; format(index(dxts), &quot;%u&quot;) Day_by_Month = .indexmday(dxts), # -&gt; format(index(dxts), &quot;%d&quot;) Day_by_Year = 1 + .indexyday(dxts), # -&gt; format(index(dxts), &quot;%j&quot;) Unix_Seconds = .index(dxts), # -&gt; as.numeric(index(dxts)) * 86400 Unix_Days = .index(dxts) / 86400) # -&gt; as.numeric(index(dxts)) ## Date Year Month Weekday Day_by_Month Day_by_Year Unix_Seconds Unix_Days ## [1,] 9435 1995 11 3 1 305 815184000 9435 ## [2,] 9465 1995 12 5 1 335 817776000 9465 ## [3,] 9496 1996 1 1 1 1 820454400 9496 ## [4,] 9527 1996 2 4 1 32 823132800 9527 ## [5,] 9556 1996 3 5 1 61 825638400 9556 ## [6,] 9587 1996 4 1 1 92 828316800 9587 For a deeper understanding of Unix time and date-time formatting strings such as %Y, %m, %u, etc., please refer to Chapter 3.3. Here’s how to convert between Unix time and date format: # Unix time conversions all(index(dxts) == as.Date(as.numeric(index(dxts)))) all(index(dxts) == as.Date(as.POSIXct(.index(dxts)))) ## [1] TRUE ## [1] TRUE When analyzing time series data, recognizing the periodicity, frequency, and data span is crucial. The xts package offers a set of functions specifically designed for these tasks: # Determine the start and end date of the series start(dxts) end(dxts) ## [1] &quot;1995-11-01&quot; ## [1] &quot;1996-04-01&quot; # Detect the underlying frequency of the data periodicity(dxts) ## Monthly periodicity from 1995-11-01 to 1996-04-01 # Gauge the time span covered by the data c(y = nyears(dxts), q = nquarters(dxts), m = nmonths(dxts), w = nweeks(dxts), d = ndays(dxts), H = nhours(dxts), M = nminutes(dxts), S = nseconds(dxts)) ## y q m w d H M S ## 2 3 6 6 6 6 6 6 4.8.3 Select and Modify One major strength of xts objects is the intuitive syntax for time subsetting. The following showcases various methods to subset data based on its Date format timestamp: # Examples of time subsetting: dxts[&quot;1995&quot;] # All data from the year 1995 ## a b ## 1995-11-01 1 7 ## 1995-12-01 2 8 dxts[&quot;1995-12-01&quot;] # All data from Dec 1, 1995 dxts[&quot;1994/1996&quot;] # Data from 1994 through 1996 dxts[&quot;1995-02-22/1998-08-03&quot;] # Data from Feb 22, 1995, to Aug 3, 1998 dxts[&quot;1996-01/03&quot;] # Data from Jan through Mar 1996 dxts[&quot;1995-12/&quot;] # Data from Dec 1995 to the end xts::first(dxts, 3) # First three observations xts::last(dxts, &quot;4 months&quot;) # Data from the last four months xts::first( xts::last(dxts, &quot;1 year&quot;), &quot;2 months&quot;) # First 2 months of the last year The function xts::first() specifically references the first() function from the xts package. This is essential because both dplyr and xts have a function named first(). Since we’ve loaded dplyr earlier, using just first() would call the dplyr::first() version. By prefixing with xts::, we ensure we’re using the function from the xts package. Because xts objects are built upon matrices, the selection and modification procedures described in Chapter 4.3.3 can also be employed with xts objects: # Retrieve the element from the 2nd row and 1st column dxts[2, 1] ## a ## 1995-12-01 2 # Two approaches to update the aforementioned element to 100 dxts[2, 1] &lt;- 100 dxts[&quot;1995-12-01&quot;, &quot;a&quot;] &lt;- 100 4.8.4 Add Labels While column names in xts objects can be modified similarly to matrices, as detailed in Chapter 4.3.4, xts objects don’t support row names. This is because the rows inherently have a time index. # Fetching column names colnames(dxts) ## [1] &quot;a&quot; &quot;b&quot; # Altering column names colnames(dxts) &lt;- c(&quot;A&quot;, &quot;B&quot;) print(dxts) ## A B ## 1995-11-01 1 7 ## 1995-12-01 100 8 ## 1996-01-01 3 9 ## 1996-02-01 4 10 ## 1996-03-01 5 11 ## 1996-04-01 6 12 4.8.5 Handle Missing Values Given the time-dependent nature of xts objects, specific strategies can be employed to address missing values in the data. Below are some common functions and their use-cases: na.omit(): Removes any rows containing NA values, effectively shrinking the dataset. na.trim(): Eliminates rows with NA values only at the beginning and end of the dataset, preserving the central portion. na.fill(): This function replaces NA entries with a specified value, for example, 666. na.locf(): Stands for “Last Observation Carried Forward.” If an NA value is encountered, this function fills it with the previous non-missing value. na.locf(, fromLast = TRUE): A variation of the above, but instead of using the previous value, it utilizes the subsequent non-missing value for filling. na.approx(): Replaces NA values with linearly interpolated values, which can be particularly useful when data points are missing at regular intervals. # Create an xts object with missing values date_seq &lt;- as.Date(&quot;2000-11-03&quot;) + 0:3 values &lt;- c(3.5, NA, 4.5, NA) dxts &lt;- xts(x = values, order.by = date_seq) # Different methods to handle missing values locf &lt;- na.locf(dxts) locf_fromLast &lt;- na.locf(dxts, fromLast = TRUE) filled &lt;- na.fill(dxts, fill = 666) interpolated &lt;- na.approx(dxts) # Merging for comparison merge(dxts, locf, locf_fromLast, filled, interpolated) ## dxts locf locf_fromLast filled interpolated ## 2000-11-03 3.5 3.5 3.5 3.5 3.5 ## 2000-11-04 NA 3.5 4.5 666.0 4.0 ## 2000-11-05 4.5 4.5 4.5 4.5 4.5 ## 2000-11-06 NA 4.5 NA 666.0 NA 4.8.6 Combine xts Objects In the realm of time series analysis using R, combining multiple datasets becomes a common requirement. With xts objects, merging data according to the time index is elegantly handled by the merge() function. Notably, when you apply the merge() function to xts objects, R smartly invokes the merge.xts() function. This specialized function considers the chronological order of time indices and merges the data accordingly, eliminating the need for an explicit by argument, which one might use when dealing with matrices. Here’s a simple demonstration: # Sample xts objects with different time indices xts_a &lt;- xts(x = 1:3, order.by = as.Date(c(&quot;2022-01-01&quot;, &quot;2022-01-02&quot;, &quot;2022-01-04&quot;))) xts_b &lt;- xts(x = 4:6, order.by = as.Date(c(&quot;2022-01-01&quot;, &quot;2022-01-03&quot;, &quot;2022-01-04&quot;))) # Merging the xts objects merge(xts_a, xts_b) ## xts_a xts_b ## 2022-01-01 1 4 ## 2022-01-02 2 NA ## 2022-01-03 NA 5 ## 2022-01-04 3 6 For situations where a variable amount of xts objects need to be combined, the Reduce() function offers a flexible solution: # Create an additional xts object with slightly different time indices xts_c &lt;- xts(x = 7:9, order.by = as.Date(&#39;2022-01-04&#39;) + 0:2) # Use Reduce with merge function to combine multiple xts objects Reduce(f = merge, x = list(xts_a, xts_b, xts_c)) ## init x..i.. x..i...1 ## 2022-01-01 1 4 NA ## 2022-01-02 2 NA NA ## 2022-01-03 NA 5 NA ## 2022-01-04 3 6 7 ## 2022-01-05 NA NA 8 ## 2022-01-06 NA NA 9 To combine xts objects in a manner analogous to rbind() with matrices, you’d append one time series dataset after another. This approach is especially handy when consecutively adding datasets—for instance, appending data for 2023 after 2022: # Construct xts data for 2022 and 2023 xts_2022 &lt;- xts(1:3, order.by = as.Date(&#39;2022-01-01&#39;) + 0:2) xts_2023 &lt;- xts(4:6, order.by = as.Date(&#39;2023-01-01&#39;) + 0:2) # Combine the two xts objects combined_xts &lt;- rbind(xts_2022, xts_2023) print(combined_xts) ## [,1] ## 2022-01-01 1 ## 2022-01-02 2 ## 2022-01-03 3 ## 2023-01-01 4 ## 2023-01-02 5 ## 2023-01-03 6 However, a word of caution with rbind(): Ensure the time indices don’t overlap. Unlike merge(), which handles overlaps gracefully, rbind() can result in duplicated entries if there’s an overlap. Align Time Indices Sometimes, the time indices of two xts objects don’t match. For example, one dataset could have dates representing the start of a month, while another dataset might use the month’s end. In such situations, alignment functions such as as.yearmon() and as.yearqtr() from the zoo package can be invaluable. These functions effectively transform dates, making them represent a generalized month or quarter, regardless of the specific day: # Convert daily dates to generalized monthly dates xts_start &lt;- xts(1:3, order.by = as.Date(c(&quot;2022-01-01&quot;, &quot;2022-02-01&quot;, &quot;2022-03-01&quot;))) xts_end &lt;- xts(4:6, order.by = as.Date(c(&quot;2022-01-31&quot;, &quot;2022-02-28&quot;, &quot;2022-03-31&quot;))) # Merge unaligned datasets merge(xts_start, xts_end) ## xts_start xts_end ## 2022-01-01 1 NA ## 2022-01-31 NA 4 ## 2022-02-01 2 NA ## 2022-02-28 NA 5 ## 2022-03-01 3 NA ## 2022-03-31 NA 6 # Aligned time indices index(xts_start) &lt;- as.yearmon(index(xts_start)) index(xts_end) &lt;- as.yearmon(index(xts_end)) # Merge the aligned datasets merge(xts_start, xts_end) ## xts_start xts_end ## Jan 2022 1 4 ## Feb 2022 2 5 ## Mar 2022 3 6 Such alignment is instrumental in ensuring datasets are combined correctly and without introducing unintended NA values. In conclusion, the xts package, with its suite of functions and compatibility with other packages like zoo, offers comprehensive tools for efficiently combining and managing time series data in R. 4.8.7 Time Series Transformations In the realm of time series analysis, certain operations are quintessential for processing and deducing meaningful insights. The xts package provides a rich set of tools to facilitate these transformations, such as lagging time series, computing differences, and calculating returns. Lag Time Series Lagging, in time series terminology, refers to the shifting of data by a specified number of periods. This is useful for comparing values across different points in time. The lag() function shifts the data by k periods. However, it’s essential to note that lag() is a wrapper function: its behavior adapts based on the object type. For xts objects, lag() internally calls lag.xts(). Additionally, various other packages such as dplyr also include a function called lag(), potentially leading to confusion. Therefore, for xts objects, the direct use of lag.xts() is advised. To elucidate, here’s a sample xts object: # Define an xts object dxts &lt;- xts(c(100, 102, 103, 101, 108), order.by = as.Date(&quot;2022-01-01&quot;) + 0:4) Notice that when we introduce a one-period lag to the data, the initial value 100 from the original series aligns with the subsequent date 2022-01-02 in the lagged series. This indicates that, as of 2022-01-02, the value 100 is a lagged observation as it pertains to the previous period: # Implement a single-period lag lagged_data &lt;- lag.xts(dxts, k = 1) print(lagged_data) ## [,1] ## 2022-01-01 NA ## 2022-01-02 100 ## 2022-01-03 102 ## 2022-01-04 103 ## 2022-01-05 101 # Comparison: original vs. lagged data merge(dxts, lagged_data) ## dxts lagged_data ## 2022-01-01 100 NA ## 2022-01-02 102 100 ## 2022-01-03 103 102 ## 2022-01-04 101 103 ## 2022-01-05 108 101 If the intent is to lag the data by two periods, the operation would be: # Shift data two periods forward lagged_data_two &lt;- lag.xts(dxts, k = 2) print(lagged_data_two) ## [,1] ## 2022-01-01 NA ## 2022-01-02 NA ## 2022-01-03 100 ## 2022-01-04 102 ## 2022-01-05 103 # Comparison: original and doubly-lagged data merge(dxts, lagged_data_two) ## dxts lagged_data_two ## 2022-01-01 100 NA ## 2022-01-02 102 NA ## 2022-01-03 103 100 ## 2022-01-04 101 102 ## 2022-01-05 108 103 Should you wish to remove the initial NA values, the command transforms slightly: # Forward shift by two periods, excluding initial NAs lagged_trimmed &lt;- lag.xts(dxts, k = 2, na.pad = FALSE) print(lagged_trimmed) ## [,1] ## 2022-01-03 100 ## 2022-01-04 102 ## 2022-01-05 103 Intriguingly, the k parameter is flexible enough to process negative values (which leads the series) or even vector inputs. The latter results in a matrix output where each column stands for a different lag duration: # Various lag periods applied to data various_lags &lt;- lag.xts(dxts, k = -2:2) print(various_lags) ## lag.2 lag.1 lag0 lag1 lag2 ## 2022-01-01 103 102 100 NA NA ## 2022-01-02 101 103 102 100 NA ## 2022-01-03 108 101 103 102 100 ## 2022-01-04 NA 108 101 103 102 ## 2022-01-05 NA NA 108 101 103 In the realm of time series analysis, the ability to lag proves indispensable. For instance, discerning the correlation between the present and lagged values can be insightful in gauging the persistence or autocorrelation of a time series: # Compute time series persistence cor(lag.xts(dxts, k = 0:1), use = &quot;complete.obs&quot;)[1,2] ## [1] -0.3321819 Convert Levels to Changes A critical step in the analysis of economic and financial time series is the conversion of time series levels into changes or differences. For instance, discerning a change in the unemployment rate can offer insights about the economy’s trajectory. To undertake this, the diff() function computes these differences. But, as with the lag function, it’s safer to use diff.xts(). Comparing the outputs of diff.xts() and lag.xts(): # Compute first difference with both diff.xts() and lag.xts() first_diff &lt;- diff.xts(dxts) first_diff_alt &lt;- dxts - lag.xts(dxts) # Comparison: original data vs. computed differences merge(dxts, first_diff, first_diff_alt) ## dxts first_diff first_diff_alt ## 2022-01-01 100 NA NA ## 2022-01-02 102 2 2 ## 2022-01-03 103 1 1 ## 2022-01-04 101 -2 -2 ## 2022-01-05 108 7 7 To compute differences relative to two periods prior: # Derive difference considering two preceding periods first_diff_two &lt;- diff.xts(dxts, lag = 2) first_diff_two_alt &lt;- dxts - lag.xts(dxts, k = 2) # Comparison: original data with two-period differences merge(dxts, first_diff_two, first_diff_two_alt) ## dxts first_diff_two first_diff_two_alt ## 2022-01-01 100 NA NA ## 2022-01-02 102 NA NA ## 2022-01-03 103 3 3 ## 2022-01-04 101 -1 -1 ## 2022-01-05 108 5 5 The second difference is the difference of the first difference: # Compute the second difference second_diff &lt;- diff.xts(dxts, differences = 2) second_diff_alt &lt;- (first_diff - lag.xts(first_diff, k = 1)) # Comparison: original, first-differenced, and second-differenced data merge(dxts, first_diff, second_diff, second_diff_alt) ## dxts first_diff second_diff second_diff_alt ## 2022-01-01 100 NA NA NA ## 2022-01-02 102 2 NA NA ## 2022-01-03 103 1 -1 -1 ## 2022-01-04 101 -2 -3 -3 ## 2022-01-05 108 7 9 9 For those analyzing financial and economic time series, growth rates offer a lens into relative changes over time. For instance, GDP growth rates provide insights into economic progression: # Calculate growth rates in percent growth_rate &lt;- 100 * diff.xts(dxts) / lag.xts(dxts, k = 1) growth_rate_alt &lt;- 100 * (dxts - lag.xts(dxts, k = 1)) / lag.xts(dxts, k = 1) # Comparison: original data and computed growth rates merge(dxts, growth_rate, growth_rate_alt) ## dxts growth_rate growth_rate_alt ## 2022-01-01 100 NA NA ## 2022-01-02 102 2.0000000 2.0000000 ## 2022-01-03 103 0.9803922 0.9803922 ## 2022-01-04 101 -1.9417476 -1.9417476 ## 2022-01-05 108 6.9306931 6.9306931 Additionally, log differences are pivotal for financial and economic series. They approximate growth rates when they hover around smaller values (e.g., within \\(\\pm 20\\%\\)): # Derive log differences in percent log_diff &lt;- 100 * diff.xts(dxts, log = TRUE) log_diff_alt &lt;- 100 * (log(dxts) - lag.xts(log(dxts), k = 1)) # Comparison: original data, growth rates, and log differences merge(dxts, growth_rate, log_diff, log_diff_alt) ## dxts growth_rate log_diff log_diff_alt ## 2022-01-01 100 NA NA NA ## 2022-01-02 102 2.0000000 1.9802627 1.9802627 ## 2022-01-03 103 0.9803922 0.9756175 0.9756175 ## 2022-01-04 101 -1.9417476 -1.9608471 -1.9608471 ## 2022-01-05 108 6.9306931 6.7010710 6.7010710 Make Time Series Regular In time series data, especially in financial datasets, you might find datasets with irregular intervals between observations. For example, daily yield data might exclude weekends and public holidays. Such irregularities can complicate analyses that assume regular intervals. When using functions like lag.xts() and diff.xts() that relate the current with the previous period, the span of a single “lag” can vary, sometimes representing a day or even an entire week. Consequently, interpretations of differences or growth rates can be skewed, as they might be influenced by these varying intervals. Example of an irregular monthly time series: # Define an irregular monthly time series xts_irregular &lt;- xts(x = 1:4, order.by = as.Date(c(&quot;2022-01-01&quot;, &quot;2022-02-01&quot;, &quot;2022-05-01&quot;, &quot;2022-07-01&quot;))) print(xts_irregular) ## [,1] ## 2022-01-01 1 ## 2022-02-01 2 ## 2022-05-01 3 ## 2022-07-01 4 To standardize these intervals, a common approach in the xts universe involves merging the irregular data with an xts object that has a regular frequency. Here’s how you can achieve this: Creating a Regular Time Index: Firstly, create a sequence of dates with regular intervals, covering the entire span of your irregular dataset. # Creating a daily sequence from the start to the end of your irregular data regular_time &lt;- seq(from = start(xts_irregular), to = end(xts_irregular), by = &quot;month&quot;) print(regular_time) ## [1] &quot;2022-01-01&quot; &quot;2022-02-01&quot; &quot;2022-03-01&quot; &quot;2022-04-01&quot; &quot;2022-05-01&quot; ## [6] &quot;2022-06-01&quot; &quot;2022-07-01&quot; Construct an Empty xts Object with Regular Frequency: Using the regularly spaced time index, initialize an empty xts object. empty_xts &lt;- xts(x = NULL, order.by = regular_time) Merge the Irregular Data with the Empty xts Object: This step fills the gaps in your irregular dataset with NA values, corresponding to the dates in the regular xts object where data isn’t available. xts_regular &lt;- merge(xts_irregular, empty_xts) print(xts_regular) ## xts_irregular ## 2022-01-01 1 ## 2022-02-01 2 ## 2022-03-01 NA ## 2022-04-01 NA ## 2022-05-01 3 ## 2022-06-01 NA ## 2022-07-01 4 Dealing with Missing Values: When needed, address NA values using methods like na.locf(), na.fill(), or na.approx(), as detailed in Chapter 4.8.5. xts_regular &lt;- na.approx(xts_regular) print(xts_regular) ## xts_irregular ## 2022-01-01 1.000000 ## 2022-02-01 2.000000 ## 2022-03-01 2.314607 ## 2022-04-01 2.662921 ## 2022-05-01 3.000000 ## 2022-06-01 3.508197 ## 2022-07-01 4.000000 By utilizing these transformations in xts, you can refine your time series data into a format that’s more amenable for subsequent analyses. 4.8.8 Apply Family In R, the apply family of functions offers a mechanism to perform repeated operations across data structures without the necessity for explicit loops. This becomes especially beneficial for time series data in xts objects when the task is to aggregate across temporal intervals, such as converting quarterly GDP data to an annual frequency. Apply by Columns and Rows Since an xts object is a matrix with an attached time index, the apply functions highlighted in Chapter 4.3.9 for matrices also work with xts objects. Specifically, the apply() function allows for the application of a function across entire columns or rows of an xts object. Additionally, more efficient alternatives like rowSums(), colSums(), colMeans(), and rowMeans() are available, and the sweep() function proves useful for time series normalization to specific mean and variance values. Apply by Groups As elaborated in Chapter @ref(apply-family-data.frame), the split-apply-combine strategy segments the rows of a data frame into distinct groups, applies functions like averages or minimum values per group, and then combines the summarized groups to a single data object. When engaging with an xts object, remember that every row refers to a time period. Consequently, a collection of rows denotes a time interval. Thus, using the split-apply-combine strategy on an xts object, for instance, to calculate the monthly mean within a year, effectively aggregates the time series data from a more granular frequency (like monthly) to a coarser one (like annually). Whilte the split-apply-combine strategy for data frames revolves around the sequence: split() + lapply() + do.call() + c(), aggregating time-series data with xts objects extends the sequence as follows: endpoints() + cut() + split() + lapply() + do.call() + rbind(). The goal is to break the time series into relevant intervals using endpoints() + cut() + split(), apply a desired function to each interval using lapply(), and then combine the results into a single xts object using do.call() + rbind(). Here’s how to aggregate the provided monthly time series to a yearly frequency by computing the mean by year: # Sample monthly time series dates &lt;- seq(from = as.Date(&quot;2022-10-01&quot;), length.out = 8, by = &quot;1 month&quot;) sample_xts &lt;- xts(c(100 + 1:7, NA), order.by = dates) print(sample_xts) ## [,1] ## 2022-10-01 101 ## 2022-11-01 102 ## 2022-12-01 103 ## 2023-01-01 104 ## 2023-02-01 105 ## 2023-03-01 106 ## 2023-04-01 107 ## 2023-05-01 NA endpoints(): This function within the xts package determines where in the data set the splits should be made. # Identify endpoints for each year end_points &lt;- endpoints(x = sample_xts, on = &quot;years&quot;, k = 1) print(end_points) ## [1] 0 3 8 Here’s a breakdown of the endpoints() function inputs: x: This is the xts object you want to find the endpoints for. In our example, it’s the sample_xts dataset. on: Specifies the time unit to identify as endpoints. Options can be “seconds”, “minutes”, “hours”, “days”, “weeks”, “months”, “quarters”, or “years”. For our purpose, we’ve used “years”. k: This sets the frequency for marking endpoints. A k value of 1 combined with on = \"years\" will set endpoints at the conclusion of each year. However, if k were 2, endpoints would be identified every two years. In our sample, the endpoints 0, 3, 8 indicate that observations at positions 0, 3, 8 are the terminal points for their respective years. cut(): After pinpointing the endpoints, we need to segment the time index of the xts object based on these intervals. The cut() function facilitates this segmentation: # Segment the time index into intervals cut_time &lt;- cut(x = seq_along(sample_xts), breaks = end_points, labels = index(sample_xts)[end_points]) print(cut_time) ## [1] 2022-12-01 2022-12-01 2022-12-01 2023-05-01 2023-05-01 2023-05-01 2023-05-01 ## [8] 2023-05-01 ## Levels: 2022-12-01 2023-05-01 The function inputs are elaborated below: x: This refers to the sequence of numbers representing the row positions of the xts object. In essence, seq_along(sample_xts) generates a sequence from 1 to the length of sample_xts, corresponding to each row of our time series data. breaks: This parameter determines where to segment the data. Here, we utilize the positions established by the endpoints() function to mark where each interval begins and ends. labels: This argument assigns a name or label to each interval. In our scenario, we’ve used the actual dates from our xts object to label the intervals, marking each segment with the corresponding endpoint date. split(): With the intervals defined, the xts object can be segmented using split() function based on the categories established by cut(). # Split data into time intervals split_data &lt;- split(x = sample_xts, f = cut_time) print(split_data) ## $`2022-12-01` ## ## 2022-10-01 101 ## 2022-11-01 102 ## 2022-12-01 103 ## ## $`2023-05-01` ## ## 2023-01-01 104 ## 2023-02-01 105 ## 2023-03-01 106 ## 2023-04-01 107 ## 2023-05-01 NA lapply(): After segmentation, lapply() can be harnessed to apply a specific function, like calculating an average, across each segment. # Apply a function to the data of each time interval monthly_means_list &lt;- lapply(X = split_data, FUN = mean, na.rm = TRUE) print(monthly_means_list) ## $`2022-12-01` ## [1] 102 ## ## $`2023-05-01` ## [1] 105.5 Observe that in the lapply() function, extra arguments are passed directly to the function specified in the FUN parameter. In this context, the supplementary argument na.rm = TRUE serves as an input to the mean function, equivalent to designating FUN = function(x) mean(x, na.rm = TRUE). do.call() + rbind(): After processing each interval, the subsequent step is to combine the results into a unified xts object. The combination of do.call() and rbind() offers an efficient method for this task. # Merging aggregated data into a matrix combined_means_matrix &lt;- do.call(what = rbind, args = monthly_means_list) # Transitioning matrix to an xts format aggregated_xts &lt;- as.xts(combined_means_matrix) # Display the consolidated data print(aggregated_xts) ## [,1] ## 2022-12-01 102.0 ## 2023-05-01 105.5 It’s worth noting that during our use of the cut() function, we labeled each segment with its respective endpoint, i.e., labels = index(sample_xts)[end_points]. As a consequence, the merged data is indexed by the concluding date of each year. To modify this, such as setting it to the initial day of the year, one can tweak the index as follows: # Adjusting the time index to reflect the first day of each corresponding year index(aggregated_xts) &lt;- as.Date(format(index(aggregated_xts), &quot;%Y-01-01&quot;)) print(aggregated_xts) ## [,1] ## 2022-01-01 102.0 ## 2023-01-01 105.5 The split-apply-combine strategy isn’t limited to producing a singular value for each group. Instead, it can also return multiple values or even a series of values. For instance, consider the goal of obtaining the cumulative product for each month: # Computing the cumulative product for each time interval monthly_cumprod_list &lt;- lapply(X = split_data, FUN = cumprod) print(monthly_cumprod_list) ## $`2022-12-01` ## ## 2022-10-01 101 ## 2022-11-01 10302 ## 2022-12-01 1061106 ## ## $`2023-05-01` ## ## 2023-01-01 104 ## 2023-02-01 10920 ## 2023-03-01 1157520 ## 2023-04-01 123854640 ## 2023-05-01 NA # Consolidating the results into an xts object aggregated_xts &lt;- as.xts(do.call(what = rbind, args = monthly_cumprod_list)) # Displaying the aggregated results print(aggregated_xts) ## [,1] ## 2022-10-01 101 ## 2022-11-01 10302 ## 2022-12-01 1061106 ## 2023-01-01 104 ## 2023-02-01 10920 ## 2023-03-01 1157520 ## 2023-04-01 123854640 ## 2023-05-01 NA Here, rather than condensing each month’s data into a single summary statistic, we’ve expanded each month’s data into a series representing the cumulative product. Finally, there are several xts functions that simplify the split-apply-combine approach: period.apply(): This function streamlines the split-apply-combine strategy. By determining the intervals using the endpoints, you can apply a function across these specified segments. # Use period.apply() for aggregation period.apply(sample_xts, INDEX = end_points, FUN = mean, na.rm = TRUE) ## [,1] ## 2022-12-01 102.0 ## 2023-05-01 105.5 apply.*(): The series of functions - apply.daily(), apply.weekly(), apply.monthly(), apply.quarterly(), and apply.yearly() - enable aggregation or transformation of time series data grounded on precise time intervals. Essentially, they act as specialized versions of the period.apply() function, designed for specific time frames. # Yearly aggregation example apply.yearly(sample_xts, FUN = mean, na.rm = TRUE) ## [,1] ## 2022-12-01 102.0 ## 2023-05-01 105.5 While these apply.*() functions simplify the split-apply-combine strategy, for unconventional aggregation frequencies, like quadrennial aggregations focusing on leap year endpoints, one would revert to the more intricate sequence: endpoints() + cut() + split() + lapply() + do.call() + rbind(). to.period(): Within financial time series data, the OHLC format is prevalent. It represents the Open, High, Low, and Close prices for assets within specific intervals, like daily, monthly, or yearly. The Open is the starting price, the High and Low represent the maximum and minimum prices achieved, respectively, and the Close is the ending price. This quartet offers insights into price movements and volatility within the given time span. # OHLC aggregation with to.period() to.period(sample_xts, period = &quot;years&quot;) ## sample_xts.Open sample_xts.High sample_xts.Low sample_xts.Close ## 2022-12-01 101 103 101 103 ## 2023-04-01 104 107 104 107 # OHLC aggregation without to.period() OHLC_functions &lt;- list(Open = function(x) head(na.omit(x), 1), High = function(x) max(x, na.rm = TRUE), Low = function(x) min(x, na.rm = TRUE), Close = function(x) tail(na.omit(x), 1)) sapply(X = OHLC_functions, FUN = apply.yearly, x = sample_xts) ## Open High Low Close ## [1,] 101 103 101 103 ## [2,] 104 107 104 107 Apply by Overlapping Groups In time series analysis, there are occasions when you need to apply a function to overlapping periods or windows of data rather than distinct, non-overlapping intervals. This distinction brings into focus two types of time windows, illustrated in Figure 4.2: Figure 4.2: Time Windows Tumbling Window: Also known as a fixed window, this method uses a stable start or endpoint for the window and aggregates data based on that anchor. A common illustration is when transitioning from monthly data to yearly data, where each year represents a separate, non-overlapping group of 12 months. Sliding Window: Alternatively called a rolling window, this method allows the window to “slide” over time. Both the beginning and the end of the window shift with each step, consistently encompassing a fixed number of periods. This approach produces a continuous sequence of averages (or other aggregates) and is particularly beneficial for observing smooth temporal trends. For instance, if you apply a 12-month sliding window to monthly data, it would calculate a moving average for every subsequent 12-month segment. In financial time series contexts, metrics such as rolling volatility or moving averages often necessitate the examination of sliding windows instead of tumbling windows. The rollapply() function in the xts package provides an efficient solution for these scenarios. Let’s dive deeper into the rollapply() function using an example. Consider a financial market dataset eq_mkt that contains daily returns, and you’re interested in computing the rolling standard deviation (volatility) for a 3-day window. Instead of using non-overlapping 3-day chunks, you want every single 3-day interval. This means the first calculation would be for days 1-3, the second for days 2-4, the third for days 3-5, and so on. # Sample dataset creation dates &lt;- seq(from = as.Date(&quot;2022-01-01&quot;), length.out = 10, by = &quot;1 day&quot;) eq_mkt &lt;- xts(rnorm(10, 0, 0.02), order.by = dates) print(eq_mkt) ## [,1] ## 2022-01-01 -0.002482031 ## 2022-01-02 0.028447852 ## 2022-01-03 0.002718264 ## 2022-01-04 -0.014120914 ## 2022-01-05 -0.030353679 ## 2022-01-06 -0.010653598 ## 2022-01-07 0.008525818 ## 2022-01-08 0.009157934 ## 2022-01-09 -0.006985625 ## 2022-01-10 -0.013736686 # Application of rollapply() rolling_volatility &lt;- rollapply(eq_mkt, width = 3, FUN = sd) print(rolling_volatility) ## [,1] ## 2022-01-01 NA ## 2022-01-02 NA ## 2022-01-03 0.016561564 ## 2022-01-04 0.021438554 ## 2022-01-05 0.016536898 ## 2022-01-06 0.010516797 ## 2022-01-07 0.019440330 ## 2022-01-08 0.011260154 ## 2022-01-09 0.009143476 ## 2022-01-10 0.011764034 Here’s a breakdown of the rollapply() function and its parameters: data: This is the xts object (or any time-series object) on which you want to apply the function. In our example, it’s the eq_mkt dataset. width: Specifies the size of the rolling window. Here, we’ve chosen a width of 3, which means we’ll compute the standard deviation for every 3-day overlapping period. FUN: The function you want to apply to each rolling window. We’ve used sd to compute the standard deviation, but it could be any other function, such as mean for a moving average. The output is an xts object containing the result of the function applied to each rolling window. Note that the resulting time series will be shorter than the original by width - 1, since the first width - 1 observations don’t have enough data points before them to form a complete window. This approach is extremely versatile and powerful. By changing the width parameter and the function applied (FUN), you can compute a variety of rolling statistics. The concept of overlapping periods is crucial in financial analysis, particularly for calculating metrics that provide insights into short-term dynamics and potential trend changes. 4.8.9 Resources To delve deeper into xts and zoo objects, consider reading its vignette, by executing vignette(\"xts\", package = \"xts\"), as well as exploring the following DataCamp courses: Manipulating Time Series Data with xts and zoo in R Importing and Managing Financial Data in R If you’re working within the tidyverse environment, the R package tidyquant by Dancho and Vaughan (2023) offers seamless integration with xts and zoo, see vignette(\"TQ02-quant-integrations-in-tidyquant\", package = \"tidyquant\"). Lastly, the following handy cheat sheet provides a quick reference on xts and zoo functions: s3.amazonaws.com/assets.datacamp.com/blog_assets/xts_Cheat_Sheet_R.pdf. References "],["process-data-in-r.html", "Chapter 5 Process Data in R 5.1 Process Bitcoin Price Data 5.2 Process Stock and Oil Prices 5.3 Process House Price Data 5.4 Resources", " Chapter 5 Process Data in R R specializes in data processing, encompassing a wide range of operations such as downloading, importing, cleaning, merging, transforming, summarizing, analyzing, and visualizing data. Previous chapters have introduced the core functions for performing these tasks (refer to Chapter 3 and 4). This chapter takes a practical approach by applying these concepts to real financial datasets in three illustrative examples. The key tools introduced in each example are summarized in Table 5.1. Table 5.1: Data Processing Examples Data Import Format Tasks Bitcoin prices DSV file \\(\\downarrow\\) data.frame Import CSV file as a data frame Inspect and clean data Visualize data with a line graph Export data as RDS and DSV files Stock and oil prices Web API data \\(\\downarrow\\) xts Import via Web API as an xts object Use quantmod and Quandl for API import Manipulate data with xts functions Visualize two variables in a single line graph Tabulate key statistics Visualize correlations with scatter plots House prices Excel workbook \\(\\downarrow\\) tibble Import Excel workbook as a tibble Manipulate data with tidyverse functions Reshape data from wide to long format Visualize data with layered line graphs Introduce the ggplot2 package for plotting Merge house prices with policy rates Compute and visualize key statistics As shown in Table 5.1, each example leverages different file types, data structures, and tools. Example 1 manipulates data frames using base R, as introduced in Chapter @ref(data.frame). Example 2 employs xts objects, detailed in Chapter 4.8. Example 3 utilizes Tidyverse functions on tibbles, as discussed in Chapter 4.6. New in this chapter is the focus on data import methods. Example 1 deals with DSV files, including CSV and TSV. Example 2 retrieves data via Web API. Example 3 deals with Excel workbooks. For importing data formats not covered here, search “import data in R” along with the format name in Google. You’ll likely find a dedicated package for the task. For instance, the DBI and RSQLite packages facilitate SQL database imports, while haven enables importing SPSS, SAS, and Stata files. Additionally, this chapter expands on visualization methods beyond those covered in Chapter 3.4. For instance, Example 2 introduces plot.zoo() for graphing xts objects, and Example 3 incorporates advanced plotting via the ggplot2 package. 5.1 Process Bitcoin Price Data This section demonstrates how to process Bitcoin prices from the Bitstamp exchange, obtained from CryptoDataDownload (Bitstamp 2024). Once the data is downloaded as a CSV file, it is imported into R. The date column undergoes conversion to a POSIXct data type. Subsequently, the data is visualized and exported in both RDS and CSV formats. 5.1.1 Download DSV File The procedure to obtain this dataset is detailed in Chapter 3.7.3. Briefly: Visit www.cryptodatadownload.com. Navigate to “Historical Data” &gt; “US &amp; UK Exchanges”. Click on “Bitstamp”, then find the “Bitstamp minute data broken down by year” section. Choose “BTC/USD 2023 minute” and click “Download CSV”. After downloading, ensure you place the file in your R working directory, specifically inside a folder named “files”, and rename the file to BTCUSD_minute.csv. For information on how to identify and set your working directory, refer to Chapters 3.7.1 and 3.7.2. 5.1.2 Import DSV File In R, datasets are primarily stored and manipulated using two-dimensional structures such as matrices and data frames, complemented by extensions such as tibbles, data tables, and xts objects. Various functions exist for importing external datasets into R. The choice of function depends both on the external dataset’s format, whether CSV, Excel, or XML, and the intended data structure in R, whether a data frame, matrix, tibble, or xts object. DSV (delimiter-separated values) is a common file format used to store tabular data. As the name suggests, the values in each row of a DSV file are separated by a delimiter. This delimiter is a sequence of one or more characters used to specify the boundary between separate, independent regions in a data table. Depending on the character used as a delimiter, different notations emerge, such as CSV (comma-separated values) and TSV (tab-separated values). Sometimes semicolons, spaces, or other characters are used as delimiters in similar formats. Here’s an example of how data is stored in a CSV file: Gender, Age, Score, Rank Male, 25, 100, 3 Female, 22, 90, 4 And for a TSV file, the data would look like this: Gender Age Score Rank Male 25 100 3 Female 22 90 4 Note the use of tabs in the TSV example to separate the data fields, in contrast to the commas used in the CSV example. To read DSV files into R and convert them to data frames, the read.table() function comes in handy. The function has several arguments that allow customization based on the structure and specifics of your DSV file. Before importing, it’s advisable to preview your DSV file in a spreadsheet program like Microsoft Excel to understand its layout and structure. Here’s an illustration of how to employ the function using the downloaded BTCUSD_minute.csv file: # Load DSV file containing Bitcoin prices btcusd &lt;- read.table( file = &quot;files/BTCUSD_minute.csv&quot;, sep = &quot;,&quot;, # Delimiter is a comma (CSV) header = TRUE, # The file has a header row with column names dec = &quot;.&quot;, # Decimal point character is a period na.strings = &quot;NA&quot;, # Character string representing NA values skip = 1, # Skips the introductory row (website&#39;s name) nrows = 5000 # Limits reading to the initial 5000 rows ) # Display the first 4 rows of columns 1 to 7 for an overview btcusd[1:3, 1:7] ## unix date symbol open high low close ## 1 1697413380 2023-10-15 23:43:00 BTC/USD 27114 27119 27107 27119 ## 2 1697413320 2023-10-15 23:42:00 BTC/USD 27114 27120 27114 27120 ## 3 1697413260 2023-10-15 23:41:00 BTC/USD 27115 27119 27115 27119 In this illustration: file: Denotes the DSV file’s path and name. sep: The delimiter is a comma for this CSV file, i.e. sep = \",\". For a TSV file, you would use sep = \"\\t\". If sep = \",\", the read.csv() function could be used similarly without the need for the sep argument. header: Indicates the presence of a header in the file. When TRUE, the row following the skipped rows (if any) is considered as the column names. dec: Specifies the character for decimal points. In the U.S., this is typically 2.84 with dec = \".\", while in parts of Europe it’s 2,84 with dec = \",\". na.strings: Sets the string that corresponds to NA values. Some datasets may use “N/A” or might leave the entry blank, leading to configurations like na.strings = \"N/A\" or na.strings = \"\", respectively. skip: States the initial lines to omit from the file. nrows: Caps the rows read from the file. By tweaking these parameters, you can handle a wide range of DSV file structures and peculiarities. It’s important to note that the read.table() function by default imports the DSV file as a data frame: # Determine the class of the imported data class(btcusd) ## [1] &quot;data.frame&quot; Once the data is imported, you can transform the data frame into various data structures: Convert to a tibble using the as_tibble() function from the tibble package. Transform into a data table via the as.data.table() function from the data.table package. Create an xts object using the xts() function from the xts package. For more direct approaches: The read_delim() function from the readr package by Wickham, Hester, and Bryan (2024), part of the Tidyverse, directly imports DSV files as a tibble. The fread() function, found in the data.table package, imports DSV files as a data table. To directly read DSV files as zoo objects, one can use the read.zoo() function from the zoo package. Subsequently, to transform the zoo object into an xts object, apply the as.xts() function from the xts package. While these functions share certain traits with read.table(), they also come with their own unique features. To fully understand their capabilities and differences, consider consulting their respective documentation using ?read_delim, ?fread, and ?read.zoo after loading the readr, data.table, and zoo packages. 5.1.3 Inspect Data Once data is imported, a preliminary examination ensures its accuracy. This can be done by simply entering the data name btcusd into the console. However, since R prints the entire data frame, the columns will not be visible if the data contains more rows than the size of the screen. To accommodate this, you can use the head() function to print the column names and the first six observations, or the tail() function to print the column names and the last six observations, where the additional input n = 10 increases it to ten observations: # Show the concluding 10 rows of the first 7 columns tail(btcusd[1:7], n = 10) ## unix date symbol open high low close ## 4991 1697099580 2023-10-12 08:33:00 BTC/USD 26799 26799 26799 26799 ## 4992 1697099520 2023-10-12 08:32:00 BTC/USD 26805 26805 26805 26805 ## 4993 1697099460 2023-10-12 08:31:00 BTC/USD 26796 26807 26796 26807 ## 4994 1697099400 2023-10-12 08:30:00 BTC/USD 26789 26800 26789 26800 ## 4995 1697099340 2023-10-12 08:29:00 BTC/USD 26794 26794 26794 26794 ## 4996 1697099280 2023-10-12 08:28:00 BTC/USD 26798 26798 26796 26796 ## 4997 1697099220 2023-10-12 08:27:00 BTC/USD 26786 26790 26784 26790 ## 4998 1697099160 2023-10-12 08:26:00 BTC/USD 26790 26790 26790 26790 ## 4999 1697099100 2023-10-12 08:25:00 BTC/USD 26790 26792 26787 26790 ## 5000 1697099040 2023-10-12 08:24:00 BTC/USD 26778 26778 26777 26778 In cases where numerous columns still prove problematic even with head() or tail(), RStudio provides a spreadsheet-like viewer for data frames via the View() function (exclusive to RStudio and not included in R): # Open data in a spreadsheet-style viewer View(btcusd) For an exploration of the data’s structure and summary, execute str(btcusd) and summary(btcusd). 5.1.4 Clean Data It’s important to check the data type of each column, as the import process may have misassigned data types. Utilize sapply() together with class() for this task: # Examine the data structure of the dataset class(btcusd) # Identify the data type for every column sapply(btcusd, class) ## [1] &quot;data.frame&quot; ## unix date symbol open high low ## &quot;integer&quot; &quot;character&quot; &quot;character&quot; &quot;integer&quot; &quot;integer&quot; &quot;integer&quot; ## close Volume.BTC Volume.USD ## &quot;integer&quot; &quot;numeric&quot; &quot;numeric&quot; In this scenario, R has accurately discerned Bitcoin prices and trading volumes as numerical values. The columns open, high, low, and close denote the opening, peak, trough, and closing prices within a minute interval, respectively. Volume.BTC and Volume.USD quantify transaction volumes in bitcoins and USD. However, the date column wasn’t identified as a specific date format, such as POSIXct. This requires manual conversion (refer to Chapter 3.3 for understanding date objects like POSIXct). # Modify date strings to the POSIXct data type btcusd$date &lt;- as.POSIXct(btcusd$date) # Recheck the data type of the date column class(btcusd$date) ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; Such type adjustments are common when importing data in R. When employing read.table() in R, the function inspects the first 1,000 rows to deduce each column’s type. If these rows aren’t indicative of the complete dataset, misinterpretations can arise. An example: if the initial 1,000 rows of a column seem numeric but later on contain a character, R might incorrectly perceive it as numeric, where the characters are converted to numeric NA values. Another scenario is R reading a column as character values when the first 1,000 rows of a column are absent, instead of numeric values. Also, R might read “N/A” as a character string instead of a missing value, causing the column to become character type instead of numeric. The last issue can be addressed by defining na.strings = \"N/A\" within read.table(), or by replacing “N/A” post-import with NA and then using as.numeric() to convert to numeric. To improve data clarity, consider adding contextual attributes like the data source and the count of imported rows. These attributes can be easily appended using R’s attr() function. For a cleaner data structure, it’s also beneficial to eliminate the row.names attribute for our btcusd object, as it merely consists of sequential numbers. In R, setting an attribute to NULL effectively removes it. # Attach source and row count as attributes to the btcusd object attr(btcusd, &quot;source&quot;) &lt;- &quot;www.cryptodatadownload.com/cdd&quot; attr(btcusd, &quot;n_rows_imported&quot;) &lt;- nrow(btcusd) # Erase the row.names attribute attr(btcusd, &quot;row.names&quot;) &lt;- NULL # Examine attributes to confirm changes attributes(btcusd) ## $names ## [1] &quot;unix&quot; &quot;date&quot; &quot;symbol&quot; &quot;open&quot; &quot;high&quot; ## [6] &quot;low&quot; &quot;close&quot; &quot;Volume.BTC&quot; &quot;Volume.USD&quot; ## ## $class ## [1] &quot;data.frame&quot; ## ## $source ## [1] &quot;www.cryptodatadownload.com/cdd&quot; ## ## $n_rows_imported ## [1] 5000 By doing so, anyone reviewing this data object can immediately understand its origin and size without having to consult external documentation. This additional layer of metadata can be especially beneficial when sharing data objects across different phases of a project or with other collaborators. 5.1.5 Visualize Data A graph is a visual representation that displays data points on a coordinate system, typically showing the relationship between two or more variables. Each variable is assigned to an axis, and the data points are plotted accordingly. In R, the plot() function is one of the primary tools for creating graphs. Consider the Bitcoin prices dataset, btcusd, from our previous section. We’ll map the date (btcusd$date) onto the x-axis and the closing price at each trading minute (btcusd$close) to the y-axis: # Plot data plot(x = btcusd$date, y = btcusd$close, type = &quot;l&quot;, main = &quot;Bitcoin Prices During 5,000 Trading Minutes&quot;, ylab = &quot;USD&quot;, xlab = paste(&quot;Time in&quot;, format(tail(btcusd$date, 1), &quot;%Y&quot;))) Figure 5.1: Bitcoin Prices During 5,000 Trading Minutes Here’s a detailed breakdown of the plot() function arguments used in the example: x and y: These are the primary arguments that specify the data for the x-axis and y-axis, respectively. In the given example, btcusd$date represents the time variable on the x-axis, while btcusd$close signifies the Bitcoin closing prices on the y-axis. type: This argument determines how the data points should be represented. The value \"l\" indicates that the data points should be connected with lines, producing a line graph. Other common values include \"p\" for points (producing a scatter plot), \"b\" for both points and lines, and \"h\" for vertical lines (like high-density plots). main: This argument defines the main title of the graph. Here, “Bitcoin Prices During 5,000 Trading Minutes” serves as the title, succinctly conveying the subject of the visualization. xlab and ylab: These arguments specify the labels for the x-axis and y-axis, respectively. Properly labeling axes is vital as it provides context and clarifies the variables being visualized. In our example, “Time in 2023” denotes the time intervals on the x-axis, and “USD” indicates that the y-axis represents Bitcoin prices in US dollars. Figure 5.1 showcases the closing prices of Bitcoin over a span of 5,000 trading minutes. By visually inspecting the graph, one can gauge the volatility, identify any potential data inconsistencies, and generally understand the behavior of Bitcoin prices over the observed interval. 5.1.6 Export Data This section shows how to export data in two commonly used formats: RDS and DSV. RDS R provides a unique storage format known as RDS (R Data Structure). It’s a binary format, which means it represents data in a way that’s directly readable by computers but not easily readable by humans. The primary advantage of binary formats, like RDS, is they often allow faster read and write operations and can maintain all the intricacies of data structures. This ensures that when an R object is saved in RDS format and then reloaded, its structure and metadata remain unaltered. To save an R object as an RDS file, use the saveRDS() function. # Create folder for exported files dir.create(path = &quot;exported-files&quot;, showWarnings = FALSE) # Save btcusd data as RDS file saveRDS(object = btcusd, file = &quot;exported-files/btcusd.rds&quot;) In this example: object represents the R object to be saved. file indicates the path and filename where the data will be stored or overwritten. To retrieve the object from the RDS file into R, use the readRDS() function. # Load btcusd data from an RDS file btcusd_loaded &lt;- readRDS(file = &quot;exported-files/btcusd.rds&quot;) identical(btcusd, btcusd_loaded) ## [1] TRUE Key Points: Always check your working directory using getwd() to ensure files are saved where intended. Adjust it with setwd() if necessary. Remember, RDS files are designed for R. While efficient for R-to-R data transfers, they might not be compatible with other software. Ensure your storage can accommodate the file size, especially for large datasets. Though RDS files are compressed, large datasets can still occupy significant space. Always exercise caution when loading RDS files from unknown sources, as they could contain harmful code. Overall, RDS is a reliable choice for preserving R objects between sessions and projects. DSV For compatibility across various software applications, it’s often useful to save data as DSV files, instead of R-specific formats. The write.table() function facilitates the export of data frames to DSV formats, including CSV and TSV. # Create folder for exported files dir.create(path = &quot;exported-files&quot;, showWarnings = FALSE) # Export Bitcoin price data as TSV file write.table(x = btcusd, file = &quot;exported-files/btcusd.tsv&quot;, sep = &quot;\\t&quot;, row.names = FALSE) Here: x represents the data frame to export. file denotes the desired path and filename for the DSV file. sep specifies the delimiter between fields. In this case, we’re using a tab (\\t) for a TSV file. Notably, using write.csv() is akin to write.table() with sep = \",\". With row.names = FALSE, the exported file won’t include row names, which are typically just sequential numbers. By tailoring the parameters of write.table(), you can effectively store your data in the preferred format. Points to remember: Always verify the working directory with getwd() to ensure the file is saved where intended. If necessary, adjust it using setwd(). After exporting, open the file in a text editor or spreadsheet application to ensure its contents are as expected. Be patient with large datasets. Exporting might take time, so allow R to finish the process before exiting. Here are alternative methods for exporting DSV files in R: The write_delim() function from the readr package, part of the Tidyverse, exports data frames to DSV format. This function is especially useful when working with tibbles. The fwrite() function from the data.table package is another option for exporting data frames to DSV files. This function is optimized for speed and efficiency. For time series data, the write.zoo() function from the zoo package allows you to export zoo or xts objects directly as DSV files. To fully understand the capabilities and unique features of these functions, it’s recommended to consult their respective documentation using ?write_delim, ?fwrite, and ?write.zoo after loading the readr, data.table, and zoo packages. Note that DSV export doesn’t retain attributes and may misclassify data types on import: # Import the exported TSV file btcusd_loaded &lt;- read.table(file = &quot;exported-files/btcusd.tsv&quot;, sep = &quot;\\t&quot;, header = TRUE) # Verify if the original and imported TSV files match identical(btcusd, btcusd_loaded) ## [1] FALSE Therefore, while DSV files offer better software compatibility than RDS, they fall short in preserving all attributes and data types of the original object. 5.2 Process Stock and Oil Prices This section offers a roadmap for acquiring, manipulating, visualizing, and analyzing financial data using R using stock and oil price data as example. This section begins by discussing Web APIs as a method for directly fetching data, circumventing manual downloads and uploads. The quantmod and Quandl packages are used to fetch data from various APIs. Following data acquisition, the section guides the reader through merging disparate data streams—such as stock and commodity prices—into a unified dataset. With the data merged, it presents methods to visualize the time-series data using plotting functionalities tailored for xts objects. The focus then shifts to transforming this raw data into a more analytically amenable form, dealing specifically with the issues of irregular frequency and missing values to create a regular time series at a monthly frequency. Finally, the section concludes by demonstrating how to tabulate key statistics like mean, volatility, and correlation. These statistics offer a quantitative lens through which to understand the dataset’s central tendencies, dispersion, and relational aspects. Altogether, the section provides a comprehensive workflow for handling financial data, from acquisition to data analysis. 5.2.1 Web API If your computer is connected to the internet, you can directly fetch data within R, bypassing the need to download data files manually and then import them into R. This section explains how to download financial and economic data in R using a Web API. An Application Programming Interface (API), specifically a Web API, is more than just a data file saved on a website. It is a set of rules, protocols, and tools that enable two software entities to communicate with each other. In the context of data retrieval, a Web API serves as an interface between web servers and clients, offering a systematic way to access data. When you query a Web API, the server typically generates a response dynamically based on the request parameters it receives. This data can come in various formats such as JSON, XML, HTML, or plain text. In R, direct interaction with Web APIs is often unnecessary, thanks to specialized R packages that handle this task. These packages translate fetched data into familiar R formats like data frames or xts objects. For instance, the imfr package fetches data from the IMF API, while the OECD and Quandl packages fetch data from the OECD and Nasdaq APIs, respectively. General-purpose packages like quantmod can interact with multiple APIs. To find an API for a specific database, a simple Google search such as “r web api database_name” should suffice. In this chapter, we utilize the getSymbols() function from the quantmod package and the Quandl() function from the Quandl package to fetch data directly into R. 5.2.2 Download via quantmod The quantmod package by Ryan and Ulrich (2024a) serves as a comprehensive toolkit for quantitative finance, primarily focused on data acquisition, manipulation, and visualization. It is designed to assist in the workflow of both financial traders and researchers. One of the package’s key functions is getSymbols(), which allows for easy access to financial data from multiple sources like Yahoo Finance, Google Finance, and Federal Reserve Economic Data (FRED). Using getSymbols(), you can fetch various types of financial data such as stock prices, trading volumes, and other market indicators. The function downloads the data and converts it into an xts object, a data structure introduced in Chapter 4.8 that is designed to hold time-ordered data. Here’s a basic example of using getSymbols() to fetch stock data for Apple Inc: # Load the quantmod package library(&quot;quantmod&quot;) # Download Apple stock prices from Yahoo Finance AAPL_stock_price &lt;- getSymbols( Symbols = &quot;AAPL&quot;, src = &quot;yahoo&quot;, auto.assign = FALSE, return.class = &quot;xts&quot;) # Display the most recent downloaded stock prices tail(AAPL_stock_price) ## AAPL.Open AAPL.High AAPL.Low AAPL.Close AAPL.Volume AAPL.Adjusted ## 2024-05-22 192.27 192.82 190.27 190.90 34648500 190.90 ## 2024-05-23 190.98 191.00 186.63 186.88 51005900 186.88 ## 2024-05-24 188.82 190.58 188.04 189.98 36294600 189.98 ## 2024-05-28 191.51 193.00 189.10 189.99 52280100 189.99 ## 2024-05-29 189.61 192.25 189.51 190.29 53068000 190.29 ## 2024-05-30 190.76 192.18 190.63 191.29 49889100 191.29 After running these commands, you’ll have the Apple stock data stored in an xts object, ready for your analysis and visualization tasks. The arguments in this context are: Symbols: Specifies the instrument to import. This could be a stock ticker symbol, exchange rate, economic data series, or any other identifier. auto.assign: When set to FALSE, data must be assigned to an object (here, AAPL_stock_price). If not set to FALSE, the data is automatically assigned to the Symbols input (in this case, AAPL). return.class: Determines the type of data object returned, e.g., ts, zoo, xts, or timeSeries. src: Denotes the data source from which the symbol originates. Some examples include “yahoo” for Yahoo Finance, “google” for Google Finance, “FRED” for Federal Reserve Economic Data, and “oanda” for Oanda foreign exchange rates. To find the correct symbol for financial data from Yahoo Finance, visit finance.yahoo.com and search for the company of interest, e.g., Apple. The symbol is the series of letters inside parentheses. For example, for Apple Inc. (AAPL), use Symbols = \"AAPL\" (and src = \"yahoo\"). To download economic data from FRED, visit fred.stlouisfed.org, and search for the data series of interest, e.g., unemployment rate. Click on one of the suggested data series. The symbol is the series of letters inside parentheses. For example, for Unemployment Rate (UNRATE), use Symbols = \"UNRATE\" (and src = \"FRED\"). Note that Yahoo Finance provides data in OHLC format, consisting of six columns: AAPL.Open, AAPL.High, AAPL.Low, AAPL.Close, AAPL.Volume, and AAPL.Adjusted. These values represent the opening, highest, lowest, and closing prices for a given market day, the trade volume for that day, and the adjusted closing price, which accounts for stock splits. These columns can be accessed using quantmod’s Op(), Hi(), Lo(), Cl(), Vo(), and Ad() functions. Typically, the AAPL.Adjusted column is of most interest. # Extract adjusted Apple share price AAPL_stock_price_adj &lt;- Ad(AAPL_stock_price) # Display the most recent adjusted stock prices tail(AAPL_stock_price_adj) ## AAPL.Adjusted ## 2024-05-22 190.90 ## 2024-05-23 186.88 ## 2024-05-24 189.98 ## 2024-05-28 189.99 ## 2024-05-29 190.29 ## 2024-05-30 191.29 5.2.3 Download via Quandl Quandl is a data service that provides access to many financial and economic data sets from various providers. It has been acquired by Nasdaq Data Link and is now operating through the Nasdaq data website: data.nasdaq.com. Some data sets require a paid subscription, while others can be accessed for free. Quandl provides an API that allows you to import data from a wide variety of languages, including Excel, Python, and R. To fetch data into R, the Quandl() function from the Quandl package is employed. To use Quandl, first get a free Nasdaq Data Link API key at data.nasdaq.com/sign-up. After that, install and load the Quandl package and provide the API key with the function Quandl.api_key(): # Load Quandl package library(&quot;Quandl&quot;) # Provide API key api_quandl &lt;- &quot;CbtuSe8kyR_8qPgehZw3&quot; # Replace this with your API key Quandl.api_key(api_quandl) Then, you can download data using Quandl(), such as fetching oil price data: # Download monthly crude oil prices from Nasdaq Data Link # ODA_oil_price &lt;- Quandl(code = &quot;ODA/POILAPSP_USD&quot;, type = &quot;xts&quot;) ODA_oil_price &lt;- Quandl(code = &quot;BSE/SI1400&quot;, type = &quot;xts&quot;)$Close # Display the downloaded crude oil prices tail(ODA_oil_price) ## Close ## 2023-07-14 19064.63 ## 2023-07-17 19137.22 ## 2023-07-18 19159.95 ## 2023-07-19 19268.31 ## 2023-07-20 19370.52 ## 2023-07-21 19373.83 Here, the code argument refers to both the data source and the symbol in the format “source/symbol”. The type argument specifies the type of data object: data.frame, xts, zoo, ts, or timeSeries. To find the correct code for the Quandl function, visit data.nasdaq.com, and search for the data series of interest (e.g., oil price). The code is the series of letters listed in the left column, structured like this: .../..., where ... are letters. For example, for the crude oil price provided by IMF’s Open Data for Africa (ODA), use code = \"ODA/POILAPSP_USD\". ODA’s crude oil price data is monthly and does not include the most recent values. For more frequent and up-to-date data, go to the search results for “oil price” and look for the dataset from the Organization of the Petroleum Exporting Countries (OPEC). Its identifier, QDL/OPEC, isn’t immediately visible in the search listings. To locate it, click on the OPEC dataset link and find the code beneath the data table. To retrieve this data, use Quandl.datatable() instead of Quandl() since it’s structured as a data table, not a typical time series. # Download daily crude oil prices from Nasdaq Data Link OPEC_oil_price = Quandl.datatable(&quot;QDL/OPEC&quot;) # Convert data frame to xts object OPEC_oil_price = xts(x = OPEC_oil_price$value, order.by = OPEC_oil_price$date) # Display the downloaded crude oil prices tail(OPEC_oil_price) ## [,1] ## 2024-01-18 79.39 ## 2024-01-19 80.27 ## 2024-01-22 79.70 ## 2024-01-23 81.30 ## 2024-01-24 81.05 ## 2024-01-25 81.98 5.2.4 Clean Data When data is retrieved using R packages that interact with Web APIs, the likelihood of encountering misassigned data types is minimal. This is in contrast to the often encountered type issues when importing from DSV or Excel files. Nonetheless, the retrieved data object may still need conversion to a more suitable data structure. For example, we converted the OPEC_oil_price object from a data frame to an xts object. Storing meta-information, such as the data source and time of import, as contextual attributes can be particularly useful. Since the data is directly imported via R code, it’s possible to preserve the import command itself as an attribute for future reference or replication: # Record the time of data import attr(AAPL_stock_price, &quot;time_of_import&quot;) &lt;- Sys.time() attr(AAPL_stock_price, &quot;time_of_import&quot;) ## [1] &quot;2024-05-31 14:04:32 CDT&quot; # Define the function used to import AAPL stock data import_AAPL_data &lt;- function() { quantmod::getSymbols( Symbols = &quot;AAPL&quot;, src = &quot;yahoo&quot;, auto.assign = FALSE, return.class = &quot;xts&quot;) } # Attach the import function as an attribute to the data object attr(AAPL_stock_price, &quot;import_command&quot;) &lt;- import_AAPL_data # Demonstrate re-import of the data using the saved attribute tail(attr(AAPL_stock_price, &quot;import_command&quot;)()) ## AAPL.Open AAPL.High AAPL.Low AAPL.Close AAPL.Volume AAPL.Adjusted ## 2024-05-22 192.27 192.82 190.27 190.90 34648500 190.90 ## 2024-05-23 190.98 191.00 186.63 186.88 51005900 186.88 ## 2024-05-24 188.82 190.58 188.04 189.98 36294600 189.98 ## 2024-05-28 191.51 193.00 189.10 189.99 52280100 189.99 ## 2024-05-29 189.61 192.25 189.51 190.29 53068000 190.29 ## 2024-05-30 190.76 192.18 190.63 191.29 49889100 191.29 By doing so, the data object becomes self-descriptive, facilitating easier data sharing and replication across different project stages or among collaborators. Note that data is only available for roughly 252 days per year, as indicated by the table generated below: # Count available data points by year table(format(index(AAPL_stock_price), &quot;%Y&quot;)) ## ## 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 ## 251 253 252 252 252 250 252 252 252 252 251 251 252 253 252 251 ## 2023 2024 ## 250 104 That’s because stock market data is only available on trading days, excluding weekends and holidays. The result is an irregular time series with gaps varying from one day to over a week. To achieve a regular time series, consider interpolation or aggregating to a lower frequency like monthly data. This process is detailed in a subsequent chapter (see Chapter 5.2.8) and is also discussed in Chapters 4.8. 5.2.5 Plot Apple Stock Prices Let’s visualize the Apple stock prices using the plot() function: # Plot adjusted Apple share price plot(AAPL_stock_price_adj, main = &quot;Apple Share Price&quot;) Figure 5.2: Apple Share Price Observe that this plot differs from those created in previous chapters where data was stored as vectors - specifically, columns from data frames or tibbles. This variation occurs because plot() is a wrapper function, adapting its behavior to the object type supplied (see Chapter 3.5.11 for more on wrapper functions). When given an xts object, plot() defaults to using the plot.xts() method, optimized for visualizing time series data. Therefore, it’s no longer necessary to specify it as a line plot (type = \"l\"), as most time series are visualized with lines. To explicitly use the xts type plot() function, you can use the plot.xts() function, which behaves identically as long as the input object is an xts object. If not, the function will attempt to convert the object first. As we’ve explored in Chapter 4.8, an xts object is an enhancement of the zoo object, offering more features. My personal preference leans towards the zoo variant of plot(), plot.zoo(), over plot.xts() because it operates more similarly to how plot() does when applied to data frames. To utilize plot.zoo(), apply it to the xts object like so: # Plot adjusted Apple share price using plot.zoo() plot.zoo(AAPL_stock_price_adj, main = &quot;Apple Share Price&quot;, xlab = &quot;Date&quot;, ylab = &quot;USD&quot;) In this context, the inputs are as follows: AAPL_stock_price_adj is the xts object containing Apple Inc.’s adjusted stock prices, compatible with plot.zoo() thanks to xts’s extension of zoo. main sets the plot’s title, in this case, “Apple Share Price”. xlab and ylab respectively label the x-axis as “Date” and the y-axis as “USD”, denoting stock prices in U.S. dollars. The usage of plot.zoo() provides a more intuitive plotting method for some users, replicating plot()’s behavior with data frames more closely compared to plot.xts(). Figure 5.3: Apple Share Price Visualized with plot.zoo() The resulting plot, shown in Figure 5.3, visualizes the Apple share prices over the last two decades. Stock prices often exhibit exponential growth over time, meaning the value increases at a rate proportional to its current value. As such, even large percentage changes early in a stock’s history may appear minor due to a lower initial price. Conversely, smaller percentage changes at later stages can seem more significant due to a higher price level. This perspective can distort the perception of a stock’s historical volatility, making past changes appear smaller than they were. To more accurately represent these proportionate changes, many analysts prefer using logarithmic scales when visualizing stock price data: # Compute the natural logarithm (log) of Apple share price log_AAPL_stock_price_adj &lt;- log(AAPL_stock_price_adj) # Plot log share price plot.zoo(log_AAPL_stock_price_adj, main = &quot;Logarithm of Apple Share Price&quot;, xlab = &quot;Date&quot;, ylab = &quot;Log of USD Price&quot;) Figure 5.4: Logarithm of Apple Share Price Figure 5.4 displays the natural logarithm of the Apple share prices. This transformation is beneficial because it linearizes the exponential growth in the stock prices, with the slope of the line corresponding to the rate of growth. More importantly, changes in the logarithm of the price correspond directly to percentage changes in the price itself. For example, if the log price increases by 0.01, this equates to a 1% increase in the original price. This property allows for a direct comparison of price changes over time, making it easier to interpret and understand the magnitude of these changes in terms of relative growth or decline. 5.2.6 Plot Crude Oil Prices Now, we’ll use the plot.zoo() function to graph the crude oil price: # Plot crude oil prices plot.zoo(OPEC_oil_price, main = &quot;OPEC Crude Oil Price&quot;, xlab = &quot;Date&quot;, ylab = &quot;USD per Barrel (159 Litres)&quot;) Figure 5.5: OPEC Crude Oil Price Figure 5.5 displays the OPEC crude oil prices over the last two decades. While stock prices often exhibit long-term exponential growth due to company expansions and increasing profits, oil prices don’t inherently follow the same pattern. The primary drivers of oil prices are supply and demand dynamics, shaped by a myriad of factors such as geopolitical events, production changes, advancements in technology, environmental policies, and shifts in global economic conditions. Oil prices, as a result, can undergo substantial fluctuations, exhibiting high volatility with periods of significant booms and busts, rather than consistent exponential growth. Visualizing oil prices over time can benefit from the use of a logarithmic scale: # Compute the natural logarithm (log) of crude oil prices log_OPEC_oil_price &lt;- log(OPEC_oil_price) # Plot log crude oil prices plot.zoo(log_OPEC_oil_price, main = &quot;Logarithm of OPEC Crude Oil Price&quot;, xlab = &quot;Date&quot;, ylab = &quot;Log of USD per Barrel (159 Litres)&quot;) Figure 5.6: Logarithm of OPEC Crude Oil Price The natural logarithm of crude oil price, depicted in Figure 5.6, offers a clearer view of relative changes over time. It does this by rescaling the data such that equivalent percentage changes align to the same distance on the scale, regardless of the actual price level at which they occur. This transformation means a change in the log value can be read directly as a percentage change in price. So, a decrease of 0.01 in the log price corresponds to a 1% decrease in the actual price, whether the initial price was $10, $100, or any other value. This makes the data more intuitive and straightforward to interpret, particularly when tracking price trends and volatility across various periods. 5.2.7 Merge Data In this section, we focus on merging and cleaning two time-series datasets: Apple’s adjusted stock price (AAPL_stock_price_adj) and OPEC’s oil price (OPEC_oil_price). We’ll use R’s merge() function to combine these series into a single data object for comparative analysis. # Merge Prices &lt;- merge(AAPL_stock_price_adj, OPEC_oil_price) head(Prices, n = 3) ## AAPL.Adjusted OPEC_oil_price ## 2003-01-02 NA 30.05 ## 2003-01-03 NA 30.83 ## 2003-01-06 NA 30.71 Note that merging AAPL_stock_price_adj and OPEC_oil_price does not require a by argument because both datasets are xts objects. In the case of xts, merging is done automatically by date index via the merge.xts() method. After merging, it’s sometimes useful to trim rows with NA values at either end, as they could affect subsequent analysis. The function na.trim() serves this purpose. # Trim data Prices &lt;- na.trim(Prices) head(Prices, n = 3) ## AAPL.Adjusted OPEC_oil_price ## 2007-01-03 2.530319 55.42 ## 2007-01-04 2.586482 53.26 ## 2007-01-05 2.568063 51.30 It’s also informative to examine specific periods where data is missing for either series. For instance, we can focus on the year 2022 to identify such instances. # Missing oil price data in 2022 Prices[is.na(Prices$OPEC_oil_price)][&quot;2022&quot;] ## AAPL.Adjusted OPEC_oil_price # Missing stock price data in 2022 Prices[is.na(Prices$AAPL.Adjusted)][&quot;2022&quot;] ## AAPL.Adjusted OPEC_oil_price ## 2022-01-17 NA 86.38 ## 2022-02-21 NA 94.04 ## 2022-05-30 NA 120.01 ## 2022-06-20 NA 113.39 ## 2022-07-04 NA 115.25 ## 2022-09-05 NA 99.76 ## 2022-11-24 NA 81.52 While OPEC oil prices are globally traded and thus consistently available, AAPL stock prices have gaps due to U.S. market holidays, which are listed in Table 5.2 below. .cl-9edaf3de{}.cl-9ecd3974{font-family:'Helvetica';font-size:11pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-9ecd399c{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-9ed2bff2{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-9ed2e8c4{width:1.067in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9ed2e8ce{width:2.06in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9ed2e8d8{width:1.067in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9ed2e8e2{width:2.06in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9ed2e8e3{width:1.067in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9ed2e8ec{width:2.06in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9ed2e8ed{width:1.067in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9ed2e8f6{width:2.06in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9ed2e8f7{width:1.067in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9ed2e900{width:2.06in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9ed2e901{width:1.067in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9ed2e90a{width:2.06in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 5.2: Data Gaps in 2022 due to U.S.-Specific Holidays DateU.S.-Specific Holiday2022-01-17Martin Luther King Jr. Day2022-02-21Presidents' Day2022-05-30Memorial Day2022-06-20Juneteenth2022-07-04Independence Day2022-09-05Labor Day2022-11-24Thanksgiving Recognizing these details is important for time-series analysis. 5.2.8 Transform Data In this section, we undertake a series of data transformations to prepare for further analysis. These transformations include the removal of missing values, aggregation to a monthly frequency, and computation of stock returns. Return Formula Returns on stocks or commodities are defined as the percentage change in price, given by: \\[ r_t = 100 \\left( \\frac{P_t - P_{t-1}}{P_{t-1}} \\right) \\% \\approx 100 \\left( \\ln P_t - \\ln P_{t-1} \\right)\\% \\] Here, \\(\\ln P_t\\) is the natural logarithm of \\(P_t\\). The log difference, \\(\\ln P_t - \\ln P_{t-1}\\), approximates the arithmetic return, \\(\\frac{P_t - P_{t-1}}{P_{t-1}}\\), particularly for small returns (less than \\(\\pm 20\\%\\)). Log returns are better for understanding how money grows over time. Imagine you invest $100 in a stock. On the first day, the stock gains 50%, so your investment is now worth $150. The next day, the stock loses 33.33%, bringing your investment down to $100 again. Using arithmetic returns: First day: (150 - 100) / 100 = 0.5 or 50% Second day: (100 - 150) / 150 = -0.3333 or -33.33% Total return = 50% - 33.33% = 16.67% If you simply sum the arithmetic returns, you’d think you have a positive total return of 16.67%. But you’re right back where you started, with $100. Using log returns: First day: \\(\\ln\\)(150/100) = 0.4055 or 40.55% Second day: \\(\\ln\\)(100/150) = -0.4055 or -40.55% Total log return = 40.55% - 40.55% = 0.00% The total log return correctly shows that, over the two days, you haven’t gained or lost any money. This example illustrates why log returns can be less misleading when you are making multiple trades or assessing performance over time. They better capture the compounding effect and provide a more accurate representation of your returns. Aggregate to Regular Frequency We previously noted that stock and commodity prices have irregular frequencies, with approximately 252 trading days per year. Consequently, the notion of “return” can vary in duration, from one day to an entire week, which can introduce inconsistencies in the analysis. To address this, one approach is to interpolate the data to a daily frequency for 365 days a year. Another approach is to aggregate the data to a less frequent, but more consistent, time frame like monthly, quarterly, or yearly periods. We opt for monthly aggregation. While a month is not a regular time unit—varying from 28 to 31 days - the variation in time intervals is considerably less problematic than in the original dataset. It’s also important to note that U.S. stock market data are unavailable on U.S.-specific holidays, whereas commodity price data might still be accessible. To facilitate an apples-to-apples comparison between the two series, it’s crucial to eliminate the commodity prices for these particular days, as well. This can be accomplished using the na.omit() function on the merged dataset, which removes any rows where at least one column has missing data. # Remove missing values Prices &lt;- na.omit(Prices) The apply.monthly() function from the xts package is used to convert the data from a daily to a monthly frequency. Specifically, the last available price within each month is used for the monthly aggregation. This method is consistent with how the adjusted stock prices were aggregated to a daily frequency using each day’s closing price. # Aggregate to monthly frequency Prices_monthly &lt;- apply.monthly(Prices, FUN = last) This results in a time series with a consistent monthly frequency. Compute Returns Next, we calculate monthly returns using the log formula: # Compute returns Returns &lt;- 100 * diff.xts(Prices_monthly, log = TRUE, na.pad = FALSE) To visualize the returns of both assets on a single graph, use the plot.type = \"single\" argument within the plot.zoo() function. # Plot returns plot.zoo(Returns, plot.type = &quot;single&quot;, main = &quot;Stock vs. Commodity Returns&quot;, col = c(2, 1), lwd = c(2, 1)) legend(&quot;bottomleft&quot;, legend = c(&quot;Apple Stock Return&quot;, &quot;Crude Oil Return&quot;), col = c(2, 1), lwd = c(2, 1)) Figure 5.7: Stock vs. Commodity Returns The plot, seen in Figure 5.7, reveals substantial volatility in both investment avenues. Apple’s stock appears less volatile than oil, which is counterintuitive given that Apple is subject to company-specific risks, whereas oil is a globally traded commodity. While Apple’s stock is susceptible to company-specific risks such as earnings reports, innovation cycles, and market competition, its diversification in various products and global markets may cushion it against extreme volatility. Oil, on the other hand, faces a unique blend of risks that contribute to its high volatility. These include geopolitical tensions, supply-demand imbalances, and external shocks such as natural disasters affecting production. Oil prices are also influenced by decisions made by major producers like OPEC, and global events that affect oil consumption. These multiple and often unpredictable factors can lead to large price swings in a short period, making oil a more volatile investment compared to a single company’s stock like Apple. 5.2.9 Tabulate Key Statistics While visualizations offer a broad overview of trends and patterns, precise statistics are crucial for in-depth data analysis. We will calculate key statistics such as the mean, volatility, minimum, and maximum returns. These measures provide insights into the central tendency and dispersion of the data. Additionally, we’ll compute correlation to examine how the two series move together and autocorrelation to understand the persistence of each series. # Assemble key statistics in a data frame stats_df &lt;- rbind( &quot;Mean&quot; = apply(Returns, 2, mean), &quot;Volatility (Standard Deviation)&quot; = apply(Returns, 2, sd), &quot;Minimum Return&quot; = apply(Returns, 2, min), &quot;Maximum Return&quot; = apply(Returns, 2, max), &quot;Autocorrelation&quot; = apply(Returns, 2, function(x) cor(x, lag.xts(x, 1), use = &quot;complete.obs&quot;)), &quot;Correlation to Crude Oil Return&quot; = cor(Returns)[2, ]) colnames(stats_df) &lt;- c(&quot;Apple Stock Return&quot;, &quot;Crude Oil Return&quot;) print(stats_df) ## Apple Stock Return Crude Oil Return ## Mean 2.1151902 0.2173427 ## Volatility (Standard Deviation) 9.0237217 11.9508115 ## Minimum Return -39.9818093 -79.7423517 ## Maximum Return 21.3255986 47.4494913 ## Autocorrelation 0.1069703 0.3068170 ## Correlation to Crude Oil Return 0.3273161 1.0000000 The table title should specify the unit of return (in percent), the data frequency (monthly), and the time range for which the statistics are calculated. # Define the time range for the statistics time_interval &lt;- format(x = range(index(Returns)), format = &quot;%b %Y&quot;) print(time_interval) ## [1] &quot;Feb 2007&quot; &quot;Jan 2024&quot; # Create table title stats_title &lt;- paste(&quot;Monthly $\\\\%$-Return Statistics:&quot;, paste(time_interval, collapse = &quot; to &quot;)) print(stats_title) ## [1] &quot;Monthly $\\\\%$-Return Statistics: Feb 2007 to Jan 2024&quot; The table output from print(stats_df) is unsuitable for professional documents. For Microsoft Word or PowerPoint, export the table to a CSV file using write.table() and then insert it into the document. For LaTeX or HTML formats, employ the kable() function from the knitr package for a more refined table presentation. # Load knitr for table rendering library(&quot;knitr&quot;) # Format table with rounded statistics kable(x = round(stats_df, 2), caption = stats_title, format = &quot;simple&quot;) Table: (\\#tab:return-stats) Monthly $\\%$-Return Statistics: Feb 2007 to Jan 2024 Apple Stock Return Crude Oil Return -------------------------------- ------------------- ----------------- Mean 2.12 0.22 Volatility (Standard Deviation) 9.02 11.95 Minimum Return -39.98 -79.74 Maximum Return 21.33 47.45 Autocorrelation 0.11 0.31 Correlation to Crude Oil Return 0.33 1.00 In HTML or LaTeX documents, kable() produces neatly formatted tables, as seen in Table 5.3, by setting the format parameter to either format = \"html\" or format = \"latex\". For further customization options of the kable() function, refer to Chapter 6.7. Table 5.3: Monthly \\(\\%\\)-Return Statistics: Feb 2007 to Jan 2024 Apple Stock Return Crude Oil Return Mean 2.12 0.22 Volatility (Standard Deviation) 9.02 11.95 Minimum Return -39.98 -79.74 Maximum Return 21.33 47.45 Autocorrelation 0.11 0.31 Correlation to Crude Oil Return 0.33 1.00 Table 5.3 presents key statistical metrics for two investment avenues - Apple stock and crude oil - over a specific time period. Apple stock shows a mean return of approximately 2.12%, which is higher than crude oil’s mean return of 0.22%, suggesting better average performance. Moreover, crude oil exhibits greater volatility, with a standard deviation of about 11.95% compared to Apple’s 9.02%, indicating higher risk. Both investment avenues have experienced substantial downside risks, as evidenced by their minimum monthly returns of approximately -39.98% and -79.74% for Apple and crude oil, respectively. Crude oil has seen peaks of considerably higher returns than Apple, with a maximum of about 47.45%, compared to Apple’s 21.33%. The autocorrelation values of 0.11 for Apple and 0.31 for crude oil suggest that past returns have some, albeit limited, influence on future returns, more so for crude oil. Lastly, a correlation of 0.33 between Apple and crude oil returns indicates a mild positive relationship, implying that diversifying across these two asset classes may offer limited but non-negligible benefits. 5.2.10 Visualize Correlations The correlations outlined in Table 5.3 merit further inspection through scatter plots. These visualizations allow for an assessment of whether the observed correlations hold broadly or are mainly driven by outliers. # Put three plots on one figure par(mfrow = c(1, 3)) # Autocorrelation of Apple returns and their lags x_var &lt;- lag.xts(Returns$AAPL.Adjusted, 1) y_var &lt;- Returns$AAPL.Adjusted plot.default(x = x_var, y = y_var, xlab = &quot;Lagged Apple Stock Returns&quot;, ylab = &quot;Apple Stock Returns&quot;) abline(a = lm(y_var ~ x_var), col = &quot;blue&quot;) corr_text &lt;- round(x = cor(x = x_var, y = y_var, use=&quot;complete.obs&quot;), digits = 2) text(x = min(na.omit(x_var)), y = min(na.omit(y_var)) + 10, labels = paste(&quot;Autocorrelation:&quot;, corr_text), pos = 4, col = &quot;blue&quot;) # Autocorrelation of oil returns and their lags x_var &lt;- lag.xts(Returns$OPEC_oil_price, 1) y_var &lt;- Returns$OPEC_oil_price plot.default(x = x_var, y = y_var, xlab = &quot;Lagged Oil Returns&quot;, ylab = &quot;Oil Returns&quot;) abline(a = lm(y_var ~ x_var), col = &quot;red&quot;) corr_text &lt;- round(x = cor(x = x_var, y = y_var, use=&quot;complete.obs&quot;), digits = 2) text(x = min(na.omit(x_var)), y = min(na.omit(y_var)) + 10, labels = paste(&quot;Autocorrelation:&quot;, corr_text), pos = 4, col = &quot;red&quot;) # Correlation of Apple and oil returns x_var &lt;- Returns$OPEC_oil_price y_var &lt;- Returns$AAPL.Adjusted plot.default(x = x_var, y = y_var, xlab = &quot;Oil Returns&quot;, ylab = &quot;Apple Stock Returns&quot;) abline(a = lm(y_var ~ x_var), col = &quot;purple&quot;) corr_text &lt;- round(x = cor(x = x_var, y = y_var, use=&quot;complete.obs&quot;), digits = 2) text(x = min(na.omit(x_var)), y = min(na.omit(y_var)) + 10, labels = paste(&quot;Correlation:&quot;, corr_text), pos = 4, col = &quot;purple&quot;) # Add a title above all three plots par(mfrow = c(1, 1)) # Reset to a single plot layout title(&quot;Cross- and Autocorrelations of Financial Returns&quot;, outer = TRUE, line = -2) Figure 5.8: Cross- and Autocorrelations of Financial Returns In this R code: par(mfrow = c(1, 3)): Configures the plotting space to display three plots in one row. The parameter mfrow sets this layout, where the first number 1 specifies the number of rows and the second 3 specifies the number of columns. plot.default(...): Creates the scatter plots to compare returns. The plot.default() function is invoked explicitly to bypass the default time-series plotting behavior of xts objects that would be triggered by the generic plot() function. For a discussion on wrapper functions like plot() and their associated methods, refer to Chapter 3.5.11. abline(lm(...), ...): Adds a line to the plot, showing the best-fit straight line through the data points. This line helps to visualize the relationship between the variables. corr_text &lt;- round(cor(x, y, use = \"complete.obs), 2): Calculates the rounded correlation between the two sets of returns, omitting pairs with NA values via use = \"complete.obs\". text(x = min(na.omit(x_var)), min(y = na.omit(y_var)) + 10, ...): Places a label on each plot, showing the rounded correlation coefficient. The text appears close to the minimum values of the x and y axes to avoid overlap. par(mfrow = c(1, 1)): Reverts the plotting area to a single-frame layout, necessary for adding an overarching title. title(\"Cross- and Autocorrelations of Financial Returns\", outer = TRUE, line = -2): Positions an overarching title above all subplots. The outer = TRUE parameter ensures the title appears outside the individual plot areas, and line = -2 adjusts its vertical positioning. Figure 5.8 examines the relationships among Apple stock returns, oil price returns, and their respective lagged values. The first two panels show that data points significantly diverge from the fitted lines, suggesting that the autocorrelation values 0.11 and 0.31 are likely influenced by outliers rather than representing a consistent trend. The third panel, however, shows a more stable relationship between Apple and oil returns, which correlation value is 0.33. This suggests that low (or high) returns in one asset generally coincide with low (or high) returns in the other. This matches what we see in Figure 5.7, where if one asset goes up or down, the other usually does the same. 5.3 Process House Price Data This chapter focuses on the processing of residential property price (RPP) data from the Bank for International Settlements (BIS, 2023b). The aim is to prepare the data for further analyses, especially its comparison with policy rates, also obtained from BIS (2023a). The chapter employs functions from the Tidyverse ecosystem in R, specifically using dplyr and tidyr for data manipulation as previously introduced in Chapter 4.6. Given the dataset’s multidimensional nature - spanning various indicators, regions, and time periods - the chapter introduces advanced plotting techniques via the ggplot2 package. It also addresses the complexities of computing associative statistics in such a multidimensional setting. Upon completing this chapter, readers will gain a holistic understanding of how to manage and analyze multidimensional data using these R tools. 5.3.1 Download Excel Workbook To get RPP data from the BIS (2023b), follow these steps: Go to www.bis.org. Go to “Statistics” &gt; “Property Prices” &gt; “Selected Residential”. Click the “XLSX” button next to the data description: “Nominal and real residential property prices…”. Once downloaded, place the file in your R working directory, specifically in a folder named “files,” and name it pp_selected.xlsx. House prices will also be compared to policy rates from the BIS (2023a), obtained as follows: Go to www.bis.org. Go to “Statistics” &gt; “Other Indicators” &gt; “Policy Rates”. Click the “XLSX” button next to the data description: “Monthly data”. Once downloaded, place the file in your R working directory, specifically in a folder named “files,” and name it cbpol.xlsx. For guidance on finding and setting your working directory, see Chapters 3.7.1 and 3.7.2. To learn how to create a folder and download files directly from a website within R, check Chapter 3.7.3. 5.3.2 Import Excel Workbook An Excel workbook (xlsx) is a file that contains one or more spreadsheets (worksheets), which are the separate “pages” or “tabs” within the file. An Excel worksheet, or simply a sheet, is a single spreadsheet within an Excel workbook. It consists of a grid of cells formed by intersecting rows (labeled by numbers) and columns (labeled by letters), which can hold data such as text, numbers, and formulas. The readxl package by Wickham and Bryan (2023) provides functions that allow for importing Excel files into R as a tibble structure, which is the tidyverse-version of a data frame (see Chapter 4.6 for an overview on tibbles, and Chapter 3.6.7 for the tidyverse). Hence, to import the residential property price (RPP) database, which is an Excel workbook, install and load the readxl package by running install.packages(\"readxl\") in the console and include the package at the beginning of your R script. You can then use the excel_sheets() function to print the names of all Excel worksheets in the workbook, and the read_excel() function to import a particular worksheet. Here’s an illustration of how to employ the excel_sheets() function using the pp_selected.xlsx file downloaded from the Bank for International Settlements. # Load the packages library(&quot;readxl&quot;) library(&quot;tibble&quot;) library(&quot;dplyr&quot;) library(&quot;tidyr&quot;) # Get names of all Excel worksheets sheet_names &lt;- excel_sheets(path = &quot;files/pp_selected.xlsx&quot;) sheet_names ## [1] &quot;Content&quot; &quot;Summary Documentation&quot; &quot;Quarterly Series&quot; This reveals that there are three Excel sheets: “Content”, “Summary Documentation”, and “Quarterly Series”. The “Content” sheet provides information about how the Excel workbook is structured, as well as how to cite the data when using it, and links to documentation files. The “Summary Documentation” sheet provides information about the house price variables, such as the variable’s code, reference area, unit of measurement, and whether it’s price adjusted. Finally, the “Quarterly Series” sheet stores the house prices, where each row is a different quarter, and each column is a different variable. There are four header rows, where the first three include information about the variable that is also available in the “Summary Documentation” sheet, and the last row references the variable’s code. Hence, it is sufficient to import “Summary Documentation” for variable information and “Quarterly Series” without the first three rows for the data. # Import Excel worksheet: &quot;Summary Documentation&quot; RPP_labels &lt;- read_excel( path = &quot;files/pp_selected.xlsx&quot;, sheet = &quot;Summary Documentation&quot;, col_names = TRUE, # The file has a header row with names na = &quot;-&quot; # Character string representing NA values ) # Import Excel worksheet: &quot;Quarterly Series&quot; RPP_wide &lt;- read_excel( path = &quot;files/pp_selected.xlsx&quot;, sheet = &quot;Quarterly Series&quot;, col_names = TRUE, # The file has a header row with names na = &quot;&quot;, # Character string representing NA values skip = 3 # Skips the first three rows ) As a side note, when the Excel workbook consists of a large number of sheets, e.g. each representing different variables, then it makes sense to use lapply() to import all sheets at once: # Import all worksheets of an Excel workbook without customization import_all &lt;- lapply(sheet_names, function(x) read_excel(path = &quot;files/pp_selected.xlsx&quot;, sheet = x)) # Name the list elements, each representing a different worksheet names(import_all) &lt;- sheet_names In the provided code snippet, the lapply() function is used to apply the read_excel() function to each element in the sheet_names list, returning a list that is the same length as the input. Consequently, the import_all object becomes a list in which each element is a tibble corresponding to an individual Excel worksheet, which is named accordingly. For instance, to access the worksheet “Summary Documentation”, you would select import_all[[\"Summary Documentation\"]]. This approach is useful if each sheet refers to a different variable, but in this case, since we only have three sheets, importing each sheet separately with sheet-specific customizations makes more sense. 5.3.3 Inspect and Clean Data Once data is imported, a preliminary examination ensures its accuracy. Note that because read_excel() imports data as a tibble and not a data frame, R doesn’t print the entire data frame; hence, simply entering the data names RPP_labels and RPP into the console already provides a good overview. For more details, use RStudio’s spreadsheet-like viewer via View(RPP_labels) and View(RPP) (exclusive to RStudio and not included in R). For a more detailed exploration of the data’s structure, execute str() or glimpse() from the tibble package, and execute summary() for a summary of the data. # Overview of variable labels tibble::glimpse(RPP_labels) ## Rows: 248 ## Columns: 9 ## $ `Data set` &lt;chr&gt; &quot;BIS_SPP&quot;, &quot;BIS_SPP&quot;, &quot;BIS_SPP&quot;, &quot;BIS_SPP&quot;, &quot;BIS_SPP&quot;… ## $ Code &lt;chr&gt; &quot;Q:4T:N:628&quot;, &quot;Q:4T:N:771&quot;, &quot;Q:4T:R:628&quot;, &quot;Q:4T:R:771… ## $ Frequency &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;… ## $ `Reference area` &lt;chr&gt; &quot;Emerging market economies (aggregate)&quot;, &quot;Emerging ma… ## $ Value &lt;chr&gt; &quot;Nominal&quot;, &quot;Nominal&quot;, &quot;Real&quot;, &quot;Real&quot;, &quot;Nominal&quot;, &quot;Nom… ## $ Unit &lt;chr&gt; &quot;Index, 2010 = 100&quot;, &quot;Year-on-year changes, in per ce… ## $ Coverage &lt;chr&gt; &quot;Emerging markets covered in the BIS residential prop… ## $ `Series title` &lt;chr&gt; &quot;Residential property prices selected - Nominal - Ind… ## $ Breaks &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… Here, all variables of the RPP_labels tibble are of data type character &lt;chr&gt;, which makes sense, as they consist of verbal descriptions of the data. Examining the data types of each column is crucial as they may be incorrectly assigned during import. Use sapply() in conjunction with class() to identify the types: # Inspect the overall structure of the dataset class(RPP_wide) # Determine the class of each column RPP_data_types &lt;- sapply(RPP_wide, class) # Validate if the first column &#39;Period&#39; is of Date or POSIXct type RPP_data_types[1] # Confirm whether all remaining columns are numeric all(RPP_data_types[-1] == &quot;numeric&quot;) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; ## $Period ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; ## ## [1] TRUE In this case, R correctly identifies the ‘Period’ as POSIXct and all house prices as numeric. No type conversions are needed. To improve data clarity, consider adding contextual attributes like the data source and the count of imported rows, as we did with the imported Bitcoin price data in Chapter 5.1.4. Lastly, pay attention to variable labels. The column Value in BIS’s RPP data indicates whether the residential property price (RPP) is \"Real\" or \"Nominal\". To avoid confusion with data values, the column name is changed to Measure. Additionally, the column Reference area is simplified to Region, and the Unit descriptions are abbreviated for ease. Converting these attributes into factors enhances computational efficiency. # Rename variables RPP_labels &lt;- RPP_labels %&gt;% mutate(Code = factor(Code), Region = factor(`Reference area`), Unit = factor(Unit), Measure = factor(Value)) # Abbreviate unit labels renaming_vector &lt;- c(&quot;Year-on-year changes, in per cent&quot; = &quot;YoY-Growth&quot;, &quot;Index, 2010 = 100&quot; = &quot;Index&quot;) levels(RPP_labels$Unit) &lt;- renaming_vector[levels(RPP_labels$Unit)] This R code performs two main tasks: renaming variables and modifying their labels. Rename Variables: mutate(Code = factor(Code), Region = factor('Reference area'), Unit = factor(Unit), Measure = factor(Value)): The mutate function from dplyr is used to change the column names. Here, Reference area is renamed to Region and Value to Measure. All these variables are also converted to factors which are more efficient for computation and analysis in R. Abbreviate Labels: renaming_vector &lt;- c(\"Year-on-year changes, in per cent\" = \"YoY-Growth\", \"Index, 2010 = 100\" = \"Index\"): A vector called renaming_vector is created to store the new abbreviated labels. levels(RPP_labels$Unit) &lt;- renaming_vector[levels(RPP_labels$Unit)]: This line replaces the old labels in the Unit column with the new abbreviated labels from renaming_vector. The renaming is better done after converting to factors because R will then only change the factor levels instead of having to traverse through each string to replace it, thus enhancing computational efficiency. 5.3.4 Reshape Data The process of reshaping data from wide to long format and vice versa is often critical for effective data analysis. For data frames, the reshape() function is commonly used, as explored in Chapter 4.5.8. For tibbles, we generally use pivot_longer() and pivot_wider() functions from the tidyr package. These are discussed in detail in Chapter 4.6.8. From Wide to Long Format Let’s take a look at the initial structure of our data. Tables 5.4 and 5.5 display our RPP data and its corresponding labels. Table 5.4: RPP Data in Wide Format Period Q:US:R:628 Q:US:R:771 Q:XM:R:628 Q:XM:R:771 2022-09-30 158.1889 2.7320 115.2157 -2.4634 2022-12-31 157.5075 -0.1327 110.6743 -6.4343 2023-03-31 157.7020 -2.4097 109.2423 -7.1131 Table 5.5: RPP Labels in Long Format Code Region Measure Unit Q:US:R:628 United States Real Index Q:US:R:771 United States Real YoY-Growth Q:XM:R:628 Euro area Real Index Q:XM:R:771 Euro area Real YoY-Growth In Table 5.4, the data is organized in a wide format, where each variable forms a column. Conversely, Table 5.5 displays the labels in a long format, where each variable corresponds to a row. To align these two different structures, we employ the pivot_longer() function from the tidyr package. # Load the necessary package library(&quot;tidyr&quot;) # Reshape the data from a wide to a long format. RPP &lt;- RPP_wide %&gt;% pivot_longer(cols = -Period, names_to = &quot;Code&quot;, values_to = &quot;Value&quot;, values_drop_na = TRUE) The pivot_longer() function in the code reshapes the data from wide to long format. Here’s the breakdown of its arguments: cols = -Period: Specifies that all columns except “Period” will be converted to a long format. names_to = \"Code\": The names of the reshaped columns will be stored in a new column named “Code”. values_to = \"Value\": The values from the reshaped columns will go into a new column named “Value”. values_drop_na = TRUE: Any rows that would contain NA values in the new “Value” column are removed. So, the function takes multiple columns and condenses them into two: one for the Code (original column names) and one for the Value (original data values), as is evident in Table 5.6. Table 5.6: RPP Data in Long Format Period Code Value 2022-09-30 Q:US:R:628 158.1889 2022-09-30 Q:US:R:771 2.7320 2022-09-30 Q:XM:R:628 115.2157 2022-09-30 Q:XM:R:771 -2.4634 2022-12-31 Q:US:R:628 157.5075 2022-12-31 Q:US:R:771 -0.1327 2022-12-31 Q:XM:R:628 110.6743 2022-12-31 Q:XM:R:771 -6.4343 2023-03-31 Q:US:R:628 157.7020 2023-03-31 Q:US:R:771 -2.4097 2023-03-31 Q:XM:R:628 109.2423 2023-03-31 Q:XM:R:771 -7.1131 With the data now in long format, it becomes feasible to merge it with the labels. For this, we use the left_join() function from the dplyr package. This type of join is explained in Chapter 4.6.6. The advantage of a “left” join is that it ensures no data is omitted; if any labels are missing, the associated data remains intact, with the label fields populated as NA. # Perform the merging operation RPP &lt;- RPP %&gt;% left_join(y = RPP_labels %&gt;% select(Code, Region, Unit, Measure), by = &quot;Code&quot;) %&gt;% select(-Value, everything(), Value) The left_join() function is used to merge two datasets based on a common column, in this case, “Code”. y = RPP_labels %&gt;% select(Code, Region, Unit, Measure): Specifies that the second dataset in the join is a subset of RPP_labels, containing only the columns “Code”, “Region”, “Unit”, and “Measure,” omitting redundant or overly detailed columns like “Frequency” and “Coverage.” by = \"Code\": Specifies that the datasets will be merged based on the “Code” column. select(-Value, everything(), Value): Rearranges the columns, moving the “Value” column to the end. The left_join() operation will keep all rows from RPP (the left dataset), and add corresponding data from RPP_labels where the “Code” matches. If there’s a “Code” in RPP that is not found in RPP_labels, that row will still be in the output, but the new columns (“Region”, “Unit”, “Measure”) will have NA values. After merging, the data and labels appear fully integrated in a long format, as can be seen in Table 5.7. Table 5.7: RPP Data with Labels in Long Format Period Code Region Unit Measure Value 2022-09-30 Q:US:R:628 United States Index Real 158.1889 2022-09-30 Q:US:R:771 United States YoY-Growth Real 2.7320 2022-09-30 Q:XM:R:628 Euro area Index Real 115.2157 2022-09-30 Q:XM:R:771 Euro area YoY-Growth Real -2.4634 2022-12-31 Q:US:R:628 United States Index Real 157.5075 2022-12-31 Q:US:R:771 United States YoY-Growth Real -0.1327 2022-12-31 Q:XM:R:628 Euro area Index Real 110.6743 2022-12-31 Q:XM:R:771 Euro area YoY-Growth Real -6.4343 2023-03-31 Q:US:R:628 United States Index Real 157.7020 2023-03-31 Q:US:R:771 United States YoY-Growth Real -2.4097 2023-03-31 Q:XM:R:628 Euro area Index Real 109.2423 2023-03-31 Q:XM:R:771 Euro area YoY-Growth Real -7.1131 The data in long format, showcased in Table 5.7, is often referred to as “tidy” data. This format makes it easier to work with the data in future analysis steps. From Long to Wide Format Some data manipulation tasks and R packages need data in a specific format. For example, the xts package for time series, as mentioned in Chapter 4.8, requires data in wide format. In this format, each row corresponds to a time period, and each column represents a variable. The challenge in switching from long to wide format is that the long format has multiple descriptor columns, like Region, Unit, and Measure. In wide format, this results in complex column names, like Region.Unit.Measure, as displayed in the following Table ??. Table 5.8: RPP Data with Labels in Wide Format Period Italy.Nominal.Index Italy.Nominal.YoY-Growth Italy.Real.Index 2022-12-31 90.4318 2.6923 70.9075 2023-03-31 90.4318 1.0407 70.6296 2023-06-30 92.2100 0.7401 71.7172 The conversion from long to wide format, i.e. from Table 5.7 to 5.8, is achieved using the pivot_wider() function from the tidyr package. # Reshape from long to wide RPP_with_labels_wide &lt;- RPP %&gt;% pivot_wider(id_cols = &quot;Period&quot;, names_from = c(&quot;Region&quot;, &quot;Measure&quot;, &quot;Unit&quot;), values_from = &quot;Value&quot;, names_sep = &quot;.&quot;) In this R code snippet, the function pivot_wider() from the tidyr package is used to transform a long-format dataset (RPP) into a wide-format one (RPP_with_labels_wide). Here’s a breakdown of the function’s arguments: id_cols = \"Period\": This sets the “Period” column as the identifier column, around which the reshaping will happen. names_from = c(\"Region\", \"Measure\", \"Unit\"): The unique combinations of values in these columns (“Region,” “Measure,” “Unit”) will become new column names in the wide-format dataset. values_from = \"Value\": Specifies that the numerical “Value” column from the long-format dataset will be used to populate the cells in the new wide-format columns. names_sep = \".\": The new column names generated from “Region,” “Measure,” and “Unit” will be separated by a period (.) So the resulting RPP_with_labels_wide dataset, visualized in Table 5.8, will have a column for each unique combination of “Region,” “Measure,” and “Unit,” and these will be populated with the corresponding “Value” from the original RPP dataset. The “Period” column remains as the identifier. The resulting wide-format data can be hard to read because of the complex column names. One solution is to filter and focus on specific subsets before converting to wide format. For instance, to focus on the real RPP index, we first filter the data: # Select real RPP index RPP_real_index &lt;- RPP %&gt;% filter(Measure == &quot;Real&quot;, Unit == &quot;Index&quot;) %&gt;% select(Period, Region, `Real RPP` = Value) Then, we can convert this filtered data to wide format: # Reshape from wide to long RPP_real_index_wide &lt;- RPP_real_index %&gt;% pivot_wider(id_cols = &quot;Period&quot;, names_from = c(&quot;Region&quot;), values_from = &quot;Real RPP&quot;) Here, the column names become simpler, just representing different regions, as shown in Table 5.9. It makes the wide-format data easier to work with. Table 5.9: Real RPP Index by Region in Wide Format Period Australia Colombia Japan Germany South Africa United States 2022-09-30 133.8846 142.4936 122.0932 153.4093 93.1363 158.1889 2022-12-31 126.6149 138.2781 119.8312 142.6217 93.1787 157.5075 2023-03-31 122.4054 138.2344 123.0206 136.4207 92.2980 157.7020 By focusing on a subset, we simplify the wide-format data, making it more manageable for analysis. 5.3.5 Layered Line Graphs Line graphs serve to illustrate temporal trends and patterns. In R, there are two main approaches for creating such graphs: the base R plotting system, featuring functions like plot() and matplot(), and the ggplot2 package by Wickham et al. (2024), which employs the ggplot() function. Although both visualize data, there are stark differences in their approaches, particularly in their data format requirements, layering philosophies, and extensibility. Basic Line Graphs Using matplot() Base R functions like plot() and matplot() operate under a procedural paradigm. Essentially, what you see is what you get. Each function call adds or modifies specific attributes of the graph, and there’s limited scope for dynamically layering or extending the plot. In the basic plot(x, y) function, a vector y is plotted relative to another vector x. When y is a matrix, matplot(x, y) serves as the alternative. Generally, matplot() anticipates data in wide format, where each time-series or category is allocated its own column. While this approach is convenient for simple visualizations, it may become cumbersome for more complex, multi-layered visualizations. Here’s how to use matplot() to plot residential property prices for all regions, assuming the data is in wide format: # Produce line plots matplot(x = RPP_real_index_wide$Period, y = RPP_real_index_wide[, -1], type = &quot;l&quot;, main = &quot;Residential Property Prices - All Regions&quot;, xlab = &quot;Period&quot;, ylab = &quot;Index, 2010 = 100&quot;, ylim = c(0, 250)) This R code chunk creates a line plot of residential property prices for all regions using the matplot() function. Here’s a breakdown of the code: matplot(x = RPP_real_index_wide$Period, y = RPP_real_index_wide[, -1], type = \"l\"): The function matplot() is used to plot multiple lines. The x argument specifies the time period to be plotted on the x-axis. The y argument specifies the data for the y-axis, omitting the first column which contains the time period. The type = \"l\" argument indicates that lines should be used for the plot. main = \"Residential Property Prices - All Regions\": Sets the title of the graph. xlab = \"Period\": Labels the x-axis as “Period”. ylab = \"Index, 2010 = 100\": Labels the y-axis as “Index, 2010 = 100”. ylim = c(0, 250): Sets the limits for the y-axis to range from 0 to 250. In summary, this code chunk generates Figure 5.9, a line plot that displays the trend of residential property prices across all regions over a specified period. Note that the regions are not labeled to avoid crowding the plot. Figure 5.9: Basic Plot of Residential Property Prices - All Regions To add labels in a less cluttered plot, you can manually select a subset of regions as shown in the following example: # Select variables to plot selected_countries &lt;- c( &quot;Australia&quot;, # Oceania &quot;Colombia&quot;, # South America &quot;Japan&quot;, # Asia &quot;Germany&quot;, # Europe &quot;South Africa&quot;, # Africa &quot;United States&quot; # North America ) # Multiple line plots matplot(x = RPP_real_index_wide$Period, y = RPP_real_index_wide[, selected_countries], type = &quot;l&quot;, main = &quot;Residential Property Prices - Selected Countries&quot;, xlab = &quot;Period&quot;, ylab = &quot;Index, 2010 = 100&quot;, lty = 1:6, col = 1:6, lwd = 1 + 0:5 / 3) legend(x = &quot;topleft&quot;, legend = selected_countries, lty = 1:6, col = 1:6, lwd = 1 + 0:5 / 3, seg.len = 2, ncol = 1) This R code chunk creates a line plot for residential property prices for a selected group of countries. Here’s what each part of the code does: selected_countries &lt;- c(...) : This line defines a vector named selected_countries that contains the names of countries you want to plot. They are categorized by continent for clarity. matplot(x = RPP_real_index_wide$Period, y = RPP_real_index_wide[, selected_countries], type = \"l\"): The matplot() function plots multiple lines. The x argument specifies what should be on the x-axis (time period). The y argument specifies which columns (countries) from the data frame should be plotted on the y-axis. The type = \"l\" specifies that lines should be used for plotting. main = \"Residential Property Prices - Selected Countries\": Sets the title of the graph. xlab = \"Period\", ylab = \"Index, 2010 = 100\": These arguments label the x-axis and y-axis, respectively. lty = 1:6, col = 1:6, lwd = 1 + 0:5 / 3: These arguments set the line types (lty), colors (col), and line widths (lwd) for the lines in the plot. legend(x = \"topleft\", legend = selected_countries, ...) : Adds a legend at the top left of the plot. The legend shows which line corresponds to which country. The line types, colors, and widths are set similar to those in the plot. Overall, this code chunk generates Figure 5.10, a line plot featuring residential property prices for the selected countries over time. The plot is customized with titles, labels, and a legend for better readability and interpretation. Figure 5.10: Basic Plot of Residential Property Prices - Selected Countries Layered Line Graph Using ggplot() In contrast to base R plotting functions, ggplot2 adopts an approach known as the Grammar of Graphics. This paradigm treats a plot as a layered composition, where each layer serves a specific role. For example, you may start with a layer that defines the data and the axes, then add layers for graphical elements like lines, points, or bars. Further layers could include labels, annotations, or even statistical transformations. In essence, the Grammar of Graphics allows you to break down complex visualizations into understandable parts and then reassemble them. This layered structure offers two main advantages: Flexibility: You can add, remove, or modify individual layers without affecting the rest of the plot. This enables a high level of customization. Extensibility: Because each layer serves a defined purpose, it is easier to extend the functionality of a plot by adding new types of layers or modifying existing ones. This makes it well-suited for creating complex, multi-faceted visualizations. This layered approach aligns well with long-format data, where different description columns (e.g., Region, Measure, Unit) can be mapped to different layers or facets, enabling more intricate graphical storytelling. ggplot2 requires data in long format. In long format, each row is a single observation, making it easier to map different aesthetic attributes (like color, shape, size) to variables in the data. First, let’s select and filter the data: # Choose countries to plot selected_countries &lt;- c( &quot;Australia&quot;, # Oceania &quot;Colombia&quot;, # South America &quot;Japan&quot;, # Asia &quot;Germany&quot;, # Europe &quot;South Africa&quot;, # Africa &quot;United States&quot; # North America ) # Select variables to plot and remove missing values RPP_graph &lt;- RPP %&gt;% filter(Region %in% selected_countries &amp; Unit == &quot;Index&quot; &amp; Measure == &quot;Real&quot;) %&gt;% na.omit() Creating a simple line graph: # Load package library(&quot;ggplot2&quot;) # Make line plot ggplot(data = RPP_graph, mapping = aes(x = Period, y = Value, color = Region)) + geom_line() This R code chunk creates a line plot using the ggplot() function from the ggplot2 package. Here’s what each part of the code does: ggplot(data = RPP_graph, mapping = aes(x = Period, y = Value, color = Region)): This initializes the plot using the ggplot() function. data = RPP_graph: Specifies the data frame that contains the data to be plotted. mapping = aes(x = Period, y = Value, color = Region): The aes() function defines the aesthetic mapping, specifying which variables in the data frame are to be used for the x-axis (Period), y-axis (Value), and color (Region). geom_line(): This is a layer that adds the line geometry to the plot. It creates lines connecting the points as defined by the aes() mappings. The + operator is used to add the line layer (geom_line()) to the initial ggplot() object. In summary, this code chunk generates a line plot, displayed in Figure 5.11, where the x-axis represents the Period, the y-axis represents the Value, and lines of different colors represent different Regions. Figure 5.11: Residential Property Prices - Selected Countries Adding layers for a more complex graph: # Make line plot RPP %&gt;% filter(Region %in% selected_countries) %&gt;% ggplot(aes(x = Period, y = Value, color = Region)) + geom_line() + facet_grid(rows = vars(Unit), cols = vars(Measure), scales = &quot;free_y&quot;) This R code chunk uses a sequence of operations to create a more complex line plot using the ggplot2 package. Here’s a breakdown: RPP %&gt;% filter(Region %in% selected_countries) %&gt;%: This part filters the RPP data frame to include only the regions specified in selected_countries. The %&gt;% operator pipes the filtered data frame into the following ggplot() function. ggplot(aes(x = Period, y = Value, color = Region)): Initializes the ggplot with aesthetic mappings. x = Period: Maps the Period variable to the x-axis. y = Value: Maps the Value variable to the y-axis. color = Region: Differentiates lines by the Region variable, assigning different colors to each region. geom_line(): Adds the line geometry to the plot, connecting points as defined by the aes() mappings. facet_grid(rows = vars(Unit), cols = vars(Measure), scales = \"free_y\"): Adds facets (sub-plots) to the plot. rows = vars(Unit): Creates separate rows for each unique value of the Unit variable. cols = vars(Measure): Creates separate columns for each unique value of the Measure variable. scales = \"free_y\": Allows the y-axis scale to vary between different facets, adapting to the data within each sub-plot. The + operator is used to combine all these components into a single plot object. In summary, this code chunk creates a faceted line plot, shown in Figure 5.12, based on the Period, Value, and Region variables. The facets are arranged by unique values of Unit and Measure Figure 5.12: Residential Property Prices - Selected Countries Hence, while matplot() is more straightforward and procedural, it’s less flexible when it comes to layering and typically requires wide-format data. On the other hand, ggplot() offers a more abstract, layered approach to plotting, thrives on long-format data, and allows for more complex and customizable visualizations. The choice between the two often depends on the complexity of the visualization task at hand and the format in which your data exists. 5.3.6 Merge Data To examine the relationship between residential property prices (RPPs) and policy rates, this section outlines how to import policy rate statistics from Bank for International Settlements (2023a) and merge them with the previously acquired RPP data. Given that policy rates are available on a monthly basis and residential property prices are reported quarterly, data aggregation is a necessary step prior to merging. Import Policy Rates Just as with Residential Property Prices (RPP), policy rates are stored in an Excel workbook. The readxl package is used to import the data. # Import Excel worksheet: &quot;Summary Documentation&quot; PR_labels &lt;- read_excel( path = &quot;files/cbpol.xlsx&quot;, sheet = &quot;Summary Documentation&quot;, col_names = TRUE, # The file has a header row with names na = &quot;-&quot; # Character string representing NA values ) # Convert all character vectors to factors PR_labels &lt;- PR_labels %&gt;% mutate(across(where(is.character), as.factor)) %&gt;% rename(Region = Country) %&gt;% mutate(Measure = factor(&quot;Nominal&quot;, levels = c(&quot;Real&quot;, &quot;Nominal&quot;))) # Import Excel worksheet: &quot;Monthly Series&quot; PR_wide &lt;- read_excel( path = &quot;files/cbpol.xlsx&quot;, sheet = &quot;Monthly Series&quot;, col_names = TRUE, # The file has a header row with names na = &quot;&quot;, # Character string representing NA values skip = 3 # Skips the first three rows ) Reshape Policy Rates Post-import, the policy rates data is reshaped from wide to long format for easier manipulation and to match the format of the RPP data. # Reshape from wide to long PR &lt;- PR_wide %&gt;% pivot_longer(cols = -Period, names_to = &quot;Code&quot;, values_to = &quot;Value&quot;, values_drop_na = TRUE, names_transform = list(Code = factor)) # Merge with labels PR &lt;- PR %&gt;% left_join(y = PR_labels %&gt;% select(Code, Region, Measure, Unit), by = &quot;Code&quot;) %&gt;% select(-Value, everything(), Value) Aggregate Policy Rates The next challenge is to reconcile the different data frequencies. The zoo package by Zeileis, Grothendieck, and Ryan (2023) is used to create a quarterly date variable, which serves as the basis for aggregating monthly policy rates to a quarterly level. # Load R package library(&quot;zoo&quot;) # Create quarterly period PR$Quarter &lt;- as.yearqtr(PR$Period) RPP$Period &lt;- as.yearqtr(RPP$Period) Then aggregate the policy rates based on this quarterly date variable: # Aggregate to quarterly frequency PR_q &lt;- PR %&gt;% group_by(Quarter, Code, Region, Measure, Unit) %&gt;% summarize(Value = mean(Value, na.rm = TRUE)) %&gt;% ungroup() %&gt;% rename(Period = Quarter) Merge Policy Rates with House Prices After harmonizing the frequencies, we can now merge the datasets based on their respective time periods. # Merge RPP and policy rate data BIS &lt;- RPP %&gt;% mutate(Indicator = &quot;Residential Property Prices&quot;) %&gt;% bind_rows(PR_q %&gt;% mutate(Indicator = &quot;Policy Rate&quot;)) %&gt;% mutate(Code = factor(Code), Indicator = factor(Indicator)) %&gt;% select(Period, Code, Indicator, -Value, everything(), Value) Here, we also introduce a new column, “Indicator”, to distinguish between Residential Property Prices and Policy Rates, making it easier to visualize them together later. By completing these steps, the datasets are now merged and ready for deeper analysis, allowing for a comprehensive examination of the relationship between residential property prices and policy rates. Plot Policy Rates and House Prices Finally, we visualize the relationship between policy rates and residential property prices for a selection of countries. The ggplot2 package is employed for this visualization. # Load package library(&quot;ggplot2&quot;) # Choose countries to plot selected_countries &lt;- c( &quot;Australia&quot;, # Oceania &quot;Colombia&quot;, # South America &quot;Japan&quot;, # Asia &quot;Germany&quot;, # Europe &quot;South Africa&quot;, # Africa &quot;United States&quot; # North America ) # Make line plot BIS %&gt;% filter(Region %in% selected_countries) %&gt;% filter(Indicator == &quot;Policy Rate&quot; | (Indicator == &quot;Residential Property Prices&quot; &amp; Unit == &quot;Index&quot; &amp; Measure == &quot;Real&quot;)) %&gt;% ggplot(aes(x = Period, y = Value, color = Region)) + geom_line() + facet_wrap(facets = vars(Indicator), scales = &quot;free_y&quot;) + theme(legend.position = &quot;bottom&quot;, legend.justification = &quot;left&quot;) + guides(color = guide_legend(direction = &quot;vertical&quot;, nrow = 2)) + xlim(as.numeric(as.yearqtr(&quot;1970-01-01&quot;)), as.numeric(max(BIS$Period))) Figure 5.13: RPP and Policy Rates The resulting plot, shown in Figure 5.13, enables us to compare the trends in residential property prices and policy rates across different countries, providing valuable insights for both policymakers and researchers. 5.3.7 Transform Data To analyze the dynamics of residential property prices and policy rates, various data transformations are necessary. These transformations are designed to create meaningful metrics for comparative analyses. This section outlines the steps involved in the data transformation process. Compute Price Level Real Residential Property Prices (Real RPP) represent the value of residential property after adjusting for general changes in the price level, commonly measured by the Consumer Price Index (CPI). In other words, Real RPP accounts for the effects of inflation, providing a more accurate measure of the change in property value over time. Mathematically, Real RPP can be expressed as: \\[ R_t = 100 \\left(\\frac{N_t}{CPI_t} \\right) \\] Where: \\(R_t\\): Real RPP at time \\(t\\) \\(N_t\\): Nominal RPP at time \\(t\\) \\(CPI_t\\): Consumer Price Index at time \\(t\\), normalized to 100 at a specific base year. Given the definition of Real RPP, we can rearrange the formula to solve for the Consumer Price Index: \\[ CPI_t = 100 \\left( \\frac{N_t}{R_t} \\right) \\] Thus, if we have both Real and Nominal RPP, we can easily compute the Consumer Price Index. The code snippet below computes the CPI and adds the new data to the existing data frame BIS. # Compute CPI BIS &lt;- BIS %&gt;% filter(Indicator == &quot;Residential Property Prices&quot;) %&gt;% pivot_wider(id_cols = c(&quot;Period&quot;, &quot;Region&quot;, &quot;Unit&quot;), names_from = &quot;Measure&quot;, values_from = c(&quot;Value&quot;, &quot;Code&quot;), names_sep = &quot;__&quot;) %&gt;% mutate(Indicator = &quot;Consumer Price Index&quot;, Measure = &quot;Nominal&quot;, Value = case_when( Unit == &quot;YoY-Growth&quot; ~ Value__Nominal - Value__Real, Unit == &quot;Index&quot; ~ 100 * Value__Nominal / Value__Real, .default = NA), Code = case_when( !is.na(Code__Nominal) &amp; !is.na(Code__Real) ~ paste(Code__Nominal, Code__Real, sep = &quot; &amp; &quot;), .default = NA)) %&gt;% na.omit() %&gt;% select(-contains(&quot;__&quot;)) %&gt;% bind_rows(BIS) %&gt;% select(-Value, everything(), Value) The code performs several steps to compute the Consumer Price Index (CPI) based on the existing BIS data frame, which contains Residential Property Prices (RPP) data. Below are the operations explained step by step: filter(Indicator == \"Residential Property Prices\"): Filters rows where the Indicator column contains “Residential Property Prices.” pivot_wider(...): Converts the data from long format to wide format. It keeps “Period,” “Region,” and “Unit” as identifier columns and transforms the “Measure” column into new columns, specifically generating Value__Nominal and Value__Real. These new columns are used in subsequent calculations to compute the Consumer Price Index (CPI). mutate(Indicator = \"Consumer Price Index\", ...): Adds a new Indicator column set to “Consumer Price Index” and conditionally calculates a new Value column based on the Unit column using the case_when() function: For Unit == \"YoY-Growth\": Value = \\(\\pi_t = n_t - r_t\\) For Unit == \"Index\": Value = \\(P_t = 100 \\left(\\frac{N_t}{R_t}\\right)\\) na.omit(): Removes rows with any NA values. select(-contains(\"__\")): Removes columns with names containing double underscores, effectively removing Value__Nominal and Value__Real from the data frame. bind_rows(BIS): Appends the newly computed CPI data to the original BIS data frame. select(-Value, everything(), Value): Rearranges the columns, moving the newly computed Value column to the end for better readability. Each of these steps contributes to the computation of the Consumer Price Index (CPI) based on existing Residential Property Prices (RPP) and then adds this computed data back into the original BIS data frame. Compute Inflation In this section, we delve into the calculation of inflation, which is the growth rate of prices. The formula for calculating inflation is as follows: \\[ \\% \\Delta P_t \\equiv 100 \\left(\\frac{P_t - P_{t-1}}{P_{t-1}}\\right) \\% \\approx 100 \\left( \\ln P_t - \\ln P_{t-1} \\right) \\% \\tag{5.1} \\] To implement this calculation, the code snippet utilizes several functions from the dplyr package. One critical function is group_by(). This function is used to split the data into subsets based on the specified columns (Code, Indicator, Measure, and Region in this case). Subsequent operations are then applied within these subsets. # Compute growth rates BIS_transformed &lt;- BIS %&gt;% filter(is.na(Unit) | Unit != &quot;YoY-Growth&quot;) %&gt;% group_by(Code, Indicator, Measure, Region) %&gt;% arrange(Period) %&gt;% mutate(Level = Value, Growth = ifelse(test = Unit == &quot;Index&quot;, yes = 400 * (log(Value) - dplyr::lag(log(Value))), no = NA), Value = NULL) %&gt;% ungroup() Here’s a detailed breakdown of the code: filter(is.na(Unit) | Unit != \"YoY-Growth\"): This line ensures that only the relevant data is considered by excluding rows where the Unit column is “YoY-Growth” and keeping rows where Unit is NA. group_by(Code, Indicator, Measure, Region): The data is grouped by four variables. This is pivotal for the subsequent operations, which will be performed within these grouped subsets. arrange(Period): This function sorts the data within each group by the time period, ensuring a chronological order. mutate(Level = Value, ...): A new column named “Level” is created, which is a duplicate of the Value column. Growth = ifelse(...): The “Growth” column is calculated based on the logarithmic differences between the current and previous periods. The calculation is done only when the Unit column equals “Index”. Value = NULL: The original Value column is removed after the new columns “Level” and “Growth” are generated. ungroup(): This last step removes the grouping structure, returning the data frame to its original, ungrouped state. The use of group_by() ensures that each subset of data, defined by the combination of Code, Indicator, Measure, and Region, is treated as a separate entity. This is important when using functions like lag(), as the calculation would be restricted to within each group, ensuring no values are carried over from one group to another. In summary, group_by() allows for segmented and independent calculations within a dataset, making it a powerful tool for data manipulation and analysis. Reshape Transformed Data Data is then reshaped to make it more convenient for further analysis. The pivot_longer() function from the tidyr package is used to convert the data from wide to long format. # Reshape from wide to long BIS_t &lt;- BIS_transformed %&gt;% pivot_longer(cols = c(&quot;Level&quot;, &quot;Growth&quot;), names_to = &quot;Transformation&quot;, values_to = &quot;Value&quot;, values_drop_na = TRUE, names_transform = list(Transformation = function(x) factor( x, levels = c(&quot;Level&quot;, &quot;Growth&quot;)))) Here’s the breakdown of the code snippet: pivot_longer(cols = c(\"Level\", \"Growth\"), ...): Specifies the columns “Level” and “Growth” to be converted from wide to long format. names_to = \"Transformation\": The names of the original columns (“Level” and “Growth”) are stored in a new column named “Transformation.” values_to = \"Value\": The values of the original columns are stored in a new column called “Value.” values_drop_na = TRUE: Any NA values in the “Level” and “Growth” columns will be dropped. names_transform = list(...): This part reorders the levels of the “Transformation” factor column to have “Level” come before “Growth.” In summary, the pivot_longer() function is used to streamline the data’s structure. The “Level” and “Growth” columns are folded into a single “Value” column, and a new column “Transformation” is added to specify the original column from which each value originates. This results in a long-format data frame that can be more convenient for downstream data analysis tasks. Compute Real Interest Rate The real policy rate is essentially the nominal interest rate adjusted for inflation. It can be mathematically represented as: \\[ \\text{Real Policy Rate}_t = \\text{Policy Rate}_t - \\text{CPI Inflation}_t \\] The real interest rate is a key economic indicator that offers a more accurate measure of the return on investments. It represents the true cost of borrowing, as it accounts for the eroding effects of inflation on the purchasing power of money, making it a critical factor in financial decision-making. In the following R code, the real interest rate is computed by subtracting the growth of the Consumer Price Index (CPI) from the Policy Rate, providing an interest rate that accounts for inflationary changes. # Compute real interest rate BIS_t &lt;- BIS_t %&gt;% filter(Indicator %in% &quot;Policy Rate&quot; | (Indicator %in% &quot;Consumer Price Index&quot; &amp; Transformation == &quot;Growth&quot;)) %&gt;% pivot_wider(id_cols = c(&quot;Period&quot;, &quot;Region&quot;), names_from = &quot;Indicator&quot;, values_from = c(&quot;Value&quot;, &quot;Code&quot;), names_sep = &quot;__&quot;) %&gt;% mutate(Indicator = &quot;Policy Rate&quot;, Measure = &quot;Real&quot;, Transformation = factor(&quot;Level&quot;, levels = c(&quot;Level&quot;, &quot;Growth&quot;)), Value = `Value__Policy Rate` - `Value__Consumer Price Index`, Code = paste(`Code__Policy Rate`, `Code__Consumer Price Index`, sep = &quot; &amp; &quot;)) %&gt;% na.omit() %&gt;% select(-contains(&quot;__&quot;)) %&gt;% bind_rows(BIS_t) %&gt;% select(-Value, everything(), Value) The code snippet computes the real interest rate by performing several transformations on the dataset: filter(Indicator %in% \"Policy Rate\" | ...): Filters rows where the Indicator is either “Policy Rate” or the growth rate of the “Consumer Price Index.” pivot_wider(id_cols = c(\"Period\", \"Region\"), ...): Converts the data from long format to wide format, using “Period” and “Region” as identifier columns. mutate(Indicator = \"Policy Rate\", ...): Adds a new column named “Indicator” with the value “Policy Rate” and computes a new column Value as the difference between the Policy Rate and the Consumer Price Index (CPI) growth. Code = paste(..., sep = \" &amp; \"): Combines the codes for the Policy Rate and CPI into a single code, separated by ” &amp; “. na.omit(): Removes rows with any NA values. select(-contains(\"__\")): Removes any columns with names that contain double underscores, which in this case are the temporary columns Value__Policy Rate and Value__Consumer Price Index. bind_rows(BIS_t): Appends the new data back to the original BIS_t data frame. select(-Value, everything(), Value): Rearranges the columns so that the new Value column is at the end. By executing these steps, the real interest rate is calculated and added to the dataset, making it available for further analysis. 5.3.8 Stationary Data Statistical measures like mean, standard deviation, and correlation are meaningful only for stationary time series. A stationary time series maintains constant statistical properties over time, such as a constant mean and variance. Non-stationary time series, where the mean or variance drifts over time, are not suitable for such analysis, as current and past values are not indicative of future behavior. To illustrate, consider the U.S. house price index and its growth rate, the latter being a stationary series. The following figure contrasts the statistical properties of these two series: Figure 5.14: U.S. Residential Property Price Index: Non-Stationary Levels and Stationary Growth Rates Figure 5.14 shows that the mean and volatility for the house price index are not stable, rendering them useless for making predictions or understanding typical behavior. In contrast, the mean and volatility of the growth rate are stable and thus provide insights. The R code for generating Figure 5.14 is detailed below. # Load package library(&quot;ggplot2&quot;) BIS_t %&gt;% group_by(Region, Indicator, Measure, Transformation) %&gt;% mutate(Mean = mean(Value, na.rm = TRUE), SD = sd(Value, na.rm = TRUE)) %&gt;% filter(Region == &quot;United States&quot;, Indicator == &quot;Residential Property Prices&quot;, Measure == &quot;Real&quot;) %&gt;% ggplot(aes(x = Period, y = Value)) + geom_line() + geom_hline(aes(yintercept = Mean), color = &quot;red&quot;) + geom_hline(aes(yintercept = Mean + SD), color = &quot;blue&quot;, linetype = &quot;dashed&quot;) + geom_hline(aes(yintercept = Mean - SD), color = &quot;blue&quot;, linetype = &quot;dashed&quot;) + geom_text(aes(label = paste(&quot;Mean:&quot;, round(Mean, 2)), x = Inf, y = min(Value)), hjust = 1, vjust = -1.5, size = 3, color = &quot;red&quot;) + geom_text(aes(label = paste(&quot;Volatility (Standard Deviation):&quot;, round(SD, 2)), x = Inf, y = min(Value)), hjust = 1, vjust = 0, size = 3, color = &quot;blue&quot;) + facet_wrap(facets = vars(Transformation), scales = &quot;free&quot;) + ggtitle(&quot;U.S. Residential Property Prices&quot;) Here is the explanation of the code step-by-step: Load package library(\"ggplot2\"): Loads the ggplot2 package for data visualization. Data Preparation group_by(Region, Indicator, Measure, Transformation): Groups the data by several variables: “Region,” “Indicator,” “Measure,” and “Transformation.” mutate(Mean = mean(Value, na.rm = TRUE), SD = sd(Value, na.rm = TRUE)): Calculates the mean and standard deviation of ‘Value’ for each group. filter(Region == \"United States\", Indicator == \"Residential Property Prices\", Measure == \"Real\"): Filters the data to include only observations related to real residential property prices in the United States. Plotting ggplot(aes(x = Period, y = Value)): Initializes ggplot and sets ‘Period’ as the x-axis and ‘Value’ as the y-axis. geom_line(): Adds a line to the plot to represent the time series data. geom_hline(...): Adds horizontal lines to indicate the mean and standard deviation. geom_text(...): Adds text to the plot for the mean and standard deviation. facet_wrap(facets = vars(Transformation), scales = \"free\"): Separates the plot into subplots based on the “Transformation” variable (Level or Growth), allowing for different y-axis scales. ggtitle(\"U.S. Residential Property Prices\"): Adds a title to the plot. The resulting plot will show the time series data along with red lines indicating the mean and blue dashed lines showing one standard deviation above and below the mean. The purpose is to illustrate that while the growth rate series might be stationary (constant mean and variance over time), the house price index itself is not. Therefore, the latter is not suitable for predictions based on historical mean and volatility. Because of these considerations, we’ll focus on stationary variables. The BIS_stationary dataset is created to include only these variables. We filter out only the growth transformations for “Residential Property Prices” and “Consumer Price Index,” as well as the level for “Policy Rate.” # Select stationary variables BIS_stationary &lt;- BIS_t %&gt;% filter((Indicator == &quot;Residential Property Prices&quot; &amp; Transformation == &quot;Growth&quot;)| (Indicator == &quot;Consumer Price Index&quot; &amp; Transformation == &quot;Growth&quot;)| (Indicator == &quot;Policy Rate&quot; &amp; Transformation == &quot;Level&quot;)) %&gt;% mutate(Transformation = NULL, Indicator = case_when( Indicator == &quot;Residential Property Prices&quot; ~ &quot;RPP Inflation&quot;, Indicator == &quot;Consumer Price Index&quot; ~ &quot;CPI Inflation&quot;, .default = Indicator)) To simplify the dataset, we incorporate the information from the Measure column into the variable names and remove the Measure column. This leads to clearer variable names like “Real RPP Inflation” instead of just “RPP Inflation.” # Create the BIS_core dataset BIS_stationary &lt;- BIS_stationary %&gt;% mutate(Indicator = ifelse(test = Indicator == &quot;CPI Inflation&quot;, yes = Indicator, no = paste(Measure, Indicator))) %&gt;% select(Period, Indicator, Region, Value) This approach ensures that our subsequent analyses are based on stationary time series, making the statistical measures meaningful for inference and prediction. 5.3.9 Export Data This section outlines the methods to export the processed residential property price (RPP) and policy rate data for further analysis or sharing. Three formats are covered: RDS, DSV, and Excel workbooks. Each format has its pros and cons. As discussed previously in Chapter 5.1.6, DSV files are more compatible than RDS files but lack the ability to preserve all data types and attributes, while RDS files do. The advantage of Excel workbooks over DSV files is that they can contain multiple datasets in one file, but they are less efficient and use more storage. First, we capture the source information for the datasets, using the tribble() function to create a tibble. # Source of data BIS_source &lt;- tribble( ~Data, ~Source, ~Documentation, &quot;Residential Property Prices (RPP) by the Bank for International Settlements&quot;, &quot;https://www.bis.org/statistics/pp.htm&quot;, &quot;http://www.bis.org/statistics/pp_selected_documentation.pdf&quot;, &quot;Policy Rates (PR) by the Bank for International Settlements (BIS)&quot;, &quot;https://www.bis.org/statistics/cbpol.htm&quot;, &quot;http://www.bis.org/statistics/cbpol/cbpol_doc.pdf&quot;) RDS To preserve all attributes and data types of an R object, the saveRDS() function is used. Below, we add the source data as an attribute and then save the object. # Attach source of data as attributes to the &quot;BIS_stationary&quot; object attr(BIS_stationary, &quot;source&quot;) &lt;- BIS_source # Create a folder to store exported files dir.create(path = &quot;exported-files&quot;, showWarnings = FALSE) # Save as RDS file saveRDS(object = BIS_stationary, file = &quot;exported-files/BIS_stationary.rds&quot;) To validate the saved RDS, we read it back and check for identity with the original object. # Read saved RDS file BIS_stationary_loaded &lt;- readRDS(file = &quot;exported-files/BIS_stationary.rds&quot;) identical(BIS_stationary, BIS_stationary_loaded) ## [1] TRUE # Clean up rm(BIS_stationary_loaded) DSV For broader software compatibility, DSV formats like CSV or TSV are advantageous. The write.table() or write_delim() functions can be used. # Load packages library(&quot;readr&quot;) # Create folder for exported files dir.create(path = &quot;exported-files&quot;, showWarnings = FALSE) # Export BIS data as CSV file write_delim(x = BIS_stationary, file = &quot;exported-files/BIS_stationary.csv&quot;, delim = &quot;,&quot;) The source information can also be saved as a separate CSV file. # Export source of BIS data as CSV file write_delim(x = BIS_source, file = &quot;exported-files/BIS_stationary_source.csv&quot;, delim = &quot;,&quot;) Excel Workbook Excel workbooks offer the benefit of consolidating multiple datasets into one file. While the readxl package is often used for reading Excel files, it doesn’t support writing. Packages like writexl or openxlsx can be used to write Excel files. The write_xlsx() function from the writexl package is used to export multiple data frames to an Excel workbook. Each data frame will be saved as a separate sheet in the workbook. # Load packages library(&quot;writexl&quot;) # Create object with information about Excel sheets BIS_sheet_content &lt;- tribble( ~`Excel Sheet`, ~Content, &quot;Content&quot;, &quot;Information about Excel sheets&quot;, &quot;Source&quot;, &quot;Information about data source&quot;, &quot;Raw&quot;, &quot;Raw RPP and PR data stored in long format&quot;, &quot;Transformed&quot;, &quot;Transformed data&quot;, &quot;Stationary&quot;, &quot;Stationary data&quot;) # Create folder for exported files dir.create(path = &quot;exported-files&quot;, showWarnings = FALSE) # Export data write_xlsx(x = list(&quot;Content&quot; = BIS_sheet_content, &quot;Source&quot; = BIS_source, &quot;Raw&quot; = BIS, &quot;Transformed&quot; = BIS_t, &quot;Stationary&quot; = BIS_stationary), path = &quot;exported-files/BIS_stationary.xlsx&quot;) Here’s what each parameter is doing: x = list(...): This list contains the data frames to be exported, each with a name that will become the Excel sheet name. \"Content\" = BIS_sheet_content: Exports the data frame BIS_sheet_content to a sheet named “Content”. \"Source\" = BIS_source: Exports the data frame BIS_source to a sheet named “Source”. \"Raw\" = BIS: Exports the data frame BIS to a sheet named “Raw”. \"Transformed\" = BIS_t: Exports the data frame BIS_t to a sheet named “Transformed”. \"Stationary\" = BIS_stationary: Exports the data frame BIS_stationary to a sheet named “Stationary”. path = \"exported-files/BIS_stationary.xlsx\": Specifies the directory and the name of the Excel workbook file to be created. So, the code will create an Excel workbook named BIS_stationary.xlsx in the exported-files directory. The workbook will contain five sheets: “Content”, “Source”, “Raw”, “Transformed”, and “Stationary”, each populated with the corresponding data frame. Both writexl and openxlsx have merits. While writexl is simpler and more straightforward, openxlsx offers more features, such as advanced formatting and the inclusion of charts or images. Your choice between the two should depend on your specific needs. Thus, you have multiple options for exporting your data, each with its own set of advantages and limitations. Choose the one that best suits your project requirements. 5.3.10 Organize Data For focused analysis, we concentrate on three essential variables: Real RPP Inflation, CPI Inflation, and Real Policy Rate. We also narrow down the geographical scope to four key economic regions: the United States, the Euro Area, China, and Emerging Markets. This subset of the data will be stored in a new tibble, BIS_core. It’s important to properly order and name categorical variables at this stage. Default sorting in tables or graphs is often alphabetical, which may not be ideal for our analytical purposes. To ensure meaningful representation in subsequent analyses, we convert the ‘Region’ and ‘Indicator’ columns into ordered factors. Here’s how this is implemented: # Define regions of interest and their desired names selected_regions = c( &quot;United States&quot; = &quot;United States&quot;, &quot;Euro area&quot; = &quot;Euro Area&quot;, &quot;China&quot; = &quot;China&quot;, &quot;Emerging market economies (aggregate)&quot; = &quot;Emerging Markets&quot; ) # Define variables of interest and their desired names selected_variables = c(&quot;Real RPP Inflation&quot; = &quot;Real RPP Inflation&quot;, &quot;CPI Inflation&quot; = &quot;CPI Inflation&quot;, &quot;Real Policy Rate&quot; = &quot;Real Policy Rate&quot;) # Create the BIS_core dataset BIS_core &lt;- BIS_stationary %&gt;% filter(Region %in% names(selected_regions), Indicator %in% names(selected_variables)) %&gt;% mutate(Region = factor(x = Region, levels = names(selected_regions), ordered = TRUE), Indicator = factor(x = Indicator, levels = names(selected_variables), ordered = TRUE)) %&gt;% arrange(Indicator, Region) # Rename factor levels levels(BIS_core$Region) &lt;- selected_regions[levels(BIS_core$Region)] levels(BIS_core$Indicator) &lt;- selected_variables[levels(BIS_core$Indicator)] In summary, the BIS_core tibble now comprises only the variables and regions that are pertinent for subsequent analysis, and these are appropriately ordered and efficiently renamed for effective data presentation and interpretation. 5.3.11 Univariate Statistics In this section, we present univariate statistics across different regions for CPI inflation, real home price inflation, and real policy rates. In the context of statistics, the term “univariate” refers to the analysis of a single variable without consideration of any other variables. Metrics like mean, standard deviation, minimum, and maximum are univariate statistics because they describe features of one variable in isolation. They summarize the central tendency, variability, and range of possible values for that single variable. In contrast, associative statistics examine relationships between two or more variables, which will be discussed in the next section. Correlation coefficients, covariances, and regression coefficients are examples of associative statistics. These metrics help us understand how variables interact with each other or how one variable might predict another. For the univariate statistics, we group the data by region, variable, and measure, then calculate summary statistics for each group, consisting of the time span, mean, and volatility. # Compute univariate statistics BIS_univariate_stats &lt;- BIS_core %&gt;% group_by(Indicator, Region) %&gt;% summarize( `Time Span` = paste(range(Period), collapse = &quot; - &quot;), Mean = mean(Value, na.rm = TRUE), Volatility = sd(Value, na.rm = TRUE) ) %&gt;% ungroup() In this R code block, the goal is to calculate univariate statistics for the dataset BIS_core: group_by(Indicator, Region): This function groups the data by the Indicator and Region variables. This ensures that the following calculations are done separately for each combination of Indicator and Region. summarize(): This function is used to compute summary statistics for each group. Three statistics are calculated: Time Span: It calculates the range of the Period for which data exists. The paste() and collapse functions are used to concatenate the minimum and maximum period into a string. Mean: The mean of the Value column is calculated with na.rm = TRUE, which means that missing values are ignored. Volatility: The standard deviation (a measure of volatility) of the Value column is also calculated, ignoring missing values (na.rm = TRUE). ungroup(): Finally, the ungroup() function removes the grouping, resulting in a new tibble BIS_univariate_stats that contains the computed univariate statistics for each Indicator and Region combination. By the end of this operation, the BIS_univariate_stats tibble will have a comprehensive view of univariate statistics across the selected indicators and regions, facilitating a straightforward interpretation of each variable’s central tendency and variability within specific economic contexts. We use the kable() and kableExtra() functions from the knitr (Xie 2023) and kableExtra (Zhu 2024) packages to display the univariate statistics in a table. The metrics are rounded to two decimal places for better readability. # Load packages library(&quot;knitr&quot;) library(&quot;kableExtra&quot;) # Render table using kable and kableExtra BIS_univariate_stats %&gt;% mutate(Mean = round(Mean, 2), Volatility = round(Volatility, 2)) %&gt;% rename(`Indicator$^*$` = Indicator) %&gt;% kable(caption = &quot;Univariate Statistics&quot;, booktabs = TRUE) %&gt;% kable_styling(full_width = FALSE, latex_options = c(&quot;hold_position&quot;)) %&gt;% add_footnote(label = &quot;$^*$Annualized Quarterly Rates in Percent&quot;, notation = &quot;none&quot;) Here’s a breakdown of the code: library(\"knitr\") and library(\"kableExtra\") load the necessary packages for table rendering and customization. mutate(Mean = round(Mean, 2), Volatility = round(Volatility, 2)): Rounds the Mean and Volatility to two decimal places for better readability. rename('Indicator$^*$' = Indicator): Renames the Indicator column for special footnote notation. kable(caption = \"Univariate Statistics\", booktabs = TRUE): Uses the kable() function to create the initial table. The caption parameter adds a title to the table and booktabs = TRUE improves the table’s aesthetics. kable_styling(full_width = FALSE, latex_options = c(\"hold_position\")): Adjusts the table’s layout and positioning for PDF output. add_footnote(label = \"$^*$Annualized Quarterly Rates in Percent\", notation = \"none\"): Adds a footnote to clarify that the rates are annualized and expressed in percentages. Table 5.10: Univariate Statistics Indicator\\(^*\\) Region Time Span Mean Volatility Real RPP Inflation United States 1970 Q2 - 2023 Q2 1.79 6.48 Real RPP Inflation Euro Area 1975 Q2 - 2023 Q1 1.26 4.68 Real RPP Inflation China 2005 Q3 - 2023 Q1 0.96 5.84 Real RPP Inflation Emerging Markets 2008 Q1 - 2023 Q1 1.27 2.86 CPI Inflation United States 1970 Q2 - 2023 Q2 3.91 3.40 CPI Inflation Euro Area 1975 Q2 - 2023 Q1 3.77 3.74 CPI Inflation China 2005 Q3 - 2023 Q1 2.44 3.74 CPI Inflation Emerging Markets 2008 Q1 - 2023 Q1 4.27 2.39 Real Policy Rate United States 1970 Q2 - 2023 Q2 1.02 3.49 Real Policy Rate Euro Area 1999 Q1 - 2023 Q1 -0.50 3.24 Real Policy Rate China 2005 Q3 - 2023 Q1 2.77 3.65 \\(^*\\)Annualized Quarterly Rates in Percent The resulting Table 5.10 summarizes the annualized quarterly percentage rates for key economic variables in selected regions. These univariate statistics offer a fundamental understanding of individual variable behavior across different regions, setting the stage for more nuanced analyses. The subsequent analysis will involve comparing these variables using associative statistics. 5.3.12 Associative Statistics This section outlines methods for exploring associative statistics within economic data, focusing on relationships across the dimensions of “Period,” “Indicator,” and “Region.” Types of Relationships The data is categorized into three primary dimensions: “Period”, “Indicator”, and “Region.” These categories allow us to examine relationships in different ways: Relationships Between Regions by Indicator Across Time Periods: This type of relationship examines how a single economic indicator, such as inflation, varies between different regions over multiple time periods. For instance, one might explore how inflation rates in the United States relate to those in the Euro area over a span of several years. Relationships Between Indicators by Region Across Time Periods: This focuses on a specific region and analyzes how different indicators are interrelated within it. Time-series data is again used to estimate these relationships. As an example, within the United States, one could investigate the relationship between inflation and interest rates over time. Relationships Between Indicators by Time Period Across Regions: This approach focuses on a single time period and assesses how different indicators correlate across multiple regions. Unlike the previous types, this uses cross-sectional data from different regions for a specific period. For example, in Q1 of 2023, you could analyze how inflation rates correlate with unemployment rates across different regions. Additional types of relationships are possible; any combination of “Period,” “Indicator,” and “Region” can be explored. The first type will be the focus of this section. Cross-join Data To carry out the analysis, a cross-join is needed to restructure the data. This operation pairs every data point in one region with every data point in another region for each time period and indicator. Table 5.11 shows the data in its original long format, while Table 5.12 illustrates the cross-joined data, which allows us to compute associative statistics between regions. Table 5.11: Snapshot of Original Data Period Indicator Region Value 2023 Q1 Real RPP Inflation United States 0.49 2023 Q1 Real RPP Inflation Euro Area -5.21 2023 Q1 Real RPP Inflation China -2.71 2023 Q1 Real RPP Inflation Emerging Markets -1.85 Table 5.12: Snapshot of Cross-Joined Data Period Indicator Region X Region Y Value X Value Y 2023 Q1 Real RPP Inflation United States Euro Area 0.49 -5.21 2023 Q1 Real RPP Inflation United States China 0.49 -2.71 2023 Q1 Real RPP Inflation United States Emerging Markets 0.49 -1.85 2023 Q1 Real RPP Inflation Euro Area China -5.21 -2.71 2023 Q1 Real RPP Inflation Euro Area Emerging Markets -5.21 -1.85 2023 Q1 Real RPP Inflation China Emerging Markets -2.71 -1.85 A cross-join in database terminology is an operation that combines each row of the first table with every row of the second table. In the context of our analysis, a cross-join pairs every data point for a given economic indicator in one region with every data point for the same indicator in another region for each time period, as displayed in Table 5.12. To execute a cross-join, first determine the shared columns between the tables to be joined. In this case, “Period” and “Indicator” serve as the common columns. After identifying these columns, use a merge function, such as inner_join() from the dplyr package, to merge a copy of the dataset with itself based on these columns. For instance, to produce Table 5.12 from Table 5.11, use the following code: # Cross-join regions by period &amp; indicator BIS_scatter_by_period_indicator &lt;- BIS_core %&gt;% inner_join(BIS_core, by = c(&quot;Period&quot;, &quot;Indicator&quot;), suffix = c(&quot; X&quot;, &quot; Y&quot;), relationship = &quot;many-to-many&quot;) %&gt;% select(-`Value X`, -`Value Y`, everything()) Here’s a breakdown of what the code is doing: BIS_core %&gt;%: This part takes the original BIS_core data table and pipes (%&gt;%) it into the following operations. inner_join(BIS_core, by = c(\"Period\", \"Indicator\"), suffix = c(\" X\", \" Y\"), relationship = \"many-to-many\"): This line executes the cross-join. The first argument specifies that the table should join with itself. The by argument specifies that “Period” and “Indicator” are the common columns. The suffix argument adds a suffix (” X” or ” Y”) to the columns from each table to differentiate them. The relationship = \"many-to-many\" clarifies that multiple rows from the second table can be joined to a single row from the first table and vice versa. select(-`Value X`, -`Value Y`, everything()): This line rearranges the columns after the join, positioning Value X and Value Y at the end. The end result is stored in a new variable called BIS_scatter_by_period_indicator, which will contain the cross-joined data. This new table can then be used for subsequent analyses, such as computing correlations between different regions based on various economic indicators. Since the goal here is to compute correlations, the order in which the regions appear doesn’t matter. In other words, a correlation between Region X as the U.S. and Region Y as China would yield the same result as one between Region X as China and Region Y as the U.S. Moreover, the correlation of a region with itself is always one and can be skipped. Therefore, subsequent data filtering is needed to eliminate such redundant pairings. # Remove duplicate combinations BIS_scatter_by_period_indicator &lt;- BIS_scatter_by_period_indicator %&gt;% filter(`Region Y` &gt; `Region X`) The R code filters the BIS_scatter_by_period_indicator table to retain rows where the categorical value in Region Y is greater than that in Region X, as these are ordered factors. This step removes duplicate pairs, ensuring only one combination of any two regions remains, thus reducing redundancy. Compute Correlations After restructuring the data, we can proceed to compute the correlation statistics. # Compute associative statistics across regions BIS_scatter_by_period_indicator &lt;- BIS_scatter_by_period_indicator %&gt;% group_by(Indicator, `Region X`, `Region Y`) %&gt;% mutate(`Time Span` = paste(range(Period), collapse = &quot; - &quot;), Correlation = cor(x = `Value X`, y = `Value Y`)) %&gt;% ungroup() The R code calculates associative statistics for each combination of “Indicator,” “Region X,” and “Region Y” in the BIS_scatter_by_period_indicator table. group_by(Indicator, `Region X`, `Region Y`): Groups the data by the “Indicator,” “Region X,” and “Region Y” columns. This ensures that subsequent operations are performed within these groups. mutate(...): Within each group, two new columns are created: `Time Span` = paste(range(Period), collapse = \" - \"): Combines the minimum and maximum values of the Period column into a single string, separated by a dash. Correlation = cor(x = `Value X`, y = `Value Y`)): Computes the correlation between Value X and Value Y. ungroup(): Removes the groupings, returning the data to its original structure but with the newly calculated columns added. The modified dataset, BIS_scatter_by_period_indicator, now includes columns for time spans and correlation coefficients. To present these statistics in a table, the following R code is used: # Load packages library(&quot;knitr&quot;) library(&quot;kableExtra&quot;) # Render table using kable and kableExtra BIS_scatter_by_period_indicator %&gt;% select(-Period, -`Value X`, -`Value Y`) %&gt;% group_by(`Indicator`, `Region X`, `Region Y`) %&gt;% slice(1) %&gt;% mutate(Correlation = round(Correlation, 2)) %&gt;% rename(`Indicator$^*$` = Indicator) %&gt;% kable(caption = &quot;Correlations Between Regions by Indicator Across Time Periods&quot;, booktabs = TRUE) %&gt;% kable_styling(full_width = FALSE, latex_options = c(&quot;hold_position&quot;)) %&gt;% add_footnote(label = &quot;$^*$Annualized Quarterly Rates in Percent&quot;, notation = &quot;none&quot;) The R code carries out the following operations: select(-Period, -`Value X`, -`Value Y`) omits the columns Period, Value X, and Value Y, focusing solely on Correlation and Time Span. group_by(Indicator, `Region X`, `Region Y`) sorts the data by Indicator, Region X, and Region Y. slice(1) selects the first row within each group, as Correlation and Time Span values are constant across the group. mutate(Correlation = round(Correlation, 2)) rounds Correlation to two decimal places. rename(Indicator$^*$ = Indicator) renames Indicator to include a star for footnote annotation. kable() and kable_styling() employ the kable and kableExtra packages to format the table, including its caption and style. add_footnote() adds a footnote to the table. Table 5.13: Correlations Between Regions by Indicator Across Time Periods Indicator\\(^*\\) Region X Region Y Time Span Correlation Real RPP Inflation United States Euro Area 1975 Q2 - 2023 Q1 0.31 Real RPP Inflation United States China 2005 Q3 - 2023 Q1 -0.02 Real RPP Inflation United States Emerging Markets 2008 Q1 - 2023 Q1 0.21 Real RPP Inflation Euro Area China 2005 Q3 - 2023 Q1 0.15 Real RPP Inflation Euro Area Emerging Markets 2008 Q1 - 2023 Q1 0.03 Real RPP Inflation China Emerging Markets 2008 Q1 - 2023 Q1 0.73 CPI Inflation United States Euro Area 1975 Q2 - 2023 Q1 0.70 CPI Inflation United States China 2005 Q3 - 2023 Q1 0.02 CPI Inflation United States Emerging Markets 2008 Q1 - 2023 Q1 0.52 CPI Inflation Euro Area China 2005 Q3 - 2023 Q1 -0.29 CPI Inflation Euro Area Emerging Markets 2008 Q1 - 2023 Q1 0.27 CPI Inflation China Emerging Markets 2008 Q1 - 2023 Q1 0.67 Real Policy Rate United States Euro Area 1999 Q1 - 2023 Q1 0.63 Real Policy Rate United States China 2005 Q3 - 2023 Q1 -0.01 Real Policy Rate Euro Area China 2005 Q3 - 2023 Q1 -0.24 \\(^*\\)Annualized Quarterly Rates in Percent The generated table, referred to as Table 5.13, displays the computed correlations between regions for different indicators over time. By computing these statistics, we can identify how different economic indicators relate to each other across various dimensions: time periods, regions, and indicators themselves. This process forms the basis for more complex econometric analyses. 5.3.13 Layered Scatter Plots The correlations presented in Table 5.13 warrant further investigation. To gain deeper insights and evaluate the influence of outliers, we employ scatter plots generated using the ggplot2 package in R. # Load package library(&quot;ggplot2&quot;) # Global minimum and maximum value global_min_max &lt;- range(BIS_scatter_by_period_indicator$`Value X`) # Make scatter plots between regions by indicator BIS_scatter_by_period_indicator %&gt;% mutate(Decade = factor(10 * floor(as.numeric(format(Period, &quot;%Y&quot;)) / 10), ordered = TRUE), Decade = factor(Decade, levels = rev(levels(Decade)))) %&gt;% filter(`Region X` == &quot;United States&quot;) %&gt;% ggplot(aes(x = `Value X`, y = `Value Y`, color = Decade)) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;red&quot;, linetype = &quot;dashed&quot;) + geom_point(size = .2) + facet_grid(rows = vars(`Region Y`), cols = vars(`Indicator`, `Region X`), scales = &quot;fixed&quot;) + theme(legend.position = &quot;right&quot;, legend.justification = &quot;left&quot;) + guides(color = guide_legend(direction = &quot;vertical&quot;, ncol = 1)) + geom_text(data = . %&gt;% group_by(`Region Y`, `Region X`, `Indicator`) %&gt;% slice(1), aes(label = paste(&quot;Correlation: &quot;, sprintf(&quot;%.2f&quot;, Correlation)), x = Inf, y = -Inf), hjust = 1.1, vjust = -1, col = &quot;red&quot;, size = 3) + xlim(global_min_max[1], global_min_max[2]) + ylim(global_min_max[1], global_min_max[2]) Here’s what each part of the code does: library(\"ggplot2\"): Loads the ggplot2 package for data visualization. global_min_max &lt;- range(BIS_scatter_by_period_indicator$Value X): Determines the global minimum and maximum for Value X to set plot limits. mutate(Decade = factor(...)): Creates a new column Decade, categorizing each period into its respective decade. filter(`Region X` == \"United States\"): Filters the data to only include rows where Region X is the United States. ggplot(aes(...)): Initiates the ggplot object, setting the aesthetics for plotting Value X and Value Y, colored by Decade. geom_smooth(...): Adds a linear model (lm) smooth line to the scatter plot. geom_point(...): Adds points to the scatter plot. facet_grid(...): Facets the plot by Region Y and Indicator, keeping Region X as a constant (United States in this case). theme(...) and guides(...) : Adjusts the legend and its position. geom_text(...): Adds text labels displaying the correlation coefficient for each plot. xlim(...) and ylim(...) : Sets the x and y limits of the plots based on the global min-max values. Figure 5.15: Correlations Between Regions by Indicator Across Time Periods The result, shown in Figure 5.15, is a multi-faceted scatter plot organized by regions and indicators, providing a more comprehensive view of how economic variables correlate with each other over time. Each scatter plot has a text label indicating its correlation value, and the color scheme represents different decades, offering additional layers of analysis. 5.4 Resources This chapter has provided insights into multiple ways of importing, saving, and downloading data using various APIs. To further enhance your knowledge and skills in importing data in R, consider the following DataCamp courses: Importing and Managing Financial Data in R: Gain a solid understanding of how to import and manage financial data in R. Intermediate Importing Data in R: Build upon your data importing skills and learn how to handle larger and more complex datasets. Working with Web Data in R: Learn to process and analyze data from the web using R. Importing and Managing Financial Data in R: Learn to download financial or economic time series data using Web APIs. References "],["write-reports-with-r-markdown.html", "Chapter 6 Write Reports with R Markdown 6.1 Create an R Markdown Document 6.2 YAML Header 6.3 Markdown Syntax 6.4 R Chunks 6.5 Embed R Variables into Text 6.6 LaTeX Syntax for Math 6.7 Print Tables 6.8 Referencing 6.9 Escape Characters 6.10 Summary and Resources", " Chapter 6 Write Reports with R Markdown R Markdown is a tool for creating documents that combine text, R code, and the results of that R code, made possible by the R package rmarkdown (Allaire et al. 2024). It simplifies the process of incorporating graphs and other data outputs into a document, removing the need for separate R and word processing operations. It allows for the automation of data retrieval and updating, making it useful for maintaining up-to-date financial reports, among other applications. With R Markdown, you can produce documents in various formats, including HTML, PDF, and Word, directly from your R code. Markdown facilitates the formatting of text in a plain text syntax, while embedded R code chunks ensure the reproducibility of analysis and reports. 6.1 Create an R Markdown Document Here is the step-by-step guide to create a new R Markdown document in RStudio: Click on the top-left plus sign , then select R Markdown... In the dialog box that appears, select Document and choose PDF, then click OK. Figure 6.1: New R Markdown You should now see a file populated with text and code. Save this file by clicking File -&gt; Save As... and select an appropriate folder. To generate a document from your R Markdown file, click Knit: (or use the shortcut Ctrl + Shift + K or Cmd + Shift + K). Lastly, the Knit drop-down menu lets you export your file in different formats, such as HTML or Word, in addition to PDF. The R Markdown template includes: A YAML header, enclosed by ---, which holds the document’s metadata, such as the title, author, date, and output format. Examples of Markdown syntax, demonstrating how to use it. Examples of R code chunks, showing how to write and utilize them in your document. The R code chunks are enclosed by ```{r} at the beginning and ``` at the end, such as: ```{r cars} summary(cars) ``` Anything written within these markers is evaluated as R code. On the other hand, anything outside these markers is considered text, formatted using Markdown syntax, and, for mathematical expressions, LaTeX syntax. Hence, when you click Knit , the output - either a PDF, HTML, or Word file - integrates the text and the R code output into a single document. 6.2 YAML Header The YAML header at the top of the R Markdown document, enclosed in ---, specifies high-level metadata and options that influence the whole document. It might look like this: --- title: &quot;My Document&quot; author: &quot;Your Name&quot; date: &quot;1/1/2023&quot; output: html_document --- In this YAML header, the title, author, and date fields define the title, author, and date of the document. The output field specifies the output format of the document (which can be html_document, pdf_document, or word_document, among others). 6.3 Markdown Syntax Markdown allows you to add formatting elements to plain text. Here are some key elements and their syntax: Bold: Use **text** or __text__ to make text bold. **text** text Italic: Use *text* or _text_ to italicize text. *text* text Strikethrough: Use ~~text~~ to strike through text. ~~text~~ text Line Break: Creating a new line or line break is not achieved with a single Enter key press or a single line break in the text file. Instead, you need two line breaks (essentially a blank line) to separate paragraphs. Line Break Inside a Paragraph: If you want to create a new line without a paragraph break, you can insert two spaces at the end of the line before hitting Enter. Inline Space: In Markdown, multiple spaces in a row are collapsed into a single space when the document is rendered. Markdown does not support multiple spaces or custom spacing between words like you might do with the spacebar in a word processor. Headers: Use # for primary headers, ## for secondary headers, and so on. For example, # Main Title creates a primary header. # Main Title ## Subsection ### Lower-level Subsection Lists: For ordered lists, use 1. for numbers, and you can also use a. or A. for alphabetic lists. Unordered lists can be created with - or *. 1. Item One 1. Sub-item One 1. Sub-item Two 1. Item Two a. Alphabetic Item One a. Alphabetic Item Two - Unordered Item One - Unordered Item Two Item One Sub-item One Sub-item Two Item Two Alphabetic Item One Alphabetic Item Two Unordered Item One Unordered Item Two Horizontal Rule: Use --- to create a horizontal line (only works for HTML). --- This text is separated by horizontal lines. --- This text is separated by horizontal lines. Links: To insert links, use [Link text](url). You can specify that the link should open in a new tab by adding {target=\"_blank\"} immediately after the URL. [Google](https://www.google.com/){target=&quot;_blank&quot;} Google Images: In Markdown, use ![alt text](url) for online images or ![alt text](path) for local images. You can adjust the size to X% of its original width using width=X%. ![TTU Logo](files/icons-ttu/tt.png){width=10%} TTU Logo Footnotes: Use ^[Footnote text] to create an inline footnote. This is a text with a footnote.^[This is the footnote text.] This is a text with a footnote.2 Blockquotes: Use &gt; to create a blockquote. &gt; This is a blockquote. This is a blockquote. Tables: Use | and - to create tables. | Header 1 | Header 2 | |----------|----------| | data1 | data2 | Header 1 Header 2 data1 data2 Checklists: Use - [ ] for unchecked items and - [x] for checked items. - [ ] To-Do Item - [x] Completed Item To-Do Item Completed Item Code Blocks: Use triple backticks to create code blocks. ``` code ``` code Inline Code: Use single backticks for inline code. This is `inline code`. This is inline code. Math Formulas: Use $$ for block-level math formulas. $$ E=mc^2 $$ \\[ E=mc^2 \\] Inline Math Formulas: Use $ for inline math formulas. This formula $E=mc^2$ is inline math. This formula \\(E=mc^2\\) is inline math. By familiarizing yourself with these basic Markdown elements, you’ll be well-equipped to format text documents effectively. 6.4 R Chunks In R Markdown, you can embed chunks of R code. These chunks begin with ```{r} and end with ```. ```{r} 1 + 2 ``` 1 + 2 ## [1] 3 The code contained in these chunks is executed when the document is rendered, and the output (e.g., numbers, plots, tables) is inserted into the final document. Following the r in the chunk declaration, you can include a variety of options in a comma-separated list to control chunk behavior. For instance, ```{r, echo = FALSE} runs the code in the chunk and includes its output in the document, but the code itself is not printed in the rendered document. Detailed documentation on these options is available in the “References” section of ?knitr::opts_chunk. Here are some of the most commonly used chunk options: echo: If set to FALSE, the code chunk will not be shown in the final output. The default is TRUE. eval: If set to FALSE, the code chunk will not be executed. The default is TRUE. include: If set to FALSE, neither the code nor its results are included in the final document. The default is TRUE. message: If set to FALSE, suppresses all messages in the output. The default is TRUE. warning: If set to FALSE, suppresses all warnings in the output. The default is TRUE. fig.cap: Adds a caption to graphical results. For instance, fig.cap=\"My Plot Caption\". fig.align: Aligns the plot in the document. For example, fig.align='center' aligns the plot to the center. out.width: Controls the width of the plot output. For example, out.width=\"50%\" will make the plot take up 50% of the text width. comment: Adds a comment string at the beginning of each line of the R output when displayed in the final document. The default is \"##\". Setting it to an empty string \"\" will remove the comment string. For example, comment=\"\" will produce output without the default comment markers. collapse: If TRUE, all the code and results in the chunk are rendered as a single block. If FALSE, each line of code and its results are rendered separately. The default is FALSE. results: The results argument provides options to control the display of chunk output in the final document. When set to results='hide', the text output is concealed, while results='hold' displays the output after the code. Additionally, results='asis' allows direct inclusion of unmodified output, ideal for text or tables. results='markup' formats output as Markdown, for seamless integration into surrounding text, particularly useful when the R output is written in Markdown syntax. results='verbatim' displays the output as plain text, which is useful when the text includes special characters. fig.path: Specifies the directory where the figures produced by the chunk should be saved. fig.width and fig.height: Specifies the width and height of the plot, in inches. For example, fig.width=6, fig.height=4 will make the plot 6x4 inches. dpi: Specifies the resolution of the plot in dots per inch. For example, dpi = 300 will generate a high-resolution image. error: If TRUE, any error that occurs in the chunk will stop the knitting process. If FALSE, errors will be displayed in the output but will not stop the knitting process. Here’s an example: ```{r, echo=FALSE, fig.cap=&quot;Title&quot;, out.width = &quot;50%&quot;, fig.align=&#39;center&#39;, dpi = 300} plot(cars) ``` Figure 6.2: Title This chunk will create a plot, add a caption to it, set the width of the plot to 50% of the text width, align the plot to the center of the document, and output the plot with a resolution of 300 DPI. The actual R code will not be displayed in the final document. Instead of specifying options for each code chunk, you can modify the default settings for all code chunks in your document using the knitr::opts_chunk$set() function. For instance, I often include the following code at the start of an R Markdown document, right after the YAML header: ```{r} knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.align = &quot;center&quot;, out.width = &quot;60%&quot;) ``` The aforementioned code modifies the default settings for all chunks in the document, as described below: echo = FALSE: Each chunk’s code will be omitted from the final document, a sensible practice for official documents, as recipients don’t require visibility of code used for graph creation. message = FALSE: All messages generated by code chunks will be muted. warning = FALSE: Warnings produced by code chunks will be silenced. fig.align = \"center\": All generated figures will be centrally aligned. out.width = \"60%\": The width of any generated figures will be set to 60% of the text width. 6.5 Embed R Variables into Text A key strength of R Markdown is the ability to incorporate R variables directly within the Markdown text. This enables a dynamic text where the values are updated as the variables change. You can accomplish this by using the `r variable` syntax. Furthermore, you can format these numbers for enhanced readability. To insert the value of an R variable into your text, you encase the variable name in backticks and prepend it with r. Here’s an illustration: # R variable defined inside R chunk my_var &lt;- 123234.53983 To refer to this variable in your Markdown text, you can write the following text (outside of an R chunk): The total amount is `r my_var` USD. The output will be: “The total amount is 1.2323454^{5} USD.” That’s because when the R Markdown document is knitted, `r my_var` will be replaced by the current value of my_var in your R environment, dynamically embedding the value of my_var into your text. Additionally, you can format numbers for better readability by avoiding scientific notation, rounding, and adding a comma as a thousands separator. To do this, you can use the formatC() function in R as follows: # R variable with formatting, defined inside R chunk my_var_formatted &lt;- formatC(my_var, format = &quot;f&quot;, digits = 2, big.mark = &quot;,&quot;) Then, in your text: The total amount is `r my_var_formatted` USD. The output will be: “The total amount is 123,234.54 USD.” In this case, format = \"f\" ensures fixed decimal notation, digits = 2 makes sure there are always two decimal places, and big.mark = \",\" adds comma as the thousand separator. By properly formatting your numbers in your R Markdown documents, you enhance their clarity and make your work more professional and easier to read. 6.6 LaTeX Syntax for Math LaTeX is a high-quality typesetting system that is widely used for scientific and academic papers, particularly in mathematics and engineering. LaTeX provides a robust way to typeset mathematical symbols and equations. Thankfully, R Markdown supports LaTeX notation for mathematical formulas, which is rendered in the HTML output. In R Markdown, you can include mathematical notation within the text by wrapping it with dollar signs ($). For example, $a^2 + b^2 = c^2$ will be rendered as \\(a^2 + b^2 = c^2\\). Here are some basic LaTeX commands for mathematical symbols: Subscripts: To create a subscript, use the underscore (_). For example, $a_i$ is rendered as \\(a_i\\). Superscripts: To create a superscript (useful for exponents), use the caret (^). For example, $e^x$ is rendered as \\(e^x\\). Greek letters: Use a backslash (\\) followed by the name of the letter. For example, $\\alpha$ is rendered as \\(\\alpha\\), $\\beta$ as \\(\\beta\\), and so on. Sums and integrals: Use \\sum for summation and \\int for integration. For example, $\\sum_{i=1}^n i^2$ is rendered as \\(\\sum_{i=1}^n i^2\\) and $\\int_a^b f(x) dx$ is rendered as \\(\\int_a^b f(x) dx\\). Fractions: Use \\frac{numerator}{denominator} to create a fraction. For example, $\\frac{a}{b}$ is rendered as \\(\\frac{a}{b}\\). Square roots: Use \\sqrt for square roots. For example, $\\sqrt{a}$ is rendered as \\(\\sqrt{a}\\). If you want to display an equation on its own line, you can use double dollar signs ($$). For example: $$ \\% \\Delta Y_t \\equiv 100 \\left( \\frac{Y_t - Y_{t-1}}{Y_{t-1}}\\right) \\% \\approx 100 \\left( \\ln Y_t - \\ln Y_{t-1} \\right) \\% $$ This will be rendered as: \\[ \\% \\Delta Y_t \\equiv 100 \\left(\\frac{Y_t - Y_{t-1}}{Y_{t-1}}\\right) \\% \\approx 100 \\left( \\ln Y_t - \\ln Y_{t-1} \\right) \\% \\tag{6.1} \\] LaTeX and R Markdown together make it easy to include mathematical notation in your reports. With practice, you can write complex mathematical expressions and equations using LaTeX in your R Markdown documents. 6.7 Print Tables The R packages kable and kableExtra are great tools for creating professionally formatted tables in your R Markdown documents. Directly printing data without any formatting is not usually advisable as it lacks professionalism and can often be challenging to read and interpret. By contrast, these packages allow you to control the appearance of your tables, leading to better readability and aesthetics. You’ll first need to install and load the necessary packages. You can do so by executing install.packages(c(\"knitr\", \"kableExtra\")) in your console and then load the two packages in the beginning of your code: library(&quot;knitr&quot;) library(&quot;kableExtra&quot;) Let’s assume we have a table that we want to print: financial_metrics &lt;- tibble::tibble( Company = c(&quot;Apple&quot;, &quot;Microsoft&quot;, &quot;Tesla&quot;), `Market Cap` = c(2796.21, 2434.47, 797.05), `P/E Ratio` = c(0.30, 0.34, 0.67), `Dividend Yield` = c(0.52, 0.82, 0.00) ) financial_metrics ## # A tibble: 3 × 4 ## Company `Market Cap` `P/E Ratio` `Dividend Yield` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Apple 2796. 0.3 0.52 ## 2 Microsoft 2434. 0.34 0.82 ## 3 Tesla 797. 0.67 0 Note that the table is structured as a tibble (tbl_df) instead of a simple data frame (data.frame) to allow for column names with spaces. To improve the readability of financial metrics, avoid scientific notation, round to appropriate decimal places, and add a thousands separator. You can achieve this using R’s formatC() function: # Format numbers representing market cap values financial_metrics$`Market Cap` &lt;- formatC( financial_metrics$`Market Cap`, format = &quot;f&quot;, digits = 2, big.mark = &quot;,&quot;) You can create a basic table using the kable function from the knitr package: kable(financial_metrics) Company Market Cap P/E Ratio Dividend Yield Apple 2,796.21 0.30 0.52 Microsoft 2,434.47 0.34 0.82 Tesla 797.05 0.67 0.00 This will generate a simple, well-formatted table. However, you can further customize the table’s appearance using functions from the kableExtra package: financial_metrics %&gt;% kable(align = &quot;lrrr&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = FALSE, latex_options = c(&quot;hold_position&quot;, &quot;striped&quot;)) Company Market Cap P/E Ratio Dividend Yield Apple 2,796.21 0.30 0.52 Microsoft 2,434.47 0.34 0.82 Tesla 797.05 0.67 0.00 In this code: align = \"lrrr\" specifies the alignment for each column. The letters “l” and “r” stand for left and right alignment, respectively. The columns are aligned in the order given: “Company” is left-aligned, and the remaining columns are right-aligned. bootstrap_options = \"striped\" alternates the row colors to make the table easier to read. This option is effective for HTML outputs but doesn’t apply to LaTeX/PDF output (see latex_options for LaTeX-specific styling). full_width = FALSE specifies that the table should only take up as much width as it needs, rather than expanding to fill the page. This option isn’t relevant for LaTeX/PDF output as LaTeX automatically adjusts table widths by default. latex_options = c(\"hold_position\", \"striped\") provides LaTeX-specific table settings. The \"hold_position\" option ensures the table appears immediately after the corresponding code chunk, overriding LaTeX’s default ‘floating’ behavior. The \"striped\" option similarly adds alternating row colors, but specifically for LaTeX output. Adding a caption to your table is straightforward. Simply provide the caption argument to the kable function: financial_metrics %&gt;% kable(align = &quot;lrrr&quot;, caption = &quot;Financial Metrics as of October 2023&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = FALSE, latex_options = c(&quot;hold_position&quot;, &quot;striped&quot;)) Table 6.1: Financial Metrics as of October 2023 Company Market Cap P/E Ratio Dividend Yield Apple 2,796.21 0.30 0.52 Microsoft 2,434.47 0.34 0.82 Tesla 797.05 0.67 0.00 This code generates the same striped table, but now with a caption: “Financial Metrics as of October 2023.” These are just the basics. Both kable and kableExtra provide numerous options for customizing your tables. I encourage you to explore their documentation and experiment with different settings. 6.8 Referencing Books, investment reports, academic papers, and other professional documents typically use numbering for chapters, sections, figures, tables, equations, etc., which are then referred to as Chapter 4 or Figure 2, etc. This referencing is typically automated so that if you add a figure between Figure 3 and Figure 4, the previously labeled Figure 4 changes to Figure 5. Standard R Markdown files allow for some of this labeling and referencing; however, the bookdown package by Xie (2024a) is optimized for this, and its syntax is introduced in this section. Setting Up bookdown To make use of the bookdown functionalities, replace the output: pdf_document or output: html_document line in the YAML header with output: bookdown::pdf_document2 or output: bookdown::html_document2: --- title: &quot;My Document&quot; author: &quot;Your Name&quot; date: &quot;1/1/2023&quot; output: bookdown::pdf_document2: toc: true number_sections: true --- This setup provides additional options, such as including a table of contents (toc: true) and enabling section numbering (number_sections: true). These features enhance the organization and navigation of your document. Referencing Equations When writing equations in R Markdown with equation numbering, use \\begin{equation} and \\end{equation} instead of $$. This allows for easy referencing by including an equation label, such as (\\#eq:my-equation-label), within the equation environment: \\begin{equation} E = mc^2 (\\#eq:my-equation-label) \\end{equation} \\[\\begin{equation} E = mc^2 \\tag{6.2} \\end{equation}\\] You can reference this equation later in the text using the \\@ref(eq:my-equation-label) syntax. For example: As shown in Equation \\@ref(eq:my-equation-label), energy is related to mass and the speed of light. As shown in Equation (6.2), energy is related to mass and the speed of light. Referencing Tables Tables generated inside an R chunk can also be referenced, as long as they have a caption. To generate a caption in a table, specify a caption argument in the kable() function. To reference a table, the R chunk where it is generated needs to be labeled. To do so, add a label next to {r}: ```{r my-table-label, results=&#39;asis&#39;} knitr::kable(summary(cars), caption = &quot;My Table Caption&quot;) ``` Table 6.2: My Table Caption speed dist Min. : 4.0 Min. : 2.00 1st Qu.:12.0 1st Qu.: 26.00 Median :15.0 Median : 36.00 Mean :15.4 Mean : 42.98 3rd Qu.:19.0 3rd Qu.: 56.00 Max. :25.0 Max. :120.00 To reference this table, use: As shown in Table \\@ref(tab:my-table-label), the data is organized as follows. As shown in Table 6.2, the data is organized as follows. Referencing Figures As with tables, figures generated inside an R chunk can also be referenced, as long as they have a caption. To generate a caption for a figure, specify fig.cap inside the chunk options. And as with tables, the R chunk where the figure is generated needs to be labeled: ```{r my-figure-label, fig.cap=&quot;My Figure Caption&quot;} plot(cars) ``` Figure 6.3: My Figure Caption To reference this figure, use: Figure \\@ref(fig:my-figure-label) shows the relationship between speed and stopping distances. Figure 6.3 shows the relationship between speed and stopping distances. Referencing Sections Sections can be referenced by assigning them an ID. This is done by adding {#my-section-label} after the section title. For example: # This Is My Section {#my-section-label} This section covers the following topics. You can reference this section elsewhere in your document using \\@ref(my-section-label). For example: For more details, see Section \\@ref(my-section-label). If the ID {#my-section-label} is missing, you can use the section title for referencing, using lowercase letters and hyphens (-) instead of spaces. In the example above, you would reference it as \\@ref(this-is-my-section). To summarize, bookdown in R Markdown allows for advanced referencing of sections, tables, figures, and equations. This enhances the readability and navigability of your document. For more detailed information, refer to the bookdown documentation and the R Markdown Cookbook. 6.9 Escape Characters In Markdown, certain characters have special meanings. For instance, the # symbol is used for section headers, and the asterisk (*) is used for italicizing text when it wraps a word or phrase, like *italic*. If you want to include these special characters as plain text in your document, you’ll need to escape them. Additionally, characters may need to be escaped differently depending on whether the final output is in HTML or LaTeX, which have their own special characters and escape mechanisms. HTML: Special characters are escaped using ampersand codes, such as &amp;amp; for &amp; and &amp;percnt; for %. LaTeX: You can usually use a backslash (\\) to escape special characters in LaTeX. For example, \\% will produce a literal percent symbol. Note that the backslash itself is a special character in LaTeX (escape character), so to include a backslash, you use \\\\. Escape in Markdown Text In the text of an R Markdown document, you can generally use a single backslash (\\) to escape special characters. For instance, to include an asterisk symbol *, you could use \\* to write *. Escape in Code Chunks Inside R code chunks, the escape mechanism is different due to R’s own syntax rules for strings. Specifically, \\n inserts a line break and \\t adds a tab. The cat() function is employed to concatenate and display strings: cat(&quot;Welcome!\\nThis is a new line.&quot;, &quot;\\n\\nThis is a new line with two line breaks.&quot;, &quot;\\n\\tAnd this new line is indented.&quot;) ## Welcome! ## This is a new line. ## ## This is a new line with two line breaks. ## And this new line is indented. To render this text directly as Markdown, use the results='asis' code chunk option. It’s worth noting that in Markdown, a single break \\n doesn’t produce a new line and \\t has no effect. For a new Markdown line, two line breaks \\n\\n are required: cat(&quot;Welcome!\\nThis is a new line.&quot;, &quot;\\n\\nThis is a new line with two line breaks.&quot;, &quot;\\n\\tAnd this new line is indented.&quot;) Welcome! This is a new line. This is a new line with two line breaks. And this new line is indented. To include a literal backslash inside an R string, you’ll need to escape it with another backslash. For instance, to display an asterisk symbol (*) in an R Markdown document from an R code chunk, you’ll need to use \\\\*: cat(&quot;Here is a percentage symbol printed inside an R chunk: \\\\*.&quot;) Here is a percentage symbol printed inside an R chunk: *. Why two backslashes? The first backslash escapes the second, resulting in a single \\. When combined with *, this forms \\*, which is the R Markdown-compatible sequence that renders as a literal * in the output. As another example, to write expression (6.1) within a code chunk set to results='asis: cat(&quot;$$ \\\\% \\\\Delta Y_t \\\\equiv 100 \\\\left(\\\\frac{Y_t - Y_{t-1}}{Y_{t-1}}\\\\right) \\\\% \\\\approx 100 \\\\left( \\\\ln Y_t - \\\\ln Y_{t-1} \\\\right) \\\\% $$&quot;) \\[ \\% \\Delta Y_t \\equiv 100 \\left(\\frac{Y_t - Y_{t-1}}{Y_{t-1}}\\right) \\% \\approx 100 \\left( \\ln Y_t - \\ln Y_{t-1} \\right) \\% \\] Here, escaping ensures that LaTeX code is correctly displayed when the document is knit. Escaped Characters Here is a list of escaped characters across LaTeX, HTML, Markdown, and R code: LaTeX %: \\% — Comment symbol #: \\# — Macro parameter symbol &amp;: \\&amp; — Table cell separator $: \\$ — To start or stop inline math mode _: \\_ — Subscript in math mode {: \\{ — Open curly brace }: \\} — Close curly brace ~: \\textasciitilde or \\\\~{} — Non-breaking space ^: \\^ — Superscript in math mode \\: \\textbackslash — Backslash itself &lt;: \\textless — Less than symbol &gt;: \\textgreater — Greater than symbol HTML %: &amp;percnt; — Percent symbol #: &amp;num; — Number symbol &amp;: &amp;amp; — Ampersand $: &amp;dollar; — Dollar symbol &lt;: &amp;lt; — Less than symbol &gt;: &amp;gt; — Greater than symbol \": &amp;quot; — Double quote ': &amp;apos; — Single quote Markdown #: \\# — To use the hash symbol literally, and avoid it being interpreted as a header *: \\* — To use the asterisk literally, and avoid it being interpreted as emphasis or a bullet point _: \\_ — To use the underscore literally, and avoid it being interpreted as emphasis -: \\- — To use the hyphen literally, and avoid it being interpreted as a bullet point `: ``` — To use the backtick literally, and avoid it being interpreted as inline code R Code \\: \\\\ — To escape a backslash itself \": \\\" — To include a double quote inside a string enclosed by double quotes ': \\' — To include a single quote inside a string enclosed by single quotes %: \\% — To include a percent symbol inside a string n: \\n — Newline character to create a line break in the text t: \\t — Tab character to insert a tabulation in the text By understanding how to escape characters, you can include special characters in your text without confusing R Markdown’s formatting engine. This will allow for more flexibility and clarity in your reports. 6.10 Summary and Resources R Markdown provides a powerful framework for dynamically generating reports in R. The “dynamic” part of “dynamically generating reports” means that the document is able to update automatically when your data changes. By understanding and effectively using Markdown syntax, R code chunks, chunk options, and YAML headers, you can create sophisticated, reproducible documents with ease like the document you are currently reading. For an in-depth understanding of R Markdown, you may want to delve into R Markdown: The Definitive Guide, an extensive resource on the built-in R Markdown output formats and several extension packages. For more practical and relatively short examples, refer to the R Markdown Cookbook, which provides up-to-date solutions and recipes drawn from popular posts on Stack Overflow and other online resources. For more detailed information on advanced referencing of sections, tables, figures, and equations, refer to the bookdown documentation. Finally, DataCamp’s course Reporting with R Markdown provides practical lessons on how to create compelling reports using this tool. References "],["part-ii.html", "Part II: Measurement in Economics", " Part II: Measurement in Economics Part II discusses key measures used in Economics and Finance, detailing the sources from which this data originates. This includes both macroeconomic figures like GDP and unemployment as well as financial measures such as interest rates and stock market indices. Included chapters: Chapter 7: “Economic Data Sources” explores various indicators that provide insights into the health and direction of economies and markets, and introduces key data providers. Chapter 8: “Business Accounting” discusses how accounting is a systematic method of recording, analyzing, and summarizing financial transactions. The outcome of accounting is a comprehensive snapshot of a company’s financial health, communicated through financial statements. Chapter 9: “Financial Performance Indicators” discusses key numbers that provide a comprehensive view of a company’s financial health. These indicators are based on the three major financial statements: the balance sheet, income statement, and cash flow statement, introduced in Chapter 8. Chapter 10: “Macroeconomic Accounting” explains how key economic indicators are systematically measured and recorded to assess the performance and stability of an economy. It covers essential metrics such as GDP, consumption, investment, international trade, government budgets, prices, money supply, and the balance of payments. Chapter 11: “Financial Market Indicators” delves into topics such as interest rates, stock market metrics, and indicators of financial stability. "],["economic-data-sources.html", "Chapter 7 Economic Data Sources 7.1 Types of Indicators 7.2 Data Source", " Chapter 7 Economic Data Sources 7.1 Types of Indicators In the fields of economics and finance, various types of indicators provide valuable insights into the state and trajectory of economies and markets. These indicators range from macroeconomic measures that reflect the overall performance of an economy, to sentiment indices that capture the mood of investors: Macroeconomic Indicators: Macroeconomic indicators capture the overall performance and trends of an entire economy or a significant segment of it. These indicators include measures such as GDP, inflation rates, interest rates, national income, employment levels, and government debt. Macroeconomic indicators provide policymakers, analysts, and stakeholders with a broad understanding of the health and stability of an economy, enabling them to make informed decisions and formulate effective policies. Microeconomic Indicators: Microeconomic indicators focus on specific entities within the economy, such as individual firms, industries, or households. They provide insights into micro-level economic phenomena, such as market shares, production levels, consumer spending patterns, cost structures, and individual income levels. Microeconomic indicators help analyze the performance and dynamics of individual economic agents such as firms and consumers. Market Indicators: Market indicators track the movements of various types of market-based measures, such as equity prices, bond prices, commodity prices, and indices like the S&amp;P 500 or FTSE 100. These indicators provide insights into the dynamics and sentiments prevailing in financial markets and the economy as a whole. Commodity prices, as an example, can signal trends in global economic health and are especially critical for countries heavily dependent on commodity exports or imports. Sentiment Indices: Sentiment indices reflect the overall attitude of investors towards a particular market or economy. These indices, which can be based on surveys or derived from market data, provide insights into the mood of investors and their expectations for the future. Examples include the Consumer Confidence Index, the Investor Sentiment Index, and the Fear &amp; Greed Index. By capturing the “mood” of the market, sentiment indices can help predict market trends and identify potential turning points. Financial Indicators: Financial indicators revolve around the functioning, stability, and performance of financial systems and markets. These indicators include measures such as stock market indices, exchange rates, bond yields, credit ratings, bank lending rates, and financial market volatility indices. They are crucial in assessing the strength and resilience of financial systems, identifying potential risks, and monitoring the effectiveness of monetary and regulatory policies. While indicators related to other non-financial industries, such as the production output of the manufacturing sector, typically fall under microeconomic indicators, financial indicators often overlap with macroeconomic indicators because they reflect conditions that affect all industries. That’s because all sectors depend on the financial market for lending, borrowing, and liquidity. Demographic Indicators: Demographic indicators focus on population characteristics and dynamics. They encompass measures such as population size, age distribution, fertility rates, life expectancy, migration patterns, and labor force participation rates. Demographic indicators provide valuable insights into population trends, labor market dynamics, social challenges, and the potential impact on economic growth and development. Environmental Indicators: Environmental indicators highlight the relationship between economic activities and the environment. They encompass measures such as greenhouse gas emissions, energy consumption, water usage, waste generation, and natural resource depletion. Environmental indicators help assess the sustainability of economic development, the impact of industrial processes, and the potential risks posed by environmental degradation. Social Indicators: Social indicators focus on the well-being and quality of life of individuals within an economy. They encompass measures such as poverty rates, education levels, healthcare access, income inequality, and social mobility. Social indicators provide insights into the distribution of resources, social cohesion, and the effectiveness of social policies. By understanding these indicators and how they interact, analysts, policymakers, and investors can make informed decisions and predictions. 7.2 Data Source This chapter provides an overview of the primary U.S. institutions that either collect economic data (data sources) or compile and disseminate data from various sources (data providers). Each institution’s key datasets are also detailed. 7.2.1 Data Supplier vs. Distributor Data suppliers: Bureau of Economic Analysis (BEA) U.S. Census Bureau Bureau of Labor Statistics (BLS) Federal Reserve Board of Governors U.S. Department of the Treasury Financial Market Exchanges Data distributors: Federal Reserve Economic Data (FRED) U.S. Department of Agriculture Economic Research Service (ERS) Bloomberg Yahoo Finance 7.2.2 Bureau of Economic Analysis (BEA) The Bureau of Economic Analysis (BEA) is part of the U.S. Department of Commerce and is primarily responsible for providing comprehensive and extensive economic statistics. Key reports include: Gross Domestic Product (GDP) Personal Income and Outlays Corporate Profits U.S. International Trade in Goods and Services You can access BEA data in R using the bea.R package. # install.packages(&quot;bea.R&quot;) library(bea.R) # Get GDP data gdp &lt;- getBEA(&quot;NIPA&quot;, &quot;T10101&quot;, year = 2020) 7.2.3 U.S. Census Bureau The U.S. Census Bureau collects demographic, social, and economic data. This information, which includes the decennial census, informs everything from legislative representation to public policy decisions. Key data includes: Population and Housing Census American Community Survey Economic Census Annual Surveys of Manufacturers Census data can be accessed with the censusapi package in R. # install.packages(&quot;censusapi&quot;) library(censusapi) # Get population data pop &lt;- getCensus(name = &quot;sf1&quot;, vintage = 2010, vars = &quot;P0010001&quot;, region = &quot;state:*&quot;) 7.2.4 Bureau of Labor Statistics (BLS) The Bureau of Labor Statistics is the principal federal agency responsible for measuring labor market activity, working conditions, and price changes in the economy. Key data includes: Employment Situation Consumer Price Index Producer Price Index Real Earnings BLS data can be accessed using the blscrapeR package. # install.packages(&quot;blscrapeR&quot;) library(blscrapeR) # Get unemployment rate unemp &lt;- get_bls_county(&quot;unemployment rate&quot;, startyear = 2020, endyear = 2021) 7.2.5 Federal Reserve Board of Governors The Federal Reserve Board of Governors provides data on the functions, operations, and structure of the Federal Reserve System. Key data includes: Interest Rates Monetary Aggregates Exchange Rates Industrial Production and Capacity Utilization Federal Reserve data is best accessed through the Federal Reserve Economic Data (FRED) using the fredr package. # install.packages(&quot;fredr&quot;) library(fredr) # Get interest rates ir &lt;- fredr(series_id = &quot;GS10&quot;) 7.2.6 U.S. Department of the Treasury The U.S. Department of the Treasury collects data related to the U.S. government’s finances and debt. Key data includes: Daily Treasury Yield Curve Rates Treasury Auctions U.S. International Reserve Position The Monthly Treasury Statement (receipts and outlays of the federal government) Data can be directly downloaded from the U.S. Treasury website. # Get daily yield curve rates rates &lt;- read.csv(&quot;https://www.treasury.gov/resource-center/data-chart-center/interest-rates/Datasets/yield.csv&quot;) 7.2.7 Financial Market Exchanges Exchanges such as the New York Stock Exchange (NYSE), NASDAQ, and Chicago Mercantile Exchange (CME) provide extensive data on financial securities. Key data includes: Equity prices Trading volumes Derivatives data (futures and options) Index values Financial market data from various exchanges can be accessed using the quantmod package. # install.packages(&quot;quantmod&quot;) library(quantmod) # Get NYSE data getSymbols(&quot;AAPL&quot;, src = &quot;yahoo&quot;) 7.2.8 Federal Reserve Economic Data (FRED) FRED, maintained by the Federal Reserve Bank of St. Louis, offers a wealth of economic data from multiple sources in an easily accessible format. It includes but is not limited to: Interest Rates (sourced from Federal Reserve) Unemployment Rate (sourced from BLS) Consumer Price Index (CPI) (sourced from BLS) Money Stock Measures (sourced from Federal Reserve) Data from FRED can be accessed using the fredr package. # install.packages(&quot;fredr&quot;) library(fredr) # Get CPI cpi &lt;- fredr(series_id = &quot;CPIAUCSL&quot;) 7.2.9 U.S. Department of Agriculture Economic Research Service (ERS) The ERS is a primary source of economic information and research in the field of agriculture. Key data includes: Agricultural Outlook Crop and Livestock Production Food Prices, Expenditures, and Establishments International Agricultural Trade Data from ERS can be directly downloaded from the ERS website. # Get corn prices corn_prices &lt;- read.csv(&quot;https://www.ers.usda.gov/webdocs/DataFiles/48747/CornYearbook.xlsx?v=9149.8&quot;) 7.2.10 Bloomberg Bloomberg provides financial software tools such as an analytics and equity trading platform, data services, and news to financial companies and organizations. Key data includes: Stock Market Data Economic Indicators Commodities Prices Currency Exchange Rates Bloomberg provides a dedicated R package, RBloomberg, for Bloomberg data but requires a Bloomberg license and installation of Bloomberg software. # The RBloomberg package is not available on CRAN. To use it, Bloomberg Terminal must be installed. 7.2.11 Yahoo Finance Yahoo Finance is a media property that provides financial news, data and commentary including stock quotes, press releases, financial reports, and original content. Key data includes: Stock Quotes Market Trends Financial News Economic Calendar Data from Yahoo Finance can be accessed using the quantmod package. # install.packages(&quot;quantmod&quot;) library(quantmod) # Get Yahoo stock price getSymbols(&quot;AAPL&quot;, src = &quot;yahoo&quot;) 7.2.12 Conclusion The data collected and provided by these institutions enable a robust understanding of the U.S. economy’s state and trajectory. This information shapes both public and private sector decision-making, affecting everything from Federal Reserve monetary policy to individual investment decisions. "],["business-accounting.html", "Chapter 8 Business Accounting 8.1 Accounting Standards 8.2 Accounting Equation 8.3 Balance Sheet 8.4 Ledger 8.5 Income Statement 8.6 Cash Flow Statement 8.7 Conclusion", " Chapter 8 Business Accounting Accounting is a systematic method of recording, analyzing, and summarizing financial transactions. The outcome of accounting is a comprehensive snapshot of a company’s financial health, communicated through financial statements: The balance sheet assesses the company’s assets, liabilities, and equity at a given time, reflecting the cumulative outcome of all financial activities. The ledger acts as the central repository where all transactions are recorded, reflecting the continuous financial activity that occurs within a company. The income statement measures the company’s financial performance over an interval, providing insights into its ability to generate profit. The cash flow statement reveals the actual liquidity position of the business, identifying how cash is generated and utilized. This chapter will guide you through the fundamental principles of business accounting. 8.1 Accounting Standards Accounting standards serve as the framework for financial reporting, ensuring consistency, reliability, and comparability of financial statements. In the United States, the Financial Accounting Standards Board (FASB, 2023) develops the Generally Accepted Accounting Principles (GAAP), which companies are required to follow when compiling their financial statements. GAAP covers a broad range of accounting practices, from revenue recognition to balance sheet item classification. Internationally, the International Accounting Standards Board (IASB, 2023) issues the International Financial Reporting Standards (IFRS), which aim to standardize accounting practices across the globe, making it easier for investors to compare financial statements from companies in different countries. While IFRS is used in over 140 jurisdictions, the U.S. has not fully adopted these standards, creating a dichotomy in financial reporting. Nonetheless, there are ongoing discussions and efforts towards convergence between FASB and IASB to enhance the compatibility of the two systems, which would benefit multinational corporations and international investors. 8.2 Accounting Equation At the heart of accounting lies the fundamental equation: \\[ \\text{Assets} = \\text{Liabilities} + \\text{Equity} \\] Assets are resources owned by a company with economic value. Liabilities are obligations or debts that the company owes to outside parties. Equity represents the owner’s claims on the assets after all liabilities have been deducted. This equation forms the basis for the double-entry system of accounting, which requires that every transaction is recorded in at least two accounts, helping to maintain the balance reflected in the equation. The entries are as follows: Debit: An entry that increases an asset account, or decreases a liability or equity account. Credit: An entry that increases a liability or equity account, or decreases an asset account. These definitions imply: Increases in assets are debited. Increases in liabilities and equity are credited. Decreases in assets are credited. Decreases in liabilities and equity are debited. Each transaction involves a debit and a credit that must equal each other in value, preserving the accounting equation. 8.3 Balance Sheet The balance sheet is a concrete manifestation of the double-entry system of accounting: it is a snapshot of a firm’s assets, liabilities, and equity as of a specific date, reflecting its financial position at that moment. A sample balance sheet for a bakery is shown in Table 8.1. Assets Liabilities &amp; Equity Table 8.1: Balance Sheet for a Private Company as of December 31, 2023 Cash 10,000 Accounts Payable 18,000 Accounts Receivable 8,000 Accrued Liabilities 5,000 Inventory 15,000 Notes Payable 17,000 Prepaid Expenses 2,000 Long-term Debt 40,000 Property, Plant, and Equipment (PPE) 40,000 Owner’s Equity 60,000 Land and Buildings 100,000 Retained Earnings 25,000 Total 175,000 Total 165,000 Let’s dissect the balance sheet of a bakery as presented in Table 8.1 to understand each component. 8.3.1 Assets Assets are economic resources that are expected to benefit the bakery’s future operations. They are categorized as either current or non-current: Current Assets: These are assets that the bakery expects to convert to cash within one year. For the bakery, this includes: Cash: Money in hand and in the bank. Accounts Receivable: Money owed by customers who have purchased baked goods on credit. Inventory: Ingredients like flour, sugar, and other supplies that are to be used within the year. Prepaid Expenses: Payments made in advance for services or goods, such as insurance or rent for the bakery space. Non-Current Assets: These assets will be used over a longer period: Property, Plant, and Equipment (PPE): This includes the bakery’s ovens, refrigerators, and other long-term equipment, net of depreciation. Land and Buildings: If the bakery owns property, the value of these long-term investments is included here. Intangible Assets: Any non-physical assets like trademarks or patents, which may not be relevant for all bakeries. 8.3.2 Liabilities Liabilities represent what the bakery owes to others—these are the financial obligations and debts: Current Liabilities: Obligations due within one year, such as: Accounts Payable: Money the bakery owes to suppliers for ingredients and other goods. Accrued Expenses: Incurred expenses not yet paid, such as utilities or wages. Notes Payable: Short-term borrowings or lines of credit. Non-Current Liabilities: Debts that are payable over a period longer than one year, like a mortgage on the bakery’s property. 8.3.3 Equity Equity is what’s left after liabilities are subtracted from assets and includes: Owner’s Capital: Initial and additional investments made by the owner(s) into the bakery. Retained Earnings: Cumulative earnings retained in the business, reflecting the bakery’s profitability over time. In the day-to-day operations of the bakery, every purchase made, every sale conducted, and every payment received or expense incurred will interact with this balance sheet. For instance, when the bakery sells its goods, it increases its cash (asset) and simultaneously increases its retained earnings (equity), assuming no new liabilities are created. Conversely, if the bakery incurs a new debt, it might increase cash (asset) while simultaneously increasing accounts payable (liability). By examining the balance sheet of the bakery in Table 8.1, stakeholders can glean insights into the bakery’s financial stability and operational prowess. It answers crucial questions about liquidity (Can the bakery meet its short-term obligations?), solvency (Can the bakery sustain itself in the long term?), and overall financial health. 8.3.4 Transactions Every transaction the bakery engages in will have a dual effect on the balance sheet, maintaining the balance of the accounting equation. For example, if the bakery purchases inventory on credit, such as purchasing flour with a credit card, both assets and liabilities increase by the same amount, as demonstrated in Table 8.2. Assets Liabilities &amp; Equity Table 8.2: Changes in Balance Sheet Due to Purchase of Inventory on Credit Cash 0 Accounts Payable +5,000 Accounts Receivable 0 Accrued Liabilities 0 Inventory +5,000 Notes Payable 0 Prepaid Expenses 0 Long-term Debt 0 Property, Plant, and Equipment (PPE) 0 Owner’s Equity 0 Land and Buildings 0 Retained Earnings 0 Total +5,000 Total +5,000 Table 8.3 presents a streamlined version, highlighting only the accounts that were affected. Assets Liabilities &amp; Equity Table 8.3: Purchase of Inventory on Credit Inventory +5,000 Accounts Payable +5,000 Table 8.3 confirms the double-entry bookkeeping principle, wherein each transaction is recorded in two accounts to maintain the accounting equation. In this case, the acquisition of inventory is entered as a debit to increase the asset account “Inventory,” and simultaneously, a credit to increase the liability account “Accounts Payable,” thus detailing the purchase made on credit. When a bakery pays off its accounts payable, such as settling a bill for ingredients purchased on credit, it decreases its liabilities and its assets by the same amount, as the cash is used to pay off the debt. For this example, suppose the bakery pays off $2000 of its accounts payable, as illustrated in Table 8.4. Assets Liabilities &amp; Equity Table 8.4: Payment of Accounts Payable Cash -2,000 Accounts Payable -2,000 Table 8.4 showcases how settling debts affects financial position without altering the overall balance. In this transaction, the payment of debt with cash results in a credit to the asset account ‘Cash,’ thus decreasing it, and a debit to the liability account ‘Accounts Payable,’ also decreasing it. Lastly, consider the bakery generating $3000 in cash from the sale of baked goods. This transaction increases assets (Cash) and also increases equity (Retained Earnings), reflecting the income from sales. Assets Liabilities &amp; Equity Table 8.5: Cash Sales Cash +3,000 Retained Earnings +3,000 Table 8.5 illustrates the impact of a revenue transaction on the balance sheet, where assets and equity increase by the same amount. In this entry, the cash receipt from sales is recorded as a debit, increasing the ‘Cash’ account, while a corresponding credit is made to the ‘Retained Earnings’ account, reflecting the income retained in the business. In summary, Tables 8.3, 8.4, and 8.5 collectively illustrate the fundamental principle of double-entry bookkeeping, where every business transaction is reflected in at least two accounts, maintaining the integrity of the financial statements. 8.3.5 Book vs. Market Value Analyzing a company’s financial statements requires understanding the distinction between book value and market value, as these are fundamental to asset and equity valuation. Book value is an accounting measure reflecting the net asset value recorded on the financial statements. It is computed as the difference between total assets and total liabilities: \\[ \\text{Book Value of Equity} = \\text{Total Assets} - \\text{Total Liabilities} \\] For individual assets, book value is calculated by subtracting accumulated depreciation from the asset’s historical cost: \\[ \\text{Book Value of an Asset} = \\text{Historical Cost} - \\text{Accumulated Depreciation} \\] Historical cost encompasses the purchase price and any expenses incurred to bring the asset to its intended use. Accumulated depreciation aggregates the depreciation expenses recognized since the asset’s acquisition, signifying its usage and wear. Market value, in contrast, reflects the current valuation of a company or its assets in the market. The market value of a company’s equity is the product of the market price per share and the total shares outstanding: \\[ \\begin{aligned} \\text{Market Value of Equity} \\ =&amp;\\ \\text{Current Market Price per Share} \\\\ &amp;\\times \\text{Total Number of Outstanding Shares} \\end{aligned} \\] The market value of an individual asset is the expected sale price in the open market: \\[ \\text{Market Value of an Asset} = \\text{Current Market Selling Price of the Asset} \\] Accounting Rules Assets on the balance sheet are typically recorded at their book value, not their market value. However, there are some exceptions where market value is used: Marketable Securities: Financial assets such as stocks or bonds that are held for trading purposes are often reported at their fair market value due to their liquidity and the ease of determining their current market price. Revaluation: Some accounting frameworks, like IFRS, allow for certain assets to be revalued to their current market value. This typically applies to long-term assets that can fluctuate significantly in value, such as real estate or certain investment properties. Impairment: If an asset’s market value drops significantly and is not expected to recover, an impairment loss may be recognized, bringing the book value closer to the market value. It’s important to note that these exceptions are subject to specific accounting rules and standards, which can vary by country and by the accounting framework applied (such as GAAP or IFRS). Comparison Key distinctions between book value and market value include: Valuation Basis: Book value is based on the acquisition cost and accounting adjustments. Market value is based on current economic conditions, future prospects, and investor perceptions. Stability: Book value remains relatively stable over time, only changing due to accounting actions such as depreciation or revaluation. Market value is highly volatile, changing with market conditions. Purpose: Book value is useful for analysts who want to assess the company’s actual invested value in its components and the financial health as per the balance sheet. Market value is used by investors to determine a company’s perceived value and to make decisions about buying or selling its stock. Relevance: Book value provides a conservative measure of a company’s value, important for situations such as liquidation. Market value reflects the real-time speculation on a company’s growth potential and profitability, more relevant for investment decisions. In finance, discerning the nuances between book and market values enables a comprehensive evaluation of a company’s worth. Book value offers a historical basis for valuation, while market value presents an up-to-date appraisal informed by market dynamics. Analysts often examine the divergence between these values to determine market perceptions of over or undervaluation. 8.3.6 Public Company A public company is a business entity that has issued equity securities through an initial public offering (IPO) and is traded on at least one stock exchange or in the over-the-counter market. Unlike a private company such as a bakery, which is owned by an individual or a small group, the ownership of a public company is distributed among public shareholders. Public companies are required to adhere to strict regulatory standards, which include the obligation to disclose detailed financial reports to the public and regulatory bodies. This results in a balance sheet that not only reflects the company’s financial state but also complies with the established financial reporting frameworks such as the Generally Accepted Accounting Principles (GAAP) for the United States, or the International Financial Reporting Standards (IFRS). Table 8.6: Balance Sheet for a Public Company as of December 31, 2023 Account Description Amount Current Assets Cash and Cash Equivalents Liquid funds available for use 170,000 Accounts Receivable Funds to be received from customers 80,000 Prepaid Expenses Payments made in advance for services/goods 20,000 Short-Term Investments Investments maturing within one year 250,000 Inventory Goods available for sale or use 150,000 Other Current Assets Other assets converted to cash within one year 50,000 Non-Current Assets Property, Plant, Equipment (PPE) Assets such as machinery and buildings 1,000,000 Less Accumulated Depreciation Reduction in value of PPE due to wear and tear -150,000 Goodwill Excess of purchase price over fair market value of acquired assets 30,000 Other Intangible Assets Non-physical assets like patents and copyrights 300,000 Less Accumulated Amortization Reduction in value of intangible assets over time -50,000 Investment in Equity Affiliates Investments in affiliate companies 150,000 Long-Term Investments Investments not expected to be sold in a year 400,000 Deferred Tax Assets Estimated tax relief from loss carryforwards 0 Other Non-Current Assets Other assets used over a longer period 100,000 Total Assets 2,500,000 Current Liabilities Accounts Payable Obligations to suppliers and creditors (invoice received) 180,000 Accrued Liabilities Expenses incurred but not yet paid (invoice not yet received) 90,000 Short-Term Debt Debt obligations payable within one year 170,000 Other Current Liabilities Other short-term financial obligations 30,000 Non-Current Liabilities Long-Term Debt Financial obligations due after one year 800,000 Deferred Tax Liabilities Taxes that are accrued but not yet payable 60,000 Other Non-Current Liabilities Other Long-term obligations 110,000 Total Liabilities 1,440,000 Shareholders’ Equity Common Stock Equity raised from issuing common shares 500,000 Preferred Stock Equity raised from issuing preferred shares 100,000 Additional Paid-in Capital Capital received from investors above the par value of the stock 200,000 Retained Earnings Cumulative profits retained in the company 250,000 Accumulated Other Comprehensive Income Total of non-owner changes in equity not included in net income 50,000 Treasury Stock Company’s own stock that it has reacquired -40,000 Total Equity 1,060,000 Total Liabilities and Equity 2,500,000 The balance sheet of a public company, as seen in Table 8.6, typically exhibits a higher degree of complexity and detail compared to that of a private company like a bakery, detailed in Table 8.1. It would typically feature a broader array of assets, liabilities, and equity types, reflecting the larger scale and more complex financial activities of the entity. Public companies also list items such as diverse equity instruments, long-term investments, and may be required to measure some accounts by their fair market value. Par Value of Shares The share’s par value is a nominal value assigned to a share of stock and is the minimum legal price for which the share can be sold at the IPO. This value is largely arbitrary, set by the company at the time of stock creation, and does not necessarily reflect the actual market value of the shares. For common and preferred stock in the equity section of the balance sheet, the amounts listed are usually at par value, especially in the case of common stock. The par value is recorded in the “Common Stock” account, and any excess received from investors over the par value is recorded in the “Additional Paid-in Capital” account. Here is how it is reflected in accounting: \\[ \\begin{aligned} \\text{Common Stock}\\ = &amp;\\ \\text{Number of Shares Issued} \\times \\text{Par Value per Share} \\\\ \\text{Additional Paid-in Capital}\\ = &amp;\\ \\text{Total Capital Received} - \\text{Common Stock} \\end{aligned} \\] In many jurisdictions, the par value also represents the minimum equity that must be maintained per share and can have implications for dividends and accounting treatments. However, in modern accounting practices, many companies set the par value of their shares at a very low figure or even at zero, rendering the concept largely symbolic. The par value is part of the initial recording of equity on the balance sheet and remains constant over time in the accounts, unaffected by changes in the market value of the shares after the company’s IPO. 8.3.7 Financial Firm A financial firm, such as a bank or insurance company, is an institution that provides financial services to its clients, which may include lending, deposit-taking, investment services, and insurance. In contrast to non-financial firms, which primarily produce goods or non-financial services, the core business of financial firms revolves around the management of financial assets and liabilities. Consequently, their balance sheets have distinctive characteristics and line items that reflect their business model. Table 8.7: Balance Sheet for a Financial Firm as of December 31, 2023 Account Description Amount Current Assets Cash and Cash Equivalents Liquid funds available for use 6,000,000 Securities Available for Sale Investment securities that are available for sale 15,000,000 Loans and Receivables Loans issued to customers that are expected to be repaid 8,000,000 Trading Account Assets Securities purchased to sell in the short term for profit 4,000,000 Derivative Assets Financial derivatives used for hedging or trading 200,000 Prepaid Expenses Payments made in advance for services/goods 90,000 Other Current Assets Other assets that will be converted to cash within one year 150,000 Non-Current Assets Investment Securities Held to Maturity Debt securities to be held until maturity 12,000,000 Property, Plant, and Equipment (PPE) Long-term physical assets like buildings and equipment 3,400,000 Less Accumulated Depreciation Reduction in value of PPE due to wear and tear -500,000 Goodwill Excess of purchase price over fair market value of acquired assets 300,000 Other Intangible Assets Non-physical assets like patents and copyrights 800,000 Less Accumulated Amortization Reduction in value of intangible assets over time -40,000 Deferred Tax Assets Estimated tax relief from loss carryforwards 0 Other Non-Current Assets Long-term assets not readily convertible to cash 600,000 Total Assets 50,000,000 Current Liabilities Deposits Money held for customers in checking and savings accounts 18,000,000 Short-Term Borrowings Short-term loans and obligations 8,000,000 Trading Account Liabilities Obligations from short-term security trades 300,000 Derivative Liabilities Financial derivatives used for hedging or trading 250,000 Accounts Payable and Other Liabilities Obligations to suppliers and short-term creditors 400,000 Non-Current Liabilities Long-Term Debt Borrowings and financial obligations due after one year 5,000,000 Deferred Tax Liabilities Taxes that are accrued but not yet payable 200,000 Other Non-Current Liabilities Obligations payable over a long period not covered by other categories 300,000 Total Liabilities 32,450,000 Shareholders’ Equity Common Stock Equity raised from issuing common shares 2,500,000 Preferred Stock Equity raised from issuing preferred shares 1,000,000 Additional Paid-in Capital Capital received from investors above the par value of the stock 3,000,000 Retained Earnings Profits reinvested in the firm not distributed as dividends 500,000 Accumulated Other Comprehensive Income Changes in equity from non-owner sources not through P&amp;L 200,000 Treasury Stock Company’s own stock that it has reacquired -100,000 Total Equity 17,550,000 Total Liabilities and Equity 50,000,000 The balance sheet of a financial firm, illustrated in Table 8.7, typically features assets such as loans and receivables, which are funds lent to consumers and firms expecting repayment with interest. These are considered productive assets for a financial firm, generating revenue through interest income. Liabilities on a financial firm’s balance sheet are predominantly customer deposits, a primary source of funding for banks. They also include short-term borrowings and long-term debt, reflecting borrowing from other institutions and through debt instruments. In summary, the balance sheet of a publicly traded financial firm is characterized by a significant presence of financial assets and liabilities that are the direct result of its lending, investing, and deposit-taking activities. This contrasts with the balance sheet of a non-financial firm, as shown in Table 8.6, which is more likely to reflect the operational infrastructure required to produce goods and services. 8.4 Ledger A ledger is a comprehensive collection of all the accounts of a business, where financial transactions are recorded. It serves as the central repository for accounting data, based on initial recordings made in various types of journals. Journals are chronological records of transactions; the sales journal records sales transactions, the purchases journal documents purchases, and the general journal captures miscellaneous transactions that do not belong in the other specialized journals. 8.4.1 Ledger Accounts Each ledger account provides a historical view of all changes over time, revealing the current balance for that account. The following is an illustration of an “Inventory” ledger account for a bakery, capturing transactions like purchases of ingredients and the reduction of inventory due to sales or spoilage: Table 8.8: Ledger Account for Inventory Date Description Reference Debit Credit 2023-01-01 Beginning Inventory BAL 1,000 0 2023-01-05 Flour Purchase PUR001 500 0 2023-01-10 Sugar Purchase PUR002 900 0 2023-01-18 Reduction due to Sales COGS001 0 300 2023-01-25 Dairy Purchase PUR003 150 0 2023-01-30 Write-off due to Spoilage WRT001 0 50 2023-02-10 Reduction due to Sales COGS002 0 500 etc. Total 2023 34,000 49,000 Table 8.8 demonstrates the “Inventory” ledger account, with debit entries for ingredient purchases and credit entries for sales and spoilage. Transactions include dates and descriptions for clarity, and the “Reference” column links transactions to supporting documents, with “PUR001” for flour purchase on January 5th, and “COGS001” for the cost of goods sold on January 18th. These links ensure traceability for accurate financial reporting and audits. The “Inventory” account, being an asset, increases with debit entries and decreases with credit entries. The ending balance is calculated as the total debits minus total credits for the period. Hence, the sum of the debit column less the sum of the credit column for all transactions in 2023 equates to 15,000, representing the balance sheet value for the inventory account as of December 31, 2023, as shown in Table 8.1. On the liability side, the “Accounts Payable” ledger account records increases in liabilities with credit entries and decreases with debit entries, as shown in Table 8.9. Table 8.9: Ledger Account for Accounts Payable Date Description Reference Debit Credit 2023-01-01 Beginning Balance BAL 0 12,000 2023-01-05 Flour Purchase on Credit PUR001 0 500 2023-01-10 Sugar Purchase on Credit PUR002 0 900 2023-01-15 Payment to Flour Supplier PAY001 500 0 2023-01-19 Oven Purchase on Credit EQP001 0 82,000 2023-01-25 Dairy Purchase on Credit PUR003 0 150 2023-01-30 Payment to Sugar Supplier PAY002 900 0 2023-02-01 Payment for Half of Oven Cost PAY003 41,000 0 etc. Total 2023 225,000 243,000 As “Accounts Payable” is a liability account, increases are recorded in the credit column and decreases in the debit column. Therefore, the ending balance is the total of the credit column minus the total of the debit column for all transactions in 2023. This results in 18,000, which is the entry for the accounts payable balance on the balance sheet as of December 31, 2023, as displayed in Table 8.1. 8.4.2 T-Accounts The structure of a ledger account is often referred to as a T-account, depicted in the shape of the letter “T” which separates debit and credit transactions into the left and right columns, respectively. This format simplifies the visualization of the increases and decreases in the corresponding account. As per the accounting equation, every transaction is recorded in at least two separate ledger accounts, appearing as a debit in one account and as a credit in another. For instance, the purchase of sugar on January 10 is recorded as a debit in the “Inventory” ledger account (see Table 8.8), reflecting an increase in assets, and as a credit in the “Accounts Payable” ledger account (see Table 8.9), indicating an increase in liabilities. Table 8.10 illustrates this dual entry, showing the transaction in both T-accounts side by side. Table 8.10: T-Account Transactions for a Sugar Purchase on Credit Inventory Accounts Payable Debit Credit Debit Credit 900 0 0 900 Similarly, the payment of to the sugar supplier on January 30 is recorded as a credit in the “Cash” ledger account, reflecting a decrease in assets, and as a debit in the “Accounts Payable” ledger account (see Table 8.9), indicating a decrease in liabilities. Table 8.11 illustrates this dual entry, showing the transaction in both T-accounts side by side. Table 8.11: T-Account Transactions for a Payment to Sugar Supplier Cash Accounts Payable Debit Credit Debit Credit 0 900 900 0 In conclusion, T-accounts provide a clear and structured way to record and track the financial transactions of a business. 8.4.3 Basis for Financial Statements The information in the ledger is used to prepare Financial Statements, which include: The Balance Sheet (already discussed in Chapter 8.3): Shows the company’s assets, liabilities, and owner’s equity at a specific point in time. The Income Statement (see Chapter 8.5): Summarizes the company’s revenues and expenses over a period of time to show profit or loss. The Cash Flow Statement (see Chapter 8.6): Analyzes the company’s cash inflows and outflows during a period. Every ledger account will appear in at least one of the three financial statements, as they collectively cover all the financial activities of a company. Asset, Liability, and Equity Accounts are summarized on the Balance Sheet, which shows the company’s financial position at a specific date. Revenue and Expense Accounts appear on the Income Statement to calculate the net income or loss for the period. Cash and Cash Equivalents Accounts are detailed on the Cash Flow Statement, highlighting the sources and uses of cash. Some ledger accounts are specific to one financial statement and do not appear on the others. For example, the “Buildings” account, classified under Property, Plant, and Equipment (PPE), is presented on the Balance Sheet. It represents the company’s investment in physical, long-term assets. This specific account does not appear on the Income Statement. Instead, the financial effect of the building over time is captured by the “Depreciation” account on the Income Statement. This account reflects the building’s cost allocation over its useful life as an expense, influencing the company’s reported profits. To summarize, the ledger meticulously records and monitors the business’s financial transactions and forms the basis for creating financial statements. 8.5 Income Statement An income statement, also known as profit and loss statement (P&amp;L), is a key financial document that summarizes a company’s financial performance over a specific time frame, such as a fiscal quarter or year. It details how a company generates revenue, incurs costs, and the resulting profit or loss. 8.5.1 Stock vs. Flow Variables The balance sheet and income statement serve distinct roles in financial reporting due to their use of different types of variables. The balance sheet is a financial snapshot capturing a firm’s status at a specific point in time using stock variables. These variables - assets, liabilities, and equity - represent quantities at a moment in time, akin to a photograph of a company’s financial resources, obligations, and net worth. In contrast, the income statement employs flow variables. These are measures of economic activity over a period, reflecting the changes in stock variables. Revenues and expenses, for example, represent the inflow and outflow of economic value that alter the company’s net worth over time, much like the continuous footage of a film showing the dynamic process of a business’s operations. Thus, flow variables essentially capture the incremental changes that stock variables undergo during the accounting period. 8.5.2 Profit Equation The income statement follows the basic profit equation: \\[ \\text{Profit} = \\text{Revenue} - \\text{Expenses} \\] Here, the profit is what remains after all expenses are deducted from revenues. The income statement lists different types of profits, such as gross profit, operating income, and net income before and after taxes. Table 8.12 provides an example. Table 8.12: Income Statement for the Year Ended December 31, 2023 Account Amount Total Operating Revenues: \\(+\\) Revenue from Sales of Goods 1,100,000 \\(+\\) Service Revenue 500,000 Cost of Sales: \\(-\\) Cost of Goods Sold (COGS) -400,000 \\(-\\) Cost of Services Provided (Direct Labor) -100,000 Gross Profit 1,100,000 Operating Expenses: \\(-\\) Salaries and Wages (Non-Direct Labor) -350,000 \\(-\\) Payroll Taxes -35,000 \\(-\\) Rent -50,000 \\(-\\) Property Tax -10,000 \\(-\\) Utilities -20,000 \\(-\\) Depreciation -15,000 \\(-\\) Amortization -5,000 \\(-\\) Marketing and Advertising -30,000 \\(-\\) General and Administrative Expenses -70,000 \\(-\\) Other Operating Expenses -20,000 Operating Income (EBIT) 495,000 Non-Operating Revenues: \\(+\\) Interest Income 2,000 \\(+\\) Rental Income 12,000 \\(+\\) Gain on Sale of Property, Plant, and Equipment (PPE) 5,000 \\(+\\) Gain on Sale of Intangible Assets 3,000 \\(+\\) Net Gain on Trade in Investment Securities 7,000 \\(+\\) Net Gain on Trade in Other Assets -3,000 \\(+\\) Subsidies Received 1,000 Non-Operating Expenses: \\(-\\) Interest Expense -20,000 \\(-\\) Non-Operating Rental Expenses -1,000 \\(-\\) Loss on Sale of PPE -1,000 \\(-\\) Loss on Sale of Intangible Assets 0 Net Income Before Taxes (EBT) 500,000 \\(-\\) Benefit of Deferred Tax Asset (Loss Carryforwards) 0 Net Income Before Taxes (EBT), Adjusted 500,000 \\(-\\) Income Tax Expense (\\(=\\) Tax Rate \\(\\times\\) Adjusted EBT) -105,000 Net Income 395,000 Table 8.12 reveals the financial performance of a business over a fiscal year. The statement categorically separates operating and non-operating sections to clearly distinguish between the core and peripheral activities of the business. 8.5.3 Gross Profit The Gross Profit represents the profit a company makes after deducting the costs associated with making and selling its products, or the costs associated with providing its services. It is a measure of production efficiency and cost management. \\[ \\text{Gross Profit} = \\text{Operating Revenues} + \\text{Cost of Sales} \\] Here, “Cost of Sales” consists of “Cost of Goods Sold (COGS)” and “Cost of Services Provided,” which are the direct costs attributable to the production of the goods sold. This includes materials and labor directly used in creating the product. The Gross Profit helps in understanding how well the company is managing its production processes and supply chain. 8.5.4 Operating Income (EBIT) Operating Income, also known as Earnings Before Interest and Taxes (EBIT), reflects the company’s earnings from its core business operations. It excludes income and expenses from non-operating activities. \\[ \\begin{aligned} \\text{Operating Income (EBIT)} \\ = &amp;\\ \\text{Gross Profit}\\\\ &amp;\\ - \\text{Operating Expenses (OpEx)} \\end{aligned} \\] Operating Expenses typically include salaries, rent, utilities, depreciation, and other costs related to the day-to-day operations of the business. This metric is crucial as it focuses solely on the operational performance of a business, excluding external factors like investments, taxes, or financing costs. 8.5.5 Net Income Before Taxes (EBT) Net Income Before Taxes, also known as Earnings Before Taxes (EBT), accounts for both operating results and non-operating activities, including various incomes and expenses that are not directly tied to the core business operations. \\[ \\begin{aligned} \\text{Net Income Before Taxes (EBT)} \\ = &amp;\\ \\text{Operating Income (EBIT)} \\\\ &amp;\\ + \\text{Non-Operating Revenues} \\\\ &amp;\\ - \\text{Non-Operating Expenses} \\end{aligned} \\] This includes revenues like interest and rental income, and expenses such as interest expenses and losses on the sale of assets. It shows the company’s profitability before government taxes are applied. 8.5.6 Net Income (EAT) Net Income, also known as Earnings After Tax (EAT), is the total profit or loss after all revenues, costs, and expenses, including taxes, have been accounted for. It’s the most crucial figure for stakeholders as it represents the company’s profitability and financial health over the accounting period. \\[ \\begin{aligned} \\text{Net Income (EAT)}\\ = &amp;\\ \\text{Net Income Before Taxes (EBT)} \\\\ &amp;\\ - \\text{Income Tax Expense} \\end{aligned} \\] Net Income is the bottom line of the income statement and the number from which earnings per share (EPS) are calculated, influencing investment decisions and company valuations. 8.5.7 Transactions Just as transactions that affect the balance sheet are recorded, so too are transactions that impact the income statement recorded using the double-entry system. This ensures that the accounting equation (Assets = Liabilities + Equity) always remains in balance after each transaction. The rules for debits and credits, inclusive of revenue and expense accounts, are as follows: Assets: Debit to increase, Credit to decrease. Liabilities: Credit to increase, Debit to decrease. Equity: Credit to increase, Debit to decrease. Revenue: Credit to increase, Debit to decrease. Expenses: Debit to increase, Credit to decrease. Overall, the income statement is a key financial document that provides a summary of how the business performs, reflecting its ability to generate profits through operations, manage its expenses, and grow over time. 8.5.8 Corporate Income Tax The income statement is a critical financial document scrutinized by tax authorities to determine a company’s tax obligations and identify viable deductions and credits. Taxable Income The income statement is instrumental in establishing taxable income, specifically the Adjusted Earnings Before Taxes (EBT). Deductions play a crucial role in reducing taxable income and thus tax liabilities. Common deductions include operating expenses like salaries, rent, and utilities, as well as depreciation and amortization of assets. These deductions must be carefully itemized and substantiated in the income statement to ensure compliance with tax laws. Income Tax Expense The Income Tax Expense on an income statement represents the cost of income taxes a company paid or is expected to pay to the government for the current reporting period. The basic formula to compute the Income Tax Expense is given by: \\[ \\begin{aligned} \\text{Income Tax Expense} \\ = &amp;\\ \\text{Tax Rate} \\times \\text{Adjusted EBT} \\end{aligned} \\] In the United States, corporate income tax rates are determined by a combination of federal and state policies. The federal corporate tax rate stands at 21%, instituted by the Tax Cuts and Jobs Act of 2017. This federal rate is compounded by state corporate taxes, which fluctuate widely, from less than 0% to over 11%. Localities may impose additional taxes, further affecting a corporation’s total tax liability. Tax Shield A Tax Shield is the reduction in income taxes that results from taking an allowable deduction from taxable income. For example, interest expenses for corporations can act as a tax shield because they lower taxable income and thus results in lower tax obligations. The value of a tax shield is given by: \\[ \\begin{aligned} \\text{Value of Tax Shield}\\ = &amp;\\ \\text{Deductible Expense} \\times \\text{Tax Rate} \\end{aligned} \\] This quantifies the tax-saving effect of a deductible expense. Examples are: \\[ \\begin{aligned} \\text{Tax Shield on Interest Expenses} \\ = &amp;\\ \\text{Interest Expenses} \\times \\text{Tax Rate} \\\\ \\text{Depreciation Tax Shield} \\ = &amp;\\ \\text{Depreciation Expense} \\times \\text{Tax Rate} \\\\ \\text{Charitable Contribution Tax Shield} \\ = &amp;\\ \\text{Charitable Contributions} \\times \\text{Tax Rate} \\\\ \\text{Loss Carryforward Tax Shield} \\ = &amp;\\ \\text{Loss Carryforwards} \\times \\text{Tax Rate} \\end{aligned} \\] where “Loss Carryforwards” are discussed next. Loss Carryforward When corporations incur a negative taxable income, known as a Net Operating Loss (NOL), they can use this loss to decrease taxable income in subsequent fiscal periods through a mechanism called Loss Carryforward. Loss Carryforward allows companies to apply past losses to future profits, thereby lowering future tax liabilities. This potential tax benefit is reflected as a Deferred Tax Asset on the balance sheet, signifying probable reductions in future tax payments. The value of this asset hinges on the expectation that the firm will generate enough future taxable income against which the NOLs can be applied. Other Tax Concepts Effective Tax Rate: The effective tax rate is the average rate at which a corporation is taxed on pre-tax profits, taking into account federal, state, and local taxes, as well as any deductions or credits. Deferred Tax Liability: When a company’s taxable income is lower than its accounting earnings due to differences in accounting methods, a deferred tax liability is recorded. This liability represents future tax payments a company is expected to make. In summary, the income statement is indispensable for its role in tax reporting and planning. It not only determines the current tax liabilities but also influences future tax periods through the identification of potential deductions, tax shields, loss carryforwards, and deferred tax items. Accurate and detailed income statements ensure that companies are not only compliant with tax regulations but also strategically positioned to optimize their tax obligations. 8.6 Cash Flow Statement The Cash Flow Statement is a financial document that provides aggregate data regarding all cash inflows a company receives from its ongoing operations and external investment sources, as well as all cash outflows that pay for business activities and investments during a given period. Unlike the income statement, which is based on the accrual basis of accounting, the cash flow statement reveals the liquidity and solvency of the company, offering a more tangible measure of the cash being generated and used. Table 8.13 represents an example of a cash flow statement. Table 8.13: Cash Flow Statement for the Year Ended December 31, 2023 Activity Amount Subtotal Total Net Income (EAT) 269,000 Adjustments for Non-Cash Items (D&amp;A): \\(+\\) Depreciation 15,000 \\(+\\) Amortization 5,000 Changes in Working Capital (WC): \\(+\\) Increase in Accounts Payable 8,000 \\(-\\) Increase in Accounts Receivable -7,000 \\(+\\) Increase in Accrued Expenses 4,000 \\(-\\) Increase in Prepaid Expenses -3,000 \\(-\\) Increase in Inventory -12,000 Net Cash Provided by Operating Activities (CFO) 279,000 Capital Expenditures (CapEx): \\(-\\) Payments for Purchase of PPE -50,000 \\(-\\) Payments for Acquisition of Intangible Assets -8,000 Disposal of Fixed Assets: \\(+\\) Proceeds from Sale of PPE 12,000 \\(+\\) Proceeds from Sale of Intangible Assets 3,000 Other Investing Activities: \\(-\\) Payments for Purchase of Investment Securities -15,000 \\(+\\) Proceeds from Sale of Investment Securities 7,000 \\(-\\) Loans Provided to Others -10,000 \\(+\\) Proceeds from Repayment of Loans Made to Others 5,000 Net Cash Provided by Investing Activities (CFI) -56,000 Debt Financing Activities: \\(+\\) Proceeds from Issuance of Debt 40,000 \\(-\\) Repayments of Debt -25,000 Equity Financing Activities: \\(+\\) Proceeds from Issuance of Equity 20,000 \\(-\\) Repurchase of Equity -5,000 \\(-\\) Dividends Paid -8,000 Net Cash Provided by Financing Activities (CFF) 22,000 Net Increase in Cash and Cash Equivalents 245,000 \\(+\\) Cash and Cash Equivalents at Beginning of Period 1,200,000 Cash and Cash Equivalents at End of Period 1,445,000 As depicted in Table 8.13, the cash flow statement begins with net income, reflecting the firm’s profitability. Yet, not all revenues and expenses that contribute to profit involve immediate cash transactions - some are incurred on credit and therefore do not impact the company’s cash position in the current period. For instance, accounts payable represent the company’s credit-based expenditures and are recorded as liabilities. An increase in accounts payable signifies that the firm owes more to its suppliers or creditors at the period’s end compared to the beginning, indicating that the expenses were not paid in cash. Consequently, this increase is added back to net income when calculating cash flow, as it reflects cash retained by the company. The different parts of the cash flow statement in Table 8.13 are examined next. 8.6.1 Operating Activities Cash Flows from Operations (CFO) are the cash flows produced by the primary revenue-generating activities of the business. This section of the cash flow statement is typically derived from the net income figure, with adjustments for: \\[ \\begin{aligned} \\text{Net Cash Provided by Operating Activities (CFO)} \\ = &amp;\\ \\text{Net Income (EAT)} \\\\ &amp;\\ + \\text{Non-Cash Expenses (D\\&amp;A)} \\\\ &amp;\\ + \\text{Increase in Working Capital (WC)} \\end{aligned} \\] The adjustments include adding back non-cash expenses such as depreciation and amortization, and adjusting for changes in working capital (current assets and current liabilities). It reflects the cash effects of transactions that enter into the determination of net income. 8.6.2 Investing Activities Cash Flows from Investing (CFI) are related to the acquisition and disposal of long-term assets and other investments not included in cash equivalents. This section details the outflows and inflows from purchases and sales of long-term business investments such as property, plant, equipment, and marketable securities. \\[ \\begin{aligned} \\text{Net Cash Provided by Investing Activities (CFI)} \\ = &amp;\\ \\text{Cash Paid for Investments} \\\\ &amp;\\ - \\text{Cash Received from Disposals} \\end{aligned} \\] A negative amount indicates that more cash has been spent on investment activities than has been received from such transactions. 8.6.3 Financing Activities Cash Flows from Financing (CFF) include transactions involving debt, equity, and dividends. This section shows the net flows of cash that are used to fund the company overall, such as the issuance of debt or equity, as well as dividend payments and capital lease obligations. \\[ \\begin{aligned} \\text{Net Cash Provided by Financing Activities (CFF)} \\ = &amp;\\ \\text{Cash from Issuing Debt} \\\\ &amp; + \\text{Cash from Issuing Equity} \\\\ &amp; - \\text{Cash Paid for Dividends and} \\\\ &amp; \\quad \\ \\ \\text{Reacquisition of Debt/Equity} \\end{aligned} \\] The sign of the net cash flow from financing activities indicates whether a company is accumulating or repaying debt, engaging in equity transactions, or returning capital to shareholders. 8.6.4 Net Change in Cash This final section of the cash flow statement reconciles the net increase or decrease in cash by adding the net cash provided by (or used in) the operating, investing, and financing activities. \\[ \\begin{aligned} \\text{Net Increase in Cash} \\ =&amp;\\ \\text{Net Cash from Operating Activities (CFO)} \\\\ &amp;\\ + \\text{Net Cash from Investing Activities (CFI)} \\\\ &amp;\\ + \\text{Net Cash from Financing Activities (CFF)} \\end{aligned} \\] This figure is then adjusted by the beginning cash balance to arrive at the ending cash balance for the period. This provides a snapshot of the firm’s liquidity at the end of the period. 8.6.5 Supplemental Information The cash flow statement often includes supplemental information, such as the amount of interest and income taxes paid, which provides a clearer picture of how cash is moving in and out of the business. The statement also reconciles beginning and ending cash balances, which can be tied back to the balance sheet. Understanding the cash flow statement is essential as it helps stakeholders determine the short-term viability of the company, particularly its ability to pay bills. Unlike the income statement, it shows the actual cash the business has generated and can be used for expansion, paying dividends, or improving infrastructure. 8.6.6 Free Cash Flows Free Cash Flow to the Firm (FCFF) and Free Cash Flow to Equity (FCFE) are two critical metrics derived from the cash flow statement that investors use to evaluate a company’s cash generation efficiency. They are not standard items on the cash flow statement; rather, it is a metric that can be calculated using information from the cash flow statement and other financial statements. Free Cash Flow to the Firm (FCFF) FCFF, also known as Free Cash Flow (FCF), represents the amount of cash generated by a company’s operations that is available for distribution to all financial claimants, including debt and equity holders, after accounting for necessary capital expenditures (CapEx) and operating expenses (OpEx). The formula for calculating FCFF is as follows: \\[ \\begin{aligned} FCFF \\ = &amp;\\ \\underbrace{\\text{EAT}+\\text{D\\&amp;A}-\\text{WC}}_{\\text{CFO}} - \\text{CapEx} \\\\ &amp;\\ + \\underbrace{\\text{Interest Expense} - \\text{Interest Tax Shield}}_{ \\text{Interest Expense} \\times (1 - \\text{Tax Rate}) } \\end{aligned} \\] Note that the first three items add up to Cash Flow From Operating Activities (CFO), available on the cash flow statement, and CapEx is also on the cash flow statement under Operating Activities. Interest Expenses are available on the income statement, and the Tax Rate can be calculated using the income statement as Income Tax Expense divided by the Adjusted EBT. Non-cash expenses typically include items like depreciation and amortization. The tax-adjusted interest expense is added back because FCFF is meant to represent cash available to both debt and equity holders, and interest is a cash flow available to debt holders. Free Cash Flow to Equity (FCFE) FCFE, on the other hand, is the amount of cash available to be returned to shareholders after all expenses, reinvestments, and debt repayments have been made. It is essentially the cash flow from operations minus capital expenditures and debt service (including net debt issuance). The formula for FCFE is: \\[ \\begin{aligned} FCFE \\ = &amp;\\ \\underbrace{\\text{EAT}+\\text{D\\&amp;A}-\\text{WC}}_{\\text{CFO}} - \\text{CapEx} \\\\ &amp;\\ +\\underbrace{\\text{Proceeds from Issuance of Debt} - \\text{Repayments of Debt}}_{ \\text{Net Borrowing}} \\end{aligned} \\] where net borrowing is the difference between any new debt taken on and debt that is paid off, which is available on the cash flow statement under borrowing financing activities. Both FCFF and FCFE are leveraged by companies to make investment decisions, determine dividend policies, and assess potential growth opportunities. They are also extensively used in Discounted Cash Flow (DCF) analysis to estimate a company’s value. DCF analysis computes the stock price as the present value of all future dividend payments. Since dividend payments are sometimes arbitrary, influenced by dividend politics, the FCFE is often used for DCF instead of dividends. 8.7 Conclusion In closing, the discipline of business accounting is the cornerstone of financial transparency and strategic planning within a company. The meticulous art of recording, analyzing, and summarizing financial transactions crystallizes into the foundational financial statements: the balance sheet, the ledger, the income statement, and the cash flow statement. Each statement provides unique insights, capturing different facets of a business’s financial reality. As we turn the page from understanding the scaffolding of business accounting, we venture into the realm of interpreting and leveraging this financial information. The upcoming Chapter 9 builds upon the data presented in the financial statements to compute a suite of financial performance indicators. These indicators serve as the compass for stakeholders, guiding investment decisions, operational improvements, and strategic shifts. By translating raw financial data into actionable insights, these indicators empower users to analyze a company with precision and context. References "],["financial-performance-indicators.html", "Chapter 9 Financial Performance Indicators 9.1 Profitability Indicators 9.2 Liquidity Indicators 9.3 Leverage Indicators 9.4 Efficiency Indicators 9.5 Valuation Indicators 9.6 Cash Flow Indicators 9.7 Temporal Dimensions 9.8 Conclusion", " Chapter 9 Financial Performance Indicators When we look at a company’s financial health, we turn to key numbers that tell us a lot without needing to dig into every detail. These are called financial performance indicators, and they are based on the big three financial statements: the balance sheet, income statement, and cash flow statement, introduced in Chapter 8. These indicators, often called ratios or metrics, are like shortcuts that help us compare different businesses. They give us quick signals about how well a company is doing in areas like making money (profitability), paying off its short-term debts (liquidity), using borrowed money (leverage), getting the most out of its resources (efficiency), and what its shares are worth compared to its financial performance (valuation). Here’s how they help: Profitability Ratios: They tell us how good a company is at making money compared to its size or how much it sells. Liquidity Ratios: These numbers show if a company has enough cash to pay bills that are coming up soon. Leverage Ratios: These show how much the company relies on debt to fund its activities. More debt can mean higher risk. Efficiency Ratios: We use these to see how well a company uses its assets to make money. Valuation Ratios: These help us figure out if the price of a company’s stock makes sense compared to its actual earnings or book value. Cash Flow Ratios: These ratios reveal the actual cash a company generates and has available for things like dividends, reinvestment, or paying off debt. They’re crucial for understanding the real financial flexibility of a business. Each type of ratio can tell investors or managers something important about where the company stands. For instance, if a company has a lot of products in stock, we’d look closely at how quickly they can turn that inventory into cash. For a tech company with high growth expectations, we might focus more on how the stock market values it compared to its earnings. In simple terms, these financial performance indicators are like the health checkup metrics a doctor might use. They don’t give all the answers, but they’re a quick way to see if something might need a closer look. 9.1 Profitability Indicators Profitability ratios are a class of financial metrics that are used to assess a company’s ability to generate profit from its operations, assets, and capital. These indicators are essential for investors, management, and analysts as they provide insights into the company’s performance, efficiency, and comparative advantage in the market. Here is a more detailed discussion of each key profitability indicator and their broader context: 9.1.1 Net Profit Margin Definition and Formula: The Net Profit Margin is a primary measure of profitability. It reveals what percentage of sales has turned into profit. \\[ \\text{Net Profit Margin} = \\frac{\\text{Net Income}}{\\text{Revenue}} \\times 100 \\] Context and Application: This ratio is vital for comparisons within industries, as a higher net profit margin than peers can indicate better cost control, more successful marketing, or superior pricing strategies. It also reflects the overall impact of management’s policies and operational efficiency. 9.1.2 Return on Assets (ROA) Definition and Formula: ROA is a comprehensive indicator of how profitable a company is relative to its total assets. \\[ \\text{ROA} = \\frac{\\text{Net Income}}{\\text{Total Assets}} \\times 100 \\] Context and Application: ROA tells investors how effectively the company is converting the money it has invested in assets into net income. The higher the ROA, the more money the company is earning on its assets. A ROA that increases over time indicates improving asset efficiency. 9.1.3 Return on Investment (ROI) Definition and Formula: ROI measures the gain or loss generated on an investment relative to the amount of money invested. \\[ \\text{ROI} = \\frac{\\text{Net Income}}{\\text{Initial Investment}} \\times 100 \\] Context and Application: ROI is a versatile and widely used indicator of profitability. It can apply to various investment scenarios, from the performance of a stock portfolio to the returns on specific project investments within the company. It allows for assessing the efficiency of different investments. 9.1.4 Return on Equity (ROE) Definition and Formula: ROE signifies how good a company is at rewarding its shareholders with profits. \\[ \\text{ROE} = \\frac{\\text{Net Income}}{\\text{Shareholders&#39; Equity}} \\times 100 \\] Context and Application: Often referred to as the ‘return on net worth,’ ROE is a signal of how effectively management is using the shareholders’ capital. In general, a steadily high ROE suggests that a company is reinvesting its earnings wisely to fuel organic growth. 9.1.5 Earnings Per Share (EPS) Definition and Formula: EPS calculates the portion of a company’s profit allocated to each share of common stock, serving as an indicator of a company’s profitability. \\[ \\text{EPS} = \\frac{\\text{Net Income} - \\text{Preferred Dividends}}{\\text{Average Outstanding Shares}} \\] Context and Application: It is a direct link to the stock markets through the Price-to-Earnings (P/E) ratio, where the ‘E’ is the EPS. It’s closely monitored by investors as it directly impacts their returns. 9.1.6 Operating Margin Definition and Formula: The Operating Margin reflects the percentage of revenue left after paying for variable costs of production like wages and raw materials. \\[ \\text{Operating Margin} = \\frac{\\text{Operating Income}}{\\text{Revenue}} \\] Context and Application: A high operating margin is usually a sign that the company is managing its costs well. It is particularly useful for comparing companies within the same industry or sector to understand how well they are performing from core operations. 9.1.7 Dividend Yield Definition and Formula: Dividend Yield is a financial ratio that shows how much a company pays out in dividends each year relative to its share price. \\[ \\text{Dividend Yield} = \\frac{\\text{Annual Dividends Per Share}}{\\text{Market Price Per Share}} \\] Context and Application: This ratio is of particular interest to income investors looking for stable and predictable returns. A high dividend yield can be a sign of a company that generates sufficient cash flow and prioritizes returning profits to shareholders, although it could also signal a depressed stock price. 9.1.8 Gross Profit Margin Definition and Formula: The Gross Profit Margin measures the financial health of a company’s core activities, excluding the indirect costs. It represents the proportion of money left over from revenues after accounting for the cost of goods sold (COGS). \\[ \\text{Gross Profit Margin} = \\frac{\\text{Revenue} - \\text{COGS}}{\\text{Revenue}} \\times 100 \\] Context and Application: This indicator is crucial in understanding a company’s production efficiency. A higher gross profit margin suggests that a company is selling its inventory at a higher profit percentage, indicating good control over production costs and efficient management of labor and supplies. It is particularly important for comparing companies within the same industry, as it reflects the ability to manage costs compared to its competitors. 9.1.9 Return on Capital Employed (ROCE) Definition and Formula: ROCE is a ratio that indicates the efficiency and profitability of a company’s capital investments. It measures a company’s success in generating returns from its total capital employed, which includes equity and debt. \\[ \\text{ROCE} = \\frac{\\text{Earnings Before Interest and Tax (EBIT)}}{\\text{Capital Employed}} \\times 100 \\] Context and Application: ROCE is an important metric as it encompasses both equity and debt, providing a more complete view of financial performance than ROE, which only considers equity. This measure is vital for assessing the effectiveness of a firm’s investment decisions, determining how well a company is using its capital to generate profits. A high ROCE indicates efficient use of capital. It is especially useful in capital-intensive sectors, such as manufacturing, where investments in fixed assets are significant. Profitability indicators are pivotal in assessing a company’s financial viability and operational efficiency. They play a significant role in investment decision-making, corporate finance strategy, and comparative analysis across sectors. Investors and analysts use these ratios to determine a firm’s ability to generate profit and value, thus informing decisions about stock valuation, creditworthiness, and investment potential. During economic fluctuations, robust profitability can signify a company’s resilience and its capacity to weather market challenges. However, it is essential to contextualize these indicators within industry benchmarks, as profit margins and returns vary widely among different sectors. 9.2 Liquidity Indicators Liquidity indicators are financial metrics that gauge a company’s ability to meet its short-term debt obligations with its most liquid assets. These ratios are crucial for stakeholders to evaluate the short-term financial health of a business, as they reflect the firm’s capacity to generate cash quickly to cover debts and operational expenses. 9.2.1 Current Ratio Definition and Formula: The Current Ratio is a primary liquidity measure that evaluates whether a company has enough resources to pay its debts over the next 12 months. It compares a firm’s current assets to its current liabilities. \\[ \\text{Current Ratio} = \\frac{\\text{Current Assets}}{\\text{Current Liabilities}} \\] Context and Application: A current ratio above 1 suggests that the company has more liquid assets than short-term obligations, which indicates good short-term financial health. However, a very high current ratio could also imply that the company is not using its current assets efficiently or is not managing its working capital properly. 9.2.2 Quick Ratio Definition and Formula: Also known as the ‘acid-test’ ratio, the Quick Ratio measures a company’s ability to cover its short-term liabilities with its most liquid assets, hence excluding inventories, which are less liquid. \\[ \\text{Quick Ratio} = \\frac{\\text{Current Assets} - \\text{Inventories}}{\\text{Current Liabilities}} \\] Context and Application: This ratio provides a stringent assessment of liquidity by removing inventory, which may not be as readily convertible to cash. A quick ratio higher than 1 is typically considered as indicative of good short-term financial strength, but the ideal ratio varies by industry. 9.2.3 Cash Ratio Definition and Formula: The Cash Ratio is the most conservative liquidity indicator, as it considers only cash and cash equivalents as available resources to meet short-term liabilities. \\[ \\text{Cash Ratio} = \\frac{\\text{Cash and Cash Equivalents}}{\\text{Current Liabilities}} \\] Context and Application: Since it only factors in the most liquid assets, the cash ratio tells us whether a company can immediately pay off its short-term debt. This ratio is of particular interest to lenders and creditors who want a clear picture of a company’s liquidity without the potential ambiguity of receivables and inventories. 9.2.4 Operating Cash Flow Ratio Definition and Formula: This ratio measures how well current liabilities are covered by the cash flow generated from a company’s operations. \\[ \\text{Operating Cash Flow Ratio} = \\frac{\\text{Cash Flow from Operations}}{\\text{Current Liabilities}} \\] Context and Application: The operating cash flow ratio provides insight into a company’s operational efficiency and its ability to convert sales into cash. It reflects how well current liabilities are supported by cash generated from a company’s core business operations, serving as an indicator of operating liquidity. These liquidity indicators are applied across different contexts, such as in credit analysis, internal financial management, and by investors evaluating the risk of their investment. They are particularly critical in times of financial stress when a company’s ability to quickly convert assets to cash determines its capacity to survive economic downturns. It’s important to note that the interpretation of these ratios should be industry-specific, as different industries have varying standards for what constitutes a healthy liquidity level. 9.3 Leverage Indicators Leverage ratios are essential for understanding the financial structure of a company, particularly how much capital comes in the form of debt (loans) or shareholders’ equity (ownership). These indicators provide insight into the financial risk profile of the company and its capability to meet financial obligations. 9.3.1 Debt-to-Equity (D/E) Ratio Definition and Formula: The Debt-to-Equity (D/E) Ratio is a leverage ratio that compares a company’s total liabilities to its shareholder equity. It illustrates the proportion of equity and debt the company uses to finance its assets, and the balance of debt and equity financing. \\[ \\text{D/E Ratio} = \\frac{\\text{Total Liabilities}}{\\text{Shareholders&#39; Equity}} \\] Context and Application: This indicator is a reflection of a company’s capital structure and indicates how much risk is being taken by financing with debt. A higher D/E ratio suggests that a company might be a riskier investment, especially if market conditions are volatile. Creditors and investors use this ratio to assess the balance of equity and debt financing, with different industries having different benchmarks for what is considered a healthy balance. 9.3.2 Interest Coverage Ratio Definition and Formula: The Interest Coverage Ratio measures a company’s ability to meet its interest payments. It is calculated as the ratio of earnings before interest and taxes (EBIT) to interest expenses. \\[ \\text{Interest Coverage Ratio} = \\frac{\\text{EBIT}}{\\text{Interest Expenses}} \\] Context and Application: This ratio is an indicator of a company’s short-term financial health. If a company can’t generate enough profits to cover its interest payments, it runs the risk of defaulting on its debts. A higher ratio means that the company has a comfortable buffer to meet its interest payments. Investors and creditors use the interest coverage ratio to determine the risk of lending to or investing in a company. A low ratio can be a warning sign that a company is over-leveraged and facing financial difficulties. 9.3.3 Debt Ratio Definition and Formula: The Debt Ratio measures the proportion of a company’s assets that are financed by debt. It provides an overarching view of the company’s leverage and can indicate the level of financial risk the company is taking on. \\[ \\text{Debt Ratio} = \\frac{\\text{Total Debt}}{\\text{Total Assets}} \\] Context and Application: A higher Debt Ratio suggests that a greater portion of a company’s assets are funded by debt, implying a potentially higher financial risk, especially if the company’s earnings are volatile. Investors and analysts use this ratio to gauge the company’s financial stability and its ability to withstand economic downturns. 9.3.4 Debt-to-Capital Ratio Definition and Formula: The Debt-to-Capital Ratio compares the company’s total debt to its total capital, providing a comprehensive view of the company’s financial leverage by including all types of debt. \\[ \\text{Debt-to-Capital Ratio} = \\frac{\\text{Total Debt}}{\\text{Total Debt} + \\text{Shareholders&#39; Equity}} \\] Context and Application: This ratio is an essential metric for investors and creditors to assess the riskiness of the company’s financial structure. A lower ratio typically signifies a more conservative financial position with less reliance on debt, which can be advantageous in times of financial uncertainty or rising interest rates. 9.3.5 Equity Multiplier Definition and Formula: The Equity Multiplier indicates how much of a company’s total assets are financed by shareholders’ equity. It is a reflection of the degree to which a company is utilizing equity to finance its assets. \\[ \\text{Equity Multiplier} = \\frac{\\text{Total Assets}}{\\text{Shareholders&#39; Equity}} \\] Context and Application: This leverage ratio is derived from the debt-to-equity ratio and provides insight into the company’s leverage strategy. A higher Equity Multiplier suggests a greater reliance on debt to finance assets, which can increase the company’s return on equity but also its financial risk. It is a critical metric for evaluating the aggressiveness of a company’s financial practices. These leverage indicators are widely used by investors, analysts, and creditors to gauge the level of risk associated with a company’s financial structure. They can have significant implications for the cost of borrowing, rating agencies’ assessments, and overall company strategy. Leverage ratios are especially important for evaluating the financial health of companies that are capital-intensive or those that operate in industries with high levels of debt financing. As with all financial metrics, it is crucial to compare these ratios within the context of industry norms and historical performance to accurately interpret their implications. 9.4 Efficiency Indicators Efficiency indicators, or activity ratios, play a critical role in assessing a company’s operational effectiveness. They measure how adeptly a company manages its assets and liabilities to generate sales and maximize profitability. Here’s a closer look at each ratio and its broader implications: 9.4.1 Asset Turnover Ratio Definition and Formula: The Asset Turnover Ratio measures how efficiently a company can use its assets to produce sales. It is a clear indicator of the asset management efficiency of a company. \\[ \\text{Asset Turnover Ratio} = \\frac{\\text{Net Sales}}{\\text{Average Total Assets}} \\] Context and Application: This ratio helps investors understand how well a company is utilizing its assets to generate revenue. A higher ratio indicates more efficient use of assets, while a lower ratio may suggest inefficiencies. The ratio varies significantly across different industries, so it’s essential to compare it to industry averages. 9.4.2 Inventory Turnover Ratio Definition and Formula: This ratio indicates the number of times a company’s inventory is sold and replaced over a certain period, reflecting the company’s efficiency in managing its stock. \\[ \\text{Inventory Turnover Ratio} = \\frac{\\text{Cost of Goods Sold}}{\\text{Average Inventory}} \\] Context and Application: Frequent turnover is often indicative of good inventory management and high demand for a company’s products. Conversely, low turnover may signal overstocking, obsolescence, or deficiencies in the product line or marketing effort. It’s also important to compare this ratio to industry norms to draw meaningful conclusions. 9.4.3 Receivables Turnover Ratio Definition and Formula: This ratio measures how efficiently a company collects on its credit sales. It is a critical indicator of the effectiveness of the company’s credit policies and accounts receivable management. \\[ \\text{Receivables Turnover Ratio} = \\frac{\\text{Net Credit Sales}}{\\text{Average Accounts Receivable}} \\] Context and Application: A high turnover ratio implies that the company efficiently collects its receivables and has a high-quality customer base. A low ratio might indicate that the company has difficulties in collection, potentially leading to cash flow problems. Like other ratios, it should be assessed relative to industry benchmarks. Additional efficiency indicators include: 9.4.4 Payables Turnover Ratio Definition and Formula: This ratio assesses how quickly a company pays off its suppliers by dividing the cost of goods sold by the average accounts payable. \\[ \\text{Payables Turnover Ratio} = \\frac{\\text{Cost of Goods Sold}}{\\text{Average Accounts Payable}} \\] Context and Application: It measures the rate at which a company pays its suppliers. A higher ratio indicates quicker payment to suppliers, which could be a sign of a company’s strong financial position or an effort to capitalize on trade discounts. 9.4.5 Days Sales Outstanding (DSO) Definition and Formula: DSO indicates the average number of days it takes for a company to collect payment after a sale has been made. \\[ \\text{Days Sales Outstanding} = \\left( \\frac{\\text{Average Accounts Receivable}}{\\text{Net Credit Sales}} \\right) \\times \\text{Number of Days} \\] Context and Application: It provides insight into the company’s credit policy effectiveness and cash flow management. A lower DSO is generally more favorable, indicating that the company collects its receivables more quickly. 9.4.6 Fixed Asset Turnover Ratio Definition and Formula: This ratio measures a company’s efficiency in generating net sales from fixed assets such as property, plant, and equipment. \\[ \\text{Fixed Asset Turnover Ratio} = \\frac{\\text{Net Sales}}{\\text{Average Fixed Assets}} \\] Context and Application: It shows how well a company is using its investment in fixed assets to generate sales. A higher ratio suggests that the company is using its fixed assets efficiently. These efficiency indicators provide valuable insights into a company’s operational performance, specifically how well it manages inventory, receivables, and fixed assets to support sales and profitability. They should be analyzed over time for trends and benchmarked against industry standards to gain a comprehensive view of a company’s efficiency. 9.5 Valuation Indicators Valuation indicators, commonly known as valuation ratios, are utilized to discern a company’s financial value as perceived by the market, comparing different aspects of its financial state to the market price of its stock. These ratios are vital for investors and analysts who seek to determine whether a company’s shares are undervalued or overvalued in relation to its earnings, sales, and equity. 9.5.1 Price-to-Book (P/B) Ratio Definition and Formula: The P/B Ratio compares a company’s market capitalization to its book value. It indicates what the market is willing to pay for a unit of book value equity. \\[ \\text{P/B Ratio} = \\frac{\\text{Market Price Per Share}}{\\text{Book Value Per Share}} \\] Context and Application: This ratio is particularly insightful for industries like banking and insurance, where assets and liabilities are clear reflections of their market values. A lower P/B could suggest that the stock is undervalued, while a higher P/B may indicate overvaluation. However, norms vary by industry. 9.5.2 Price-to-Earnings (P/E) Ratio Definition and Formula: The P/E Ratio measures the market’s valuation of a company’s profitability by comparing the price of its shares to its earnings per share (EPS). \\[ \\text{P/E Ratio} = \\frac{\\text{Market Price Per Share}}{\\text{EPS}} \\] Context and Application: This ratio is one of the most widely recognized indicators of stock valuation. A high P/E may suggest that investors expect higher earnings growth in the future compared to companies with a lower P/E. However, it’s important to consider it in the context of the company’s growth rate and the average industry P/E. 9.5.3 Price-to-Sales (P/S) Ratio Definition and Formula: The P/S Ratio provides a comparison between a company’s market capitalization and its total sales, showing how much the market values every dollar of the company’s sales. \\[ \\text{P/S Ratio} = \\frac{\\text{Market Capitalization}}{\\text{Total Sales}} \\] Context and Application: The P/S ratio is often used for companies that do not have earnings, such as startups or those going through a turnaround. It gives a straightforward valuation measure that is less likely to be manipulated than other earnings-based ratios. Investors should be cautious with high P/S ratios, which may indicate overvaluation. Additional valuation indicators include: 9.5.4 Enterprise Value-to-EBITDA (EV/EBITDA) Definition and Formula: This ratio compares the value of a company, inclusive of debt and cash, to its earnings before interest, taxes, depreciation, and amortization. \\[ \\text{EV/EBITDA} = \\frac{\\text{Enterprise Value}}{\\text{EBITDA}} \\] Context and Application: It is used to evaluate the value of a company on an operational basis independent of its capital structure. Lower values can indicate potential undervaluation. 9.5.5 Dividend Yield Definition and Formula: Dividend Yield shows how much a company pays out in dividends each year relative to its stock price. \\[ \\text{Dividend Yield} = \\frac{\\text{Dividends Per Share}}{\\text{Market Price Per Share}} \\] Context and Application: This indicator is significant for income-focused investors. A high dividend yield may be attractive, but it is important to assess the sustainability of the dividend. 9.5.6 Price-to-Cash Flow (P/CF) Ratio Definition and Formula: This ratio compares the company’s market price to its operating cash flow, providing a valuation metric less susceptible to accounting adjustments than the P/E ratio. \\[ \\text{P/CF Ratio} = \\frac{\\text{Market Capitalization}}{\\text{Cash Flow from Operations}} \\] Context and Application: It is useful for evaluating stocks of companies with significant non-cash expenses. A lower P/CF ratio may suggest the stock is undervalued relative to its cash generation abilities. These valuation indicators collectively provide a multi-faceted view of a company’s valuation and are essential tools for investors making informed decisions about buying or selling equity. Each ratio offers different insights, and when combined, they give a more complete picture of a company’s market value and financial health. They should be used in concert with industry comparisons and historical performance trends to yield the most insight. 9.6 Cash Flow Indicators Cash flow indicators are crucial in assessing the financial flexibility of a company. They offer an understanding of the actual cash being generated by the company’s operations, which is available for reinvestment, paying dividends, or reducing debt. Unlike earnings or net income, which are subject to accounting policies and non-cash items, cash flows provide a more tangible measure of a company’s financial health and its ability to generate shareholder value. 9.6.1 Free Cash Flow to Equity (FCFE) Definition and Formula: FCFE measures the cash flow that remains available to the company’s shareholders after all operating expenses, reinvestments, and debt repayments have been accounted for. It reflects the firm’s ability to generate cash that could be distributed to shareholders. The detailed explanation of FCFE is provided in Chapter 8.6.6, where the cash flow statement is discussed. Here is the formula: \\[ \\begin{aligned} \\text{FCFE} \\ = &amp;\\ \\text{EAT}+\\text{D\\&amp;A}-\\text{WC} - \\text{CapEx} + \\text{Net Borrowing} \\end{aligned} \\] Context and Application: Investors and analysts use FCFE to evaluate the amount of cash a firm can pay as dividends or for stock buybacks. A higher FCFE indicates the firm has healthier cash flow, which is a positive signal for investors. It is a key input in Discounted Cash Flow (DCF) valuation methods to price firms. 9.6.2 Free Cash Flow to the Firm (FCFF) Definition and Formula: FCFF (a.k.a. FCF) represents the cash available to all funding providers, both equity and debt holders. It is a measure of a company’s profitability after all taxes and financial costs. The detailed explanation of FCFF can be found in Chapter 8.6.6 related to the cash flow statement. Here is the formula: \\[ \\begin{aligned} \\text{FCFF} \\ = &amp;\\ \\text{EAT}+\\text{D\\&amp;A}-\\text{WC} - \\text{CapEx} \\\\ &amp;\\ + \\text{Interest Expense} - \\text{Tax Shield on Interest Expense} \\end{aligned} \\] Context and Application: FCFF is used to evaluate the potential dividends to shareholders, potential for debt repayment, and the ability of the firm to fund new investment without additional external financing. 9.6.3 Operating Cash Flow (OCF) Definition and Formula: OCF focuses on the cash flow from core business operations, ignoring financing and investing activities. OCF is explored extensively in Chapter 8.6.1 on operating activities within the cash flow statement framework. Here is the formula: \\[ \\begin{aligned} \\text{OCF} \\ = &amp;\\ \\text{EAT} + \\text{D\\&amp;A} + \\text{WC} \\end{aligned} \\] Context and Application: This indicator is considered a purer measure of a company’s ongoing cash-generating ability because it concentrates on business operations. 9.6.4 FCF Margin Definition and Formula: The FCF Margin measures how much free cash flow the company generates relative to its total revenue. It is a profitability ratio that provides insight into the percentage of revenue converted into free cash flow. \\[ \\text{FCF Margin} = \\frac{\\text{Free Cash Flow}}{\\text{Total Revenue}} \\] Context and Application: A higher FCF Margin indicates that a company is able to sustain and grow its operations, as well as return cash to shareholders. It is a sign of a company’s operational efficiency and financial performance. 9.6.5 Cash Conversion Cycle (CCC) Definition and Formula: The CCC measures the time, in days, it takes for a company to convert resource inputs into cash flows. It is used to assess the efficiency of a company’s sales, inventory restocking, and payment collection processes. \\[ \\begin{aligned} \\text{CCC} \\ = &amp;\\ \\text{Days Inventory Outstanding} \\\\ &amp; \\ + \\text{Days Sales Outstanding} \\\\ &amp;\\ - \\text{Days Payable Outstanding} \\end{aligned} \\] Context and Application: The CCC helps in understanding how well a company is managing its working capital. A lower CCC indicates a company is quickly turning its investments into cash. Understanding cash flow indicators is essential for gauging the true financial viability of a company. They reflect the actual cash being used and generated by the business, providing stakeholders with a clear picture of financial health and the ability to sustain operations, pay debts, and return value to shareholders. Investors and creditors often scrutinize these measures to make informed decisions about investing in or lending to a company. 9.7 Temporal Dimensions Financial indicators are analyzed over different periods to suit the specific needs of the analysis, with each timeframe offering distinct insights. The selection of an appropriate timeframe is critical and must align with the analytical objectives, which may range from understanding immediate operational capabilities to evaluating long-term strategic positioning. It is also important to distinguish between flow variables, which are measured over a period of time, and stock variables, which are measured at a point in time. 9.7.1 Trailing Twelve Months (TTM) Definition and Context: TTM refers to the most recent 12-month period for which data is available and is typically used for flow variables such as revenue, earnings, or cash flow. It offers a snapshot of a company’s recent operational performance without waiting for the end of the fiscal year. Application: TTM is particularly useful for dynamic industries where trends change rapidly, providing a more current perspective than annual reports. It is also beneficial for interim analysis when the latest full-year data is not yet available. 9.7.2 Annual Reporting Definition and Context: Annual indicators provide a full fiscal year’s data, delivering a comprehensive overview of performance. This includes both flow variables, such as annual revenue or expenses, and stock variables, such as year-end assets or liabilities. Application: The annual timeframe is essential for strategic planning, trend analysis over multiple years, and comparisons with annual industry benchmarks. It is a standard reporting period for statutory financial statements. 9.7.3 Quarterly and Monthly Reporting Definition and Context: Shorter time spans like quarterly and monthly reports are suited for monitoring flow variables that are likely to show significant short-term fluctuations due to operational activities or external factors such as seasonality. Application: These timeframes are valuable for tactical management, as they allow for prompt adjustments in response to market dynamics. They also serve to detect and address performance issues before they escalate, and they are crucial for stakeholders requiring the most current data for decision-making. 9.7.4 Example and Explanation For example, an investor looking at the quarterly revenue growth (a flow variable) will use TTM data to get a sense of the most recent year’s performance without waiting for the fiscal year-end. Conversely, when considering a stock variable like the debt-to-equity ratio, they might look at the annual report to see the company’s financial structure at the fiscal year-end, as it represents a snapshot in time and does not require aggregation over a period. TTM makes the most sense for flow variables, as these are accumulated over time and can change significantly within a year. Stock variables, on the other hand, are reported at a specific date, and while they may fluctuate, the TTM concept does not apply to them since they are not about the accumulation over a period but rather a specific moment’s valuation. Choosing the correct timeframe for financial analysis is a nuanced decision that impacts the interpretation and usability of the financial data. The nature of the variable (flow vs. stock) and the industry context should guide whether a trailing, annual, or shorter period is most relevant for the indicator in question. 9.8 Conclusion As we reach the conclusion of our exploration of financial performance indicators, we recognize their pivotal role in distilling complex financial data into accessible and comparable metrics. These indicators, rooted in the core financial statements, provide a multi-dimensional view of a company’s operational success, financial stability, and long-term viability. We have examined how profitability ratios can illuminate a company’s earnings relative to its revenues, assets, or shareholders’ equity. Liquidity ratios have offered us a lens to assess a firm’s ability to meet its short-term obligations, while leverage ratios have helped us gauge the extent and implications of a company’s debt. Efficiency ratios have revealed the effectiveness with which a company employs its assets, and valuation ratios have provided us with measures to infer market perceptions and investment potential. Finally, cash flow ratios check a company’s ability to generate cash that is available for expansion, debt repayment, and return to shareholders. "],["macroeconomic-accounting.html", "Chapter 10 Macroeconomic Accounting 10.1 Important Terms and Definitions 10.2 National Accounts 10.3 Balance of Payments 10.4 Labor Market Accounts 10.5 Fiscal Accounts 10.6 Monetary and Financial Accounts 10.7 Price Level and Inflation", " Chapter 10 Macroeconomic Accounting This chapter provides an overview of key economic indicators used to assess the performance and stability of an economy. The indicators are categorized under six major macroeconomic accounting frameworks: National Accounts: Provides an overview of economic activity, including measures like Gross Domestic Product (GDP), income distribution, and capital formation. Balance of Payments (BoP): Captures an economy’s transactions with the rest of the world, including trade and financial flows. Labor Market Accounts: Details metrics related to employment, wages, and labor force participation. Fiscal Accounts: Reviews government revenues, expenditures, and debt levels to assess fiscal sustainability. Monetary and Financial Accounts: Focuses on monetary aggregates like M1 and M2, and includes other financial statistics that detail the flow of funds between various sectors. Price Level and Inflation: Looks at how prices are changing over time and its implications on purchasing power. Each section of this chapter will explain the corresponding accounting framework, covering how it works and what kind of data it produces. 10.1 Important Terms and Definitions Before delving into the specifics of each macroeconomic accounting framework, understanding key terminology is essential. Below are important terms and their definitions: Stock vs. Flow: Stock Variable: Represents a measure of economic value at a specific point in time, such as the amount of money in a bank account or the level of debt outstanding. Flow Variable: Describes economic activity over a period, like income, spending, or investment, and is usually measured per unit of time (e.g., per year or per quarter). Gross vs. Net: Gross: This term refers to the total amount before any deductions, such as taxes or expenses, are made. For example, gross income is the total income earned by an entity before any deductions. Net: Contrary to gross, “net” means the amount remaining after all deductions have been made. For instance, net income is the income left over after all costs, taxes, and other expenses have been subtracted from the gross income. Assets vs. Liabilities: Asset: Any item of economic value owned by an entity, with the expectation that it will generate future benefits, often convertible to cash. Liability: A financial obligation or debt owed by an entity, typically requiring future payments of money. Equity/Net Worth: The value of all assets minus all liabilities for a given economic entity. Transaction: An economic event that causes a change in assets, liabilities, or net worth. Funds: Pools of money set aside for specific purposes or activities. These can be collected, allocated, and managed by individuals, corporations, governments, or financial institutions. For example, mutual funds aggregate money from multiple investors to invest in a diversified portfolio of assets. Reserve: Funds or assets set aside for future obligations or contingencies. Credit vs. Debit: Credit: An accounting entry that typically increases liabilities or equity. In macroeconomic accounts, it denotes inflows or increments to balances. Debit: An accounting entry that generally increases assets. In macroeconomic accounts, it signifies outflows or decrements to balances. Revenues vs. Expenditures: Revenues: The income generated by an entity from its primary operations, such as the sale of goods and services. In the context of a government, revenue would include income from taxation, licenses, and other fees. Expenditures: The spending or outflows incurred by an entity to maintain its operations. For governments, expenditures include public services, infrastructure development, and social programs. Surplus vs. Deficit: Surplus: This term refers to the situation where revenue exceeds expenditures. In macroeconomic accounting, a surplus indicates a positive balance after all transactions have been accounted for. Deficit: The opposite of a surplus, a deficit occurs when expenditures exceed revenues. In macroeconomic terms, a deficit signifies a negative balance after accounting for all transactions. Account: In this context, an account refers to a structured record of economic transactions or financial positions between different sectors of an economy. For example, the Current Account captures transactions related to trade, services, and transfers between a country and the rest of the world. Balance: In this context, balance refers to the state of equilibrium in an account where total credits equal total debits. For instance, a trade balance is in surplus when the value of exports exceeds the value of imports, and in deficit when the opposite occurs. The balance ensures that the accounting equation remains intact. Accounting: A systematic method for recording, summarizing, and analyzing financial transactions and positions. A core accounting principle is double-entry accounting, which ensures that every transaction affects two accounts in a way that the accounting equation remains balanced: \\[ \\text{Assets} = \\text{Liabilities} + \\text{Equity} \\] For example, if a business takes out a loan, its cash account (an asset) would increase, and its loans payable account (a liability) would also increase, thereby maintaining the balance in the accounting equation. Accounting often distinguishes between two main types of financial statements: Balance Sheet: A snapshot of an entity’s assets, liabilities, and equity as of a specific date, reflecting its financial position or “stock” at that moment. Income Statement: A report that captures an entity’s revenues, expenses, and profits or losses over a specific time period, representing the “flow” of financial activities. Nominal vs. Real: Nominal: Refers to values that have not been adjusted for inflation. Nominal figures represent the face value of money or assets at a given point in time. Real: Contrary to nominal, real values are adjusted for inflation or deflation. Real figures provide a more accurate representation of an item’s value, taking into account changes in the purchasing power over time. Aggregates: These are summary measures that combine multiple individual data points to represent a larger whole. For example, GDP is an aggregate measure of economic output. Index: A statistical measure that captures relative changes in a variable or a group of variables over time or across categories. It usually starts with a base value, often set at 100, from which subsequent values show relative changes. An index lacks units and is meaningful only when comparing relative changes. For instance, if the Consumer Price Index (CPI) rises from 100 to 110, it indicates a 10% increase in the general price level compared to the base year. The value 110 alone, however, lacks interpretive meaning. Sector/Agent: In macroeconomic accounting, “sectors” usually refer to the major economic agents or entities. Common sectors include: Households: Represents individual consumers who save, borrow, invest, and spend. Corporations: Includes both financial and non-financial firms that raise capital, invest, and produce goods and services. Government: Encompasses local, state, and federal government entities that tax, spend, and issue debt. Financial Institutions: Such as banks, insurance companies, and pension funds that facilitate the flow of funds in the economy. Rest of the World: Captures transactions with entities outside of the domestic economy, such as foreign governments and international organizations. Sector/Industry: The term “sector” can also refer to specific industries within an economy, such as manufacturing, services, or agriculture. This should not be confused with the broad categories of economic agents mentioned above. For example, within the “Corporations” sector, one could further delineate sectors like the “Manufacturing Sector” or the “Financial Services Sector.” Understanding these terms will provide a foundational vocabulary for interpreting and analyzing the various macroeconomic accounting frameworks discussed in this chapter. 10.2 National Accounts National Accounts are a systematic framework that provides a detailed record of economic activities within a country over a specific period. One of its core elements is the Gross Domestic Product (GDP). GDP can be calculated using three different approaches, as detailed in the following sections. 10.2.1 Production Approach The production approach, also known as the output approach or value-added approach, is a method for calculating a country’s total output of goods and services over a specific period. It adjusts each sector’s gross output by subtracting the cost of intermediate goods and services consumed in production. The formula for the production approach is: \\[ \\begin{aligned} \\text{GDP}\\ =&amp;\\ \\text{Gross Value Added (GVA)} \\\\ &amp; +\\text{Taxes on Products}-\\text{Subsidies on Products} \\end{aligned} \\] where \\[ \\begin{aligned} \\text{GVA}\\ = &amp;\\ \\sum_{i} \\text{Value Added in Sector } i \\\\ =&amp;\\ \\sum_{i} \\Big( \\text{Value of Goods and Services Produced in Sector } i \\\\ &amp; \\hspace{1.1cm} - \\text{Value of Intermediate Goods and Services Used in Sector } i \\Big) \\end{aligned} \\] Here, the summation is over all sectors \\(i\\) of an economy, and “Value” refers to the after-tax revenue generated by the goods and services. Since taxes and subsidies on products are considered transfers and do not reflect the actual production output of a sector, taxes that were previously subtracted are added back, and subsidies that were previously added are subtracted again when transitioning from GVA to GDP. This value-added approach ensures that we only count final goods and services, preventing double-counting of intermediate goods and services that are used in the production of other goods and services. This gives us the total output of the economy as the sum of value added by all sectors. 10.2.2 Expenditure Approach The expenditure approach calculates the total amount spent by all entities within the country. The GDP is represented as \\(Y\\) in the following equation: \\[ \\begin{aligned} GDP = &amp;\\ \\text{Total Expenditures} \\\\ Y = &amp;\\ C + I + G + (X - M) \\end{aligned} \\] Here’s a breakdown of the four main components: Consumption (\\(C\\)): Measures the total value of all goods and services consumed by households. This includes expenditures on durables, non-durables, and services. For example, buying a car or paying for health services would fall under this category. Investment (\\(I\\)): Represents (private) spending on capital goods that will be used for future production. This includes business investments in equipment and structures, residential construction, and changes in business inventories. For instance, if a company buys machinery, that’s considered an investment. Government Spending (\\(G\\)): Consists of all government expenditures on goods and services. It excludes transfer payments like pensions and unemployment benefits, as these are not payments for goods or services. For example, spending on public goods like roads or schools falls under this category. Net Exports (\\(X - M\\)): Calculated as exports (\\(X\\)) minus imports (\\(I\\)). If a country exports more than it imports, it has a trade surplus; if it imports more than it exports, it has a trade deficit. For instance, if a country exports software services and imports oil, net exports would be the value of the exported services minus the value of the imported oil. 10.2.3 Income Approach The income approach sums up all income earned in a country during a particular period. \\[ \\begin{aligned} GDP = \\text{Aggregate Income} =&amp;\\ \\text{Compensation of Employees} \\\\ &amp; + \\text{Gross Operating Surplus} \\\\ &amp; + \\text{Gross Mixed Income} \\\\ &amp; + \\text{Taxes on Production and Imports} \\\\ &amp; - \\text{Subsidies} \\end{aligned} \\] Compensation of Employees: Total payment to labor, including wages and benefits. Gross Operating Surplus: Profits earned by businesses. Gross Mixed Income: Income from self-employment. Taxes on Production and Imports: Taxes like sales tax and import duties. Subsidies: Government subsidies, like agricultural subsidies. Given that the payments for final goods and services eventually revert to the factors of production as income, aggregate income equals aggregate output: \\[ \\text{Aggregate Income} = \\text{Aggregate Output} \\] These approaches - production, expenditure, and income - aim to capture the same economic output from different perspectives. They provide a multi-dimensional view of the economy, essential for macroeconomic analysis and policy-making. 10.2.4 Other Flow Measures The National Accounts system also includes other important indicators: Gross National Product (GNP): Captures the total value of goods and services produced by a country’s residents, regardless of where they are located. This includes GDP along with net income from abroad, such as foreign investments and remittances. The formula is: \\[ \\text{GNP} \\ = \\ \\text{GDP} + \\text{Net Income from Abroad} \\] Net Domestic Product (NDP), Net National Product (NNP), and Net Value Added (NVA): GDP, GNP, and GVA adjusted for depreciation on the country’s capital assets like machinery and infrastructure. It represents the net addition to the national wealth. The formula is: \\[ \\begin{aligned} \\text{NDP} \\ = &amp; \\ \\text{GDP} - \\text{Depreciation} \\\\ \\text{NNP} \\ = &amp; \\ \\text{GNP} - \\text{Depreciation} \\\\ \\text{NVA} \\ = &amp; \\ \\text{GVA} - \\text{Depreciation} \\end{aligned} \\] National Income: This measure captures the total income earned by residents of a nation, both individuals and businesses. It encompasses various sources of income, including wages, rents, and profits, adjusted for depreciation and indirect taxes. Indirect taxes are taxes on goods and services like sales tax, VAT, and excise taxes, which are collected by an intermediary (such as a retailer) from the person who bears the ultimate economic burden of the tax (such as the consumer). National income can be derived from Net National Product (NNP) by subtracting indirect taxes and adding subsidies. The formula is: \\[ \\begin{aligned} \\text{National Income} \\ = &amp; \\ \\text{NNP} - \\text{Indirect Taxes} + \\text{Subsidies} \\end{aligned} \\] Personal Income: This is a narrower concept than national income that represents the income received by the households and non-corporate businesses in a nation. It typically includes wages, interest, rent, and profits, but not all elements of national income flow to households. Mathematically, it can be expressed as: \\[ \\begin{aligned} \\text{Personal Income} \\ = &amp;\\ \\text{Compensation of Employees} \\\\ &amp; + \\text{Net Income from Property} \\\\ &amp; + \\text{Transfer Payments} \\end{aligned} \\] Disposable Income: Refers to the income available to households after taxation and transfer payments. It’s calculated as: \\[ \\begin{aligned} \\text{Disposable Income} \\ =&amp;\\ \\text{Personal Income} - \\text{Personal Taxes} \\end{aligned} \\] Savings: The part of disposable income not consumed. Savings can be either private or public. The formula is: \\[ \\text{Savings} \\ = \\ \\text{Disposable Income} - \\text{Consumption} \\] 10.2.5 Stock Measures National Accounts also include balance sheets for sectors like households, corporations, and the government, depicting their assets, liabilities, and net worth at a point in time. These balance sheets complement the Flow Variables like GDP, which capture economic activity over specific periods such as quarters or years. In contrast, Stock Variables like the balance sheets provide a snapshot of assets, liabilities, and net worth at a particular point in time. National Accounts sometimes include “Other Changes in Assets Account” to bridge the gap between flow and stock measures. This account reconciles the flow variables with the new stock positions, factoring in valuation adjustments of the stock variables and other changes not captured in the flow data. Here are some of the key stock measures in National Accounts: Net Worth of Households: Represents the value of all assets owned by households minus their liabilities. This measure provides insight into the financial health and consumer spending capacity of an economy. Corporate Balance Sheets: These display the assets, liabilities, and equity of corporations, offering an indication of the business sector’s financial stability and investment capacity. Government Debt: Captures the total amount of money the government owes to external and internal creditors. It serves as an important stock measure for assessing fiscal sustainability. Physical Capital Stock: Represents the cumulative value of all capital goods like machinery, buildings, and infrastructure. This is crucial for understanding an economy’s productive capacity. Natural Resource Stocks: The estimated value of natural resources like oil, minerals, and forests. This measure helps gauge an economy’s resource wealth and its sustainability over the long term. Financial Assets and Liabilities by Sector: These provide a detailed look at the kinds of financial instruments held or owed by different sectors, further enriching our understanding of economic conditions. 10.2.6 Implicit Price Deflators Implicit Price Deflators are tools that help us compare the value of economic activities over time by removing the influence of price changes, like inflation or deflation. To understand how they work, it’s important to know two terms: Nominal Values: These terms refer to economic variables that are measured using the prices that are current at the time the economic activity occurs, without any adjustment for price changes over time. For example, the money you pay for a coffee today is its nominal value. Real Values: These terms are used to describe economic variables that have been adjusted for price changes, making them comparable over different time periods. They are calculated by taking the economic activity from another time and asking, “What would this be worth if prices hadn’t changed?” For example, what would the coffee you bought today cost if we use the prices from 2010? Now, let’s get into some types of Implicit Price Deflators: GDP Deflator The GDP deflator is employed to convert GDP from current values to constant values, allowing for a more meaningful comparison of economic performance across time periods. It helps us see how much the economy has grown not because things are more expensive but because more goods and services have been produced. The formula to find it is: \\[ \\text{GDP Price Deflator} = \\left( \\frac{\\text{Nominal GDP}}{\\text{Real GDP}} \\right) \\times 100 \\] where “Nominal GDP” is the value of all goods and services produced within a country during a specific period, measured at the prices existing when the economic activity took place. “Real GDP,” on the other hand, is the same output but evaluated at the constant prices from a specific base year, such as 2010. The GDP deflator serves as an indicator of how much general price levels have increased over time and is commonly used as an indicator of the aggregate price level. Other Price Deflators in National Accounts Besides the GDP deflator, National Accounts produce other deflators that offer nuanced views of price changes in different economic sectors: GNP Deflator: Similar to the GDP Deflator but applied to Gross National Product. \\[ \\text{GNP Deflator} = \\left( \\frac{\\text{Nominal GNP}}{\\text{Real GNP}} \\right) \\times 100 \\] Investment Deflator: Measures changes in the overall price level for capital investments like machinery, buildings, and infrastructure. Consumption Deflator: Focuses on the price level of all goods and services consumed and is used to deflate nominal consumer spending to obtain real consumption values. Government Spending Deflator: Measures price changes for all goods and services purchased by the government. Export and Import Deflators: These are used to adjust the value of exports and imports for changes in prices, helping in real terms comparisons. Sector-Specific Deflators: These can be used to deflate the gross output or value added for specific sectors of the economy, providing a measure of real output for those sectors. These deflators serve various analytical needs, from adjusting nominal figures to real terms for policy analysis to assessing inflationary pressures in specific sectors. 10.2.7 Data Collection In the United States, the term National Income and Product Accounts (NIPA) refers to the country’s National Accounts. These accounts are compiled by the Bureau of Economic Analysis (BEA), a federal agency under the Department of Commerce. For more information, visit their website at www.bea.gov. Data related to national accounts can be accessed at www.bea.gov/data/economic-accounts, which also provides National Accounts measures segmented by region (e.g., state or metropolitan area) and by industry. Internationally, global organizations like the United Nations provides guidelines for these accounts through the System of National Accounts (SNA) (2008). These organizations also consolidate National Accounts data from multiple countries for cross-country comparisons and global analyses. Notable agencies include: World Bank: Offers a comprehensive database encompassing National Accounts and other socio-economic indicators. Visit the World Bank data site at data.worldbank.org. International Monetary Fund (IMF): Through its World Economic Outlook database, the IMF compiles National Accounts data. Visit the IMF data site at www.imf.org/en/Data. Organisation for Economic Co-operation and Development (OECD): Collects National Accounts data for member countries and other large economies. Visit the OECD data site at data.oecd.org. United Nations Statistics Division: Provides a focus on developmental aspects of National Accounts. Visit the UN Statistics Division site at unstats.un.org. Eurostat: For European Union countries, National Accounts data is compiled in accordance with the European System of Accounts. Visit the Eurostat website at ec.europa.eu/eurostat. These agencies standardize data to ensure comparability, often adhering to the SNA framework. 10.2.8 Resources For introductory insights into National Accounts data collection, the resource by the Bureau of Economic Analysis (BEA, 2015) is useful: “Measuring the Economy: A Primer on GDP and the National Income and Product Accounts.” For a deeper understanding, refer to BEA (2022): “Concepts and Methods of the U.S. National Income and Product Accounts.” 10.3 Balance of Payments Balance of Payments (BoP) is an accounting framework that captures all economic transactions between a country’s residents and the rest of the world. Think of it like a detailed bank statement for a country. This is crucial for understanding how a country is doing economically on the global stage. Conceptually, the BoP is an equation that must balance: \\[ \\begin{aligned} 0 = \\ &amp; \\text{Current Account Balance} \\\\ &amp; + \\text{Capital Account Balance} \\\\ &amp; + \\text{Financial Account Balance} \\\\ &amp; + \\text{Errors and Omissions} \\end{aligned} \\] The three main components are detailed in the following sections. 10.3.1 Current Account Current Account focuses on daily transactions primarily involving the exchange of goods, services, income, and current transfers. The Current Account Balance represents the net result of all these transactions. It is calculated as the sum of the credits (inflows) minus the debits (outflows) in the Current Account. A positive Current Account Balance indicates that the country has more inflows than outflows of financial credits, commonly referred to as a “surplus.” A negative Current Account Balance suggests the opposite, termed a “deficit.” Its formula is: \\[ \\begin{aligned} \\text{Current Account Balance} \\ =&amp; \\ \\text{Net Exports of Goods and Services} \\\\ &amp; + \\text{Net Income from Abroad} \\\\ &amp;+ \\text{Net Current Transfers} \\end{aligned} \\] Net Exports of Goods and Services: This represents the value of a country’s exported goods and services minus its imported goods and services. When exports exceed imports, this value is positive; conversely, when imports exceed exports, it’s negative. For example, if a country exports cars and imports oil, the net exports would be the value of the exported cars minus the value of the imported oil. The term Trade Balance, by contrast, refers solely to the net exports of goods, excluding services. Net Income from Abroad: This includes income from foreign investments minus payments made to foreign investors. For example, if a U.S. company owns a factory in another country, the profits from that factory would count as income from abroad. Conversely, if a foreign company owns a factory in the U.S., the profits sent back to the foreign country would be deducted from the U.S.’s net income from abroad. Net Current Transfers: This captures all unilateral transfers of money, goods, or services without a direct exchange in return. Examples include remittances from overseas workers, social security payments, and foreign aid. The “net” implies it’s the amount received minus the amount given. For instance, if a country receives foreign aid but also has citizens sending remittances to relatives in other countries, net current transfers would include both. 10.3.2 Capital Account Capital Account captures transactions that change the country’s stock of fixed and non-financial assets. The Capital Account Balance reflects the net effect of these transactions. A positive balance would indicate a net gain in capital transfers or the sale of non-financial assets, while a negative balance would indicate a net loss. The formula is: \\[ \\begin{aligned} \\text{Capital Account Balance} \\ = &amp; \\ \\text{Net Capital Transfers} \\\\ &amp; +\\ \\text{Net Transactions in Non-produced} \\\\ &amp; \\ \\ \\quad \\text{and Non-financial Assets} \\end{aligned} \\] Capital Transfers: These are transactions where ownership of an asset (other than cash or financial items) is transferred from one country to another, or where a liability is forgiven. For example, if the U.S. receives debt forgiveness from another country, that would be recorded here as a credit. Transactions in Non-produced, Non-financial Assets: These include sales and purchases of rights, like patents and copyrights, as well as sales of tangible assets like land. For instance, if a country sells mining rights to another country, it would be recorded here. 10.3.3 Financial Account Financial Account records transactions involving financial assets and liabilities between the country and the world. The Financial Account Balance measures the net result of these financial transactions. A positive balance suggests that the country has attracted more capital than it has sent abroad. However, it might also indicate that the country is accumulating liabilities to the rest of the world to finance a current account deficit, posing potential long-term risks. The formula is: \\[ \\begin{aligned} \\text{Financial Account Balance} \\ = &amp;\\ \\text{Net Direct Investment} \\\\ &amp; + \\text{Net Portfolio Investment} \\\\ &amp; + \\text{Net Financial Derivatives} \\\\ &amp; + \\text{Net Other Investment} \\\\ &amp; + \\text{Net Reserve Assets} \\end{aligned} \\] Direct Investment: This is investment made to acquire a lasting interest in an enterprise operating in another country. If an American company opens a factory in Germany, that would be categorized as a direct investment. Portfolio Investment: This refers to transactions involving financial instruments like stocks and bonds. For example, if a British investor buys stocks in an American company, that would be considered a portfolio investment. Financial Derivatives: These are financial contracts whose value is linked to the price of an underlying commodity or asset. For example, futures and options contracts would fall under this category. Other Investment: This is a catch-all category that includes other financial transactions not covered in the above categories, such as loans, currency deposits, and trade credits. Reserve Assets: These are foreign financial assets that can be readily accessed by the country’s monetary authorities. They usually include foreign currencies, gold, and Special Drawing Rights (SDRs). 10.3.4 Stock Measures The Balance of Payments (BoP) focuses on flow measures, which record economic transactions over set periods like quarters or years. In contrast to these flows, stock measures give a snapshot of economic variables at a specific point in time. The BoP does not capture changes in these stock measures arising from valuation changes, such as those caused by asset price fluctuations or exchange rate movements. One key stock measure is the Net International Investment Position (NIIP), which represents the value of a country’s external assets minus its external liabilities at a specific point in time. Since the BoP does not account for valuation changes that impact the NIIP, some detailed BoP statements include a reconciliation account. This account bridges the gap between the flows captured in the BoP and the new stock positions, accounting for valuation adjustments and other factors. Thus, for a comprehensive view of a country’s economic relations with the rest of the world, it’s essential to consider both BoP flow measures and additional stock measures like the NIIP. Here are some of the key stock measures of BoP: Net International Investment Position (NIIP): The net value of a country’s external assets minus external liabilities, providing a snapshot of a nation’s financial position relative to the rest of the world. It is the cumulative total of current account balances and valuation changes over time. External Debt: This stock measure is part of the external liabilities included in the NIIP. It provides a snapshot of the debt obligations to foreign creditors. Unlike the broader category of external liabilities, external debt excludes equity investments such as FDI. It is the cumulative sum of gross borrowing flows, captured under “Other Investment” in the financial account, plus valuation changes. Foreign Direct Investment (FDI): This is another component of the NIIP, specifically representing the equity investment portion. FDI can be categorized as either an asset or a liability. When a domestic entity invests in a foreign country, it is considered an outward FDI and contributes to external assets. Conversely, when a foreign entity invests in the domestic economy, it is termed inward FDI and constitutes an external liability. The net FDI position is calculated as outward FDI (assets) minus inward FDI (liabilities). It is the cumulative total of net direct investments recorded in the financial account plus valuation changes. Portfolio Investment: This is another part of the NIIP, representing the value of stock and bond investments made by foreign investors in the domestic country and vice versa. Portfolio investment is considered more liquid but less controlling investments compared to FDI. It is the cumulative total of net portfolio investment flows recorded in the financial account, plus valuation changes. Foreign Exchange Reserves: The total stock of foreign currencies held by a country’s central bank. They contribute to the NIIP by increasing the value of a country’s external assets. Managed by the central bank, these reserves can be used for various policy objectives, including stabilizing the domestic currency or facilitating trade. It is the cumulative sum of net reserve assets recorded in the financial account, plus valuation changes like currency revaluations. Net Equity: This is the value of net ownership in entities such as mutual funds, pension funds, and so forth. It is also a part of the NIIP under external assets or liabilities, depending on the balance. It is the cumulative total of net equity investment flows other than FDI recorded in the financial account, plus valuation changes affecting mutual funds, pension funds, and other similar financial entities. These stock measures are critical for a more comprehensive understanding of a country’s economic health and its relationships with other nations. They complement the flow measures found in the BoP, providing a fuller picture of a country’s economic standing. 10.3.5 Data Collection The Balance of Payments Manual, maintained by the International Monetary Fund (2008), serves as the global standard for compiling and presenting BoP statistics. This manual aims to achieve international comparability in BoP statistics. In the United States, the Bureau of Economic Analysis (BEA) oversees the compilation of the Balance of Payments (BoP). Operating under the Department of Commerce, the BEA provides extensive data on its website at www.bea.gov. The BoP data specifically can be accessed at www.bea.gov/data/economic-accounts/international. For international data collection and standardization of Balance of Payments, several organizations are notable: International Monetary Fund (IMF): Maintains the Balance of Payments Statistics (BOPS) database. Visit the IMF data site at www.imf.org/en/Data. World Bank: Provides a range of external debt and BoP statistics. Visit the World Bank data site at data.worldbank.org. Organisation for Economic Co-operation and Development (OECD): Collects and publishes BoP statistics for its member countries. Visit the OECD data site at data.oecd.org. United Nations Conference on Trade and Development (UNCTAD): Provides data concerning trade and BoP. Visit UNCTAD’s data site at unctadstat.unctad.org. Bank for International Settlements (BIS): Offers data on international financial statistics, including aspects of the BoP. Visit the BIS data site at www.bis.org/statistics/index.htm. These agencies work to standardize BoP data to facilitate international comparisons, often in line with the Balance of Payments Manual. 10.3.6 Resources For those interested in a deeper understanding of Balance of Payments, the International Monetary Fund’s (IMF, 2008) “Balance of Payments and International Investment Position Manual” is a valuable resource. 10.4 Labor Market Accounts Labor Market Accounts systematically organize and report labor-related activities in an economy, focusing on labor supply and demand, employment, and unemployment. The main components that form Labor Market Accounts are explored in the following sections. 10.4.1 Labor Force The Labor Force comprises individuals who are either employed or actively seeking employment. It can be formally represented as: \\[ \\text{Labor Force} = \\text{Employed} + \\text{Unemployed} \\] In the United States, the term “Labor Force” specifically refers to individuals who are 16 years of age and older, who are not institutionalized (e.g., in prison or mental facilities), and who are either employed or actively seeking employment. The labor force participation rate shows the percentage of the working-age population either employed or actively seeking employment. It is calculated as: \\[ \\text{Labor Force Participation Rate} = \\frac{\\text{Labor Force}}{\\text{Working-Age Population}} \\times 100 \\] 10.4.2 Employment, Unemployment, and Hours Worked Employment accounts for individuals who are currently working for pay or profit. Unemployment pertains to individuals who are not currently employed but are actively looking for work. This includes full-time, part-time, and temporary jobs. The employment rate and unemployment rate quantifies the proportion of employed and unemployed individuals relative to the labor force. \\[ \\begin{aligned} \\text{Employment Rate} = &amp;\\ \\frac{\\text{Employed}}{\\text{Labor Force}} \\times 100 \\\\ \\text{Unemployment Rate} =&amp;\\ \\frac{\\text{Unemployed}}{\\text{Labor Force}} \\times 100 \\end{aligned} \\] Hours worked measures the total number of hours worked by all employed individuals in the economy during a specific time period. It can be represented as: \\[ \\text{Total Hours Worked} = \\text{Average Weekly Hours} \\times \\text{Employed} \\times \\text{Number of Weeks} \\] 10.4.3 Productivity Measures Productivity metrics in Labor Market Accounts offer insights into the efficiency and effectiveness of labor in producing economic output. They are essential for assessing the performance of an economy over time or compared to others. The most basic measure of productivity in Labor Market Accounts is output per hour, which gauges the amount of output produced per unit of labor, typically hours worked. This is especially important for understanding how efficiently labor is being utilized in an economy. The formula to calculate labor productivity is: \\[ \\begin{aligned} \\text{Labor Productivity} \\ = &amp; \\ \\text{Output per Hour} \\ = \\frac{\\text{Gross Domestic Product (GDP)}}{\\text{Total Hours Worked}} \\end{aligned} \\] Labor productivity can be further broken down into sectoral labor productivity, which measures labor productivity within specific industries or sectors. This allows for a more detailed analysis of labor efficiency across various parts of the economy. While not as straightforward to calculate directly from Labor Market Accounts, multi-factor productivity (MFP) is sometimes considered for a more comprehensive view. MFP takes into account that labor is not the only factor input, but there are other factors like capital, energy, technology, or human capital. However, deriving MFP often requires combining Labor Market Accounts with other data sources. The formula for MFP generally takes the form: \\[ \\begin{aligned} \\text{Output} \\ = &amp;\\ \\text{MFP} (a \\times \\text{Labor} + b \\times \\text{Capital} + c\\times \\text{Energy} + \\ldots) \\\\ \\Downarrow \\\\ \\text{MFP} \\ = &amp;\\ \\frac{\\text{Output}}{a\\times \\text{Labor} + b \\times \\text{Capital} + c \\times \\text{Energy} + \\ldots} \\end{aligned} \\] Here: Output: Usually measured as Gross Domestic Product (GDP) or Gross Value Added (GVA) in the context of a nation or a specific sector. Labor: Typically represented by total hours worked or the total number of employees. Capital: Often measured as the capital stock, which can include machinery, buildings, and other forms of capital. Energy: Measured in appropriate units like joules or BTUs. \\(a,b,c,\\ldots\\): Weights for labor, capital, and energy, respectively, where \\(a+b+c+\\ldots=1\\). To compute labor productivity using the MFP formula, you isolate the labor input’s contribution to output: \\[ \\text{Labor&#39;s Contribution to Output} = \\text{MFP} \\times (a \\times \\text{Labor}) \\] Then, divide this by the total labor input to get labor productivity: \\[ \\text{Labor Productivity} = \\frac{\\text{Labor&#39;s Contribution to Output}}{\\text{Labor Input}} = a \\times \\text{MFP} \\] In this formula, labor productivity is a function of MFP scaled by the weight \\(a\\) associated with labor. 10.4.4 Wage and Labor Earnings In an economy, two main categories capture labor compensation: wages and labor earnings. Here, wages refer strictly to the money paid per unit of time worked or task completed. Labor earnings, on the other hand, include wages along with other forms of compensation such as benefits and employer contributions to social security: \\[ \\begin{aligned} \\text{Labor Earnings}\\ =&amp;\\ \\text{Wages} \\\\ &amp; + \\text{Benefits} \\\\ &amp; + \\text{Employer Contributions to Social Security} \\end{aligned} \\] In this formulation: Wages: Money paid per unit of time worked or per task completed. Benefits: Non-wage compensation like health insurance, pension contributions, etc. Employer Contributions to Social Security: Contributions made by the employer to social security or other mandatory welfare schemes. This broader measure of labor compensation provides a more comprehensive view of how labor is rewarded in an economy. The average wage represents the typical wage level in an economy, and average labor earnings stand for the average compensation paid to the employed population. \\[ \\begin{aligned} \\text{Average Wage} &amp;= \\frac{\\text{Total Wages}}{\\text{Employed}}\\\\ \\text{Average Labor Earnings} &amp;= \\frac{\\text{Aggregate Labor Earnings}}{\\text{Employed}} \\end{aligned} \\] Wage share is the proportion of national income allocated to wages, while labor share is the portion of national income allocated to all forms of labor compensation, including benefits and employer contributions. \\[ \\begin{aligned} \\text{Wage Share}\\ =&amp;\\ \\frac{\\text{Total Wages}}{\\text{National Income}} \\times 100 \\\\ \\text{Labor Share}\\ =&amp;\\ \\frac{\\text{Total Labor Compensation}}{\\text{National Income}} \\times 100 \\end{aligned} \\] The share of income that doesn’t go to labor is sometimes referred to as capital share: \\[ \\text{Capital Share} = 100 - \\text{Labor Share} \\] The gender wage gap quantifies the difference in average earnings between men and women. \\[ \\text{Gender Wage Gap} = \\frac{\\text{Average Wage of Men} - \\text{Average Wage of Women}}{\\text{Average Wage of Men}} \\times 100 \\] The real wage is the nominal wage adjusted for changes in consumer prices, usually measured using the Consumer Price Index (CPI). \\[ \\text{Real Wage} = \\frac{\\text{Nominal Wage}}{\\text{CPI}} \\times 100 \\] These metrics provide a broad understanding of labor compensation and wage dynamics in an economy. 10.4.5 Job Vacancy and Turnover Rate The job vacancy rate is a key metric used to evaluate the labor market’s condition. A job vacancy refers to a position that is open but has not yet been filled. A high number of job vacancies could signal a growing economy, although it might also indicate a skills mismatch between employers and the labor force. The formula for the job vacancy rate is as follows: \\[ \\text{Job Vacancy Rate} = \\frac{\\text{Number of Job Vacancies}}{\\text{Total Posts}} \\times 100 \\] The “Total Posts” term in the formula is the sum of currently unfilled positions (job vacancies) and filled positions, representing the overall number of jobs in a particular market or sector. The turnover rate is another critical indicator of labor market dynamics. It measures the rate at which employees leave and join existing positions within companies. A high turnover rate may indicate a dynamic labor market but could also signify job dissatisfaction or instability, depending on the context. The formula for the turnover rate is as follows: \\[ \\text{Turnover Rate} = \\frac{\\text{Number of Employees Leaving} + \\text{Joining}}{\\text{Average Number of Employed}} \\times 100 \\] In summary, the job vacancy and turnover rate offer insights into labor supply and demand, employee satisfaction, and market stability. 10.4.6 Job Creation and Destruction In a dynamic economy, new jobs are created, and old jobs are destroyed. These concepts can be measured using the following formulas: \\[ \\begin{aligned} \\text{Net Job Creation}\\ =&amp;\\ \\text{Number of New Jobs Created} \\\\ &amp; - \\text{Number of Jobs Destroyed} \\\\ \\text{Job Creation Rate}\\ =&amp;\\ \\frac{\\text{Number of New Jobs Created}}{\\text{Total Number of Jobs at Start of Period}}\\\\ \\text{Job Destruction Rate}\\ =&amp;\\ \\frac{\\text{Number of Jobs Destroyed}}{\\text{Total Number of Jobs at Start of Period}} \\end{aligned} \\] Here, “Number of New Jobs Created” and “Number of Jobs Destroyed” refer to the change in employment levels over a specific period. “Total Number of Jobs at Start of Period” is the total employment at the beginning of the period under study. It’s important to note that job creation and destruction rates are often considered at the firm or industry level to better understand specific dynamics. Different datasets may use various definitions and methods to calculate these rates, but the basic idea remains the same. 10.4.7 Data Collection In the United States, labor market data is mainly collected by the Bureau of Labor Statistics (BLS), a federal agency under the Department of Labor. For more information, you can visit the BLS website at www.bls.gov. Internationally, agencies like the International Labour Organization (ILO) provide guidelines and compile labor market data from various countries for cross-country analyses. You can visit the ILO website at www.ilo.org. 10.4.8 Resources For further information on Labor Market Accounts, refer to the “Handbook of Methods” published by the Bureau of Labor Statistics (BLS, 2023) and the “Key Indicators of the Labour Market (KILM)” by the International Labour Organization (ILO, 2016). 10.5 Fiscal Accounts Fiscal Accounts are a subset of the national accounts and focus on the financial activities of the government, offering insights into how public funds are raised and spent. Understanding these accounts is essential for analyzing fiscal policy and its impact on an economy. 10.5.1 Government Budget Government revenue constitutes all the financial resources the government receives, which primarily include taxes, tariffs, and fees. It can be represented as: \\[ \\text{Government Revenue} = \\text{Tax Revenue} + \\text{Non-Tax Revenue} \\] Here, “Tax Revenue” is money raised from various types of taxes, while “Non-Tax Revenue” includes sources like fines, fees, and income from government-owned assets. Government expenditure includes all types of spending incurred by the government, such as public services, social welfare programs, and infrastructure investment. The general formula is: \\[ \\begin{aligned} \\text{Government Expenditure} \\ =&amp;\\ \\text{Government Consumption} \\\\ &amp;\\ + \\text{Government Investment} \\\\ &amp;\\ + \\text{Transfers} \\end{aligned} \\] The budget balance is an indicator of fiscal health, calculated as the difference between government revenue and government expenditure. \\[ \\begin{aligned} \\text{Budget Balance}\\ =&amp;\\ \\text{Government Revenue}\\\\ &amp;\\ - \\text{Government Expenditure} \\end{aligned} \\] A positive balance indicates a budget surplus, while a negative balance signifies a budget deficit. Public debt represents the total outstanding obligations the government owes, essentially accumulating from past budget deficits. It can be categorized into domestic and external debt, depending on who the creditors are. 10.5.2 Types of Taxes Taxes play a multifaceted role in fiscal policy: they are instruments for generating government revenue, tools for redistributing wealth, and levers for incentivizing specific economic or social behaviors. Comprehending the various types of taxes is crucial for understanding the nuances of fiscal policy. Taxes are primarily divided into two categories: direct and indirect. Direct Taxes Direct Taxes are levied directly on individuals or entities. The burden of the tax cannot be passed on to someone else. Income Tax: Taxed directly on individual salaries. It’s often progressive, meaning higher earners pay higher rates. For example, in the U.S., federal income tax rates range from 10% to 37% based on income brackets. Corporate Tax: Levied on company profits. Different countries aim to balance revenue needs and business incentives, such as Ireland’s low corporate tax rate to attract multinational corporations. Capital Gains Tax: Tax on profits from selling assets like stocks or real estate. In the U.S., long-term capital gains are taxed at a lower rate than ordinary income. Property Tax: Property taxes are usually assessed on the combined value of land and any buildings or improvements on it. For example, if you own a home, the property tax would be calculated based on the total value of both the land and the house. In some jurisdictions, property tax may also include other types of personal property, like cars or boats. Land Value Tax: This tax is levied specifically on the unimproved value of land, meaning it does not consider the value of buildings, personal property, or other improvements made to the land. For instance, if you own a vacant lot in a bustling city, you would pay a land value tax based solely on the market value of the land itself, not on what could potentially be built on it. This encourages landowners to develop their land to its highest and best use. Payroll Tax: This is a specific tax that employers withhold from an employee’s salary and remit to the federal government to fund Social Security and Medicare in the United States. Payroll taxes are separate from income taxes, though both are deducted from an employee’s paycheck. Unlike income tax, payroll tax rates are generally flat, up to a certain wage limit. Estate Tax: This tax is applied to the total value of a deceased person’s assets before they are distributed to heirs. The estate itself is responsible for paying this tax, which is based on the overall value of the estate. Inheritance Tax: Unlike estate tax, inheritance tax is paid by the individuals who actually inherit the assets or property. The tax rate often depends on the relationship between the deceased and the inheritor, as well as the value of the inherited assets. Gift Tax: Imposed on the transfer of assets like money, property, or stocks from one individual to another without any payment in return. In simple terms, if you give someone a valuable item and don’t get money or another asset back, it’s considered a gift and may be subject to this tax. Franchise Tax: This is a tax that businesses must pay for the privilege of operating within a specific jurisdiction, such as a state. For example, if a corporation is incorporated in Delaware but operates in Texas, it may have to pay a franchise tax to Texas based on its revenue or assets. Occupational Tax: Applied to individuals in specific professions, such as lawyers, this tax not only generates revenue for local governments but also helps fund and regulate the profession itself. The tax can be structured as a flat fee or a percentage of income. For instance, an occupational tax on lawyers might be used to fund a state’s legal regulatory body or support pro bono legal services for low-income individuals. Wealth Tax: Levied on the total net wealth of individuals or corporations, this tax considers assets such as real estate, stocks, and bank deposits, subtracting liabilities like loans. In Switzerland, for example, cantons and counties impose a wealth tax calculated as a percentage of net worth. Alternative Minimum Tax (AMT): This is a parallel tax system designed to ensure that high-income individuals cannot avoid paying their fair share of taxes by using various deductions and credits. For example, if someone has a high income but also has many deductions that would significantly reduce their tax liability, the AMT sets a minimum tax amount they must pay. Indirect Taxes Indirect Taxes are levied not on income or wealth, but on the consumption of goods and services. Unlike direct taxes, which are paid directly to the government by the individual or entity being taxed, indirect taxes are typically collected by an intermediary, such as a retailer or a producer. The tax burden can be shifted from the entity that pays the tax to another party, usually the consumer, in the form of higher prices for goods and services. For example, when you buy a gallon of gasoline, the price includes various indirect taxes, such as excise taxes, which the retailer then remits to the government. Sales Tax: This tax is applied as a percentage of an item’s retail price at the point of sale. For instance, if you purchase an item for $100 in a state with a 5% sales tax, the total cost will be $105. The retailer collects the extra $5 and forwards it to the government. In the U.S., sales tax rates differ by state, and some items such as groceries may be exempt. Value-Added Tax (VAT): Unlike sales tax, which is collected only at the point of final sale, VAT serves as an alternative and is collected at every stage of production and distribution. Businesses in each stage must collect this tax and pass it on to the government. For example, if a manufacturer sells raw materials to a factory, the VAT is levied on that transaction. This method is prevalent in European countries but is not used in the United States. Use Tax: This tax applies to items purchased in a jurisdiction with no sales tax or a lower sales tax than the jurisdiction where the consumer resides. It is paid directly by the consumer rather than being collected by the retailer. For example, if you live in a state with a 6% sales tax and buy an item online from a state with no sales tax, you would be responsible for paying the 6% use tax to your own state’s government. Essentially, it serves to equalize tax treatment for out-of-state and online purchases, ensuring that tax revenue is not lost. Excise Tax: This is a specific form of tax levied on particular goods, services, or activities. The manufacturer or supplier pays this tax directly to the government, but is often incorporated into the price of the product. For example, a sin tax on cigarettes discourages smoking and funds healthcare initiatives. Similarly, a soft drink tax aims to reduce unhealthy sugar consumption and may help fund public health programs. Sin Tax: Targets harmful products like tobacco and alcohol to discourage consumption and generate revenue that often goes toward public health initiatives. Soft Drink Tax: Imposed on beverages with added sugar, intending to combat obesity and related health issues. Air Travel Tax: Added to the cost of an airline ticket, potentially used to offset the environmental impact of air travel. Tourist Tax: Charged on services like hotel stays, aimed at generating revenue to improve local tourism infrastructure. Hotel Occupancy Tax: Specifically targets hotel room charges, often funding tourism boards or local cultural initiatives. Entertainment Tax: Added to ticket prices for events like concerts or sports games, sometimes funding arts and cultural programs. Luxury Tax: Imposed on high-value, non-essential items like luxury cars or fine jewelry, often aimed at reducing economic inequality. Carbon Tax: Targets fossil fuel emissions, with revenue generally used for environmental conservation. Severance Tax: Levied on companies that extract non-renewable resources such as oil, gas, and minerals, often aimed at supporting environmental initiatives or offsetting local impacts. Trade Tax: Imposed on goods that cross international borders. They serve to regulate trade, protect domestic industries, and generate revenue for the government. Customs Duty: This is a tax imposed on individual categories of imported goods. For example, a 10% customs duty might be levied on imported smartphones. The importer pays this tax directly to the government, but the cost is often passed on to consumers, increasing the retail price of the imported items. Tariff: Tariffs, on the other hand, are broader in scope and can affect multiple types of imported goods or even entire sectors. For instance, a country might impose a 20% tariff on all steel imports, regardless of the specific type of steel product. Tariffs are often used as strategic tools in trade policy and can be imposed to protect domestic industries or as a retaliatory measure against another country. Export Tax: A tax imposed on goods leaving a country. This is less common but may be used to control the export of certain resources. Anti-dumping Duty: A tax imposed on imports priced below fair market value to level the playing field for domestic producers. Countervailing Duties: These are special taxes that a country can impose on imported goods to neutralize the effect of subsidies provided by the exporting country’s government. The aim is to protect domestic industries that would otherwise struggle to compete with subsidized imports. For example, if Country A subsidizes its steel industry, making it cheaper for them to produce steel, Country B might impose countervailing duties on steel imports from Country A. This would raise the cost of the imported steel, leveling the playing field for domestic steel producers in Country B. Excise Duty on Imports: Similar to an internal excise tax but applied to imported goods to ensure they are taxed similarly to domestically produced items. Trade Quotas: Though not a tax, quotas restrict the quantity of a good that can be imported, effectively serving a similar purpose by limiting trade. Toll Tax: This is a fee for using certain public infrastructure like roads, bridges, or tunnels. For example, tolls collected on a highway might be used to fund its maintenance and upgrades. Stamp Duty: This tax is applied to formalize legal documents like property deeds or contracts. For instance, when you buy a house, you might pay stamp duty on the legal documents that transfer ownership to you. Financial Transaction Tax: This is a tax on the trading of financial assets like stocks and bonds. For example, a 0.1% financial transaction tax would mean a $1 tax on a $1,000 stock trade. Tax Terminology Proportional, Progressive, and Regressive: These terms describe how tax rates change with income or spending. A progressive tax (e.g., income tax) takes a higher percentage from high earners, while a regressive tax (e.g., sales tax) takes a larger share from low earners. Direct taxes like income tax can be designed to be progressive, while indirect taxes are generally considered regressive, affecting low-income individuals more. Tax Incidence: Refers to who bears the final burden of a tax. In direct taxes, this is the entity being taxed; in indirect taxes, it can be passed on to consumers. Tax Evasion: Illegally avoiding tax payment. More prevalent in direct taxes due to their complexity, but also occurs in indirect taxes, like underreporting sales to evade sales tax. Ad Valorem vs. Specific Taxes: Ad Valorem taxes are percentage-based (e.g., sales tax), whereas specific taxes are fixed amounts (e.g., $1 per gallon of gasoline, regardless of its price). Understanding these various forms of taxation aids in interpreting fiscal policy and its impact on both individuals and the broader economy. 10.5.3 Types of Subsidies Subsidies can be viewed as negative taxes and serve multiple roles in fiscal policy: they incentivize specific economic activities, redistribute resources, and achieve various social objectives. Understanding the different types of subsidies is crucial for a comprehensive grasp of fiscal policy. Subsidies are typically categorized into direct and indirect types, similar to taxes. Direct Subsidies Direct Subsidies are provided directly to individuals, businesses, or sectors to encourage specific activities or behaviors. They often aim to achieve social, economic, or environmental goals. Income Subsidies: Direct payments to low-income families to reduce poverty. Examples include the Earned Income Tax Credit (EITC) in the United States. Agricultural Subsidies: Financial support to farmers to encourage domestic agriculture. These can be direct payments or price support mechanisms. Housing Subsidies: Offered to low-income individuals to make housing more affordable. Common examples are housing vouchers or rent-controlled apartments. Healthcare Subsidies: Designed to lower the cost of healthcare for individuals. For instance, in the U.S., subsidies are offered through programs like Medicaid or via the Affordable Care Act to reduce insurance premiums. Educational Grants: Financial assistance provided to students to make higher education more accessible, such as Pell Grants in the U.S. Business Grants: Aim to stimulate economic development by supporting start-ups and small businesses. Often targeted at specific industries like technology or renewable energy. Research &amp; Development (R&amp;D) Subsidies: Financial incentives for companies to invest in innovation and research. These can take the form of grants or tax credits. Indirect Subsidies Indirect Subsidies are not given directly to the entity but are usually implemented through the price mechanism, often by reducing the cost of inputs or production. Tax Credits: While not direct cash payments, these reduce the tax liability for individuals or businesses, effectively serving as a subsidy. Examples include the Child Tax Credit or renewable energy tax credits. Price Supports: Government intervention in markets to set minimum or maximum prices. For example, minimum price supports for agricultural goods help ensure stable income for farmers. Public Services Subsidies: Subsidization of services like public transport, which benefits the general public by reducing ticket prices. Tariff Reductions or Eliminations: Lowering or removing trade barriers to benefit domestic consumers and industries reliant on imported goods. Loan Guarantees: The government guarantees a loan to reduce risk for lenders, thereby lowering interest rates for borrowers. Export Subsidies: Financial support for domestic businesses to encourage exports, either by direct payments or tax incentives. Subsidy Terminology Means-tested vs. Universal: Means-tested subsidies are provided only to those meeting specific criteria, usually related to income, while universal subsidies are available to all, regardless of income. Ad Valorem vs. Specific Subsidies: Ad Valorem subsidies are percentage-based, reducing the price of goods or services by a specific percentage. Specific subsidies are fixed amounts, such as a $1,000 grant for college tuition. Deadweight Loss: Refers to economic inefficiency that can arise from subsidies, often due to market distortions or encouraging less efficient behavior. Understanding the various forms of subsidies is crucial for analyzing fiscal policy, and its impacts on individual well-being and overall economic health. 10.5.4 Non-Tax Revenue Apart from taxes and subsidies, governments have additional means to generate revenue and redistribute wealth. These alternative sources of income provide fiscal flexibility and can serve specific economic or social objectives. Here are some of the other significant ways governments raise funds: Service Fees, Charges, and Regulatory Income This category encapsulates various types of fees and fines that governments charge for services and regulatory purposes. These sources of income generally cover administrative costs and sometimes serve as a control mechanism for certain activities. Fees and Charges: Governments impose fees for various services and administrative functions, such as issuing passports, driver’s licenses, and building permits. These charges typically cover the cost of providing the service itself. Fines and Penalties: Legal violations, such as traffic infractions and late tax payments, serve as another revenue source. While not classified as taxes, they contribute to government income. User Fees for Public Services: Certain public services like transportation and recreational facilities require user fees. These fees, though often covering only a fraction of the actual cost, are another form of government revenue. Regulatory Income: This refers to the revenue generated by governments through regulation, often in the form of licenses or permits required for conducting certain types of businesses or activities. While similar to fees and charges, regulatory income is specifically tied to oversight and control functions of the government. Public-Private Partnerships (PPPs): Governments sometimes collaborate with private entities to deliver public services or infrastructure projects. In such arrangements, the government might generate revenue through lease payments or a share of the profits. Lotteries and Gambling: In some countries, the government runs or heavily regulates lotteries and gambling operations, and takes a share of the revenue. Educational and Training Services: Some governments provide specialized training or educational services for a fee, both to citizens and to other governments. Advertisement Revenue: Some governments own media outlets or have advertising space in public areas and can generate revenue through these means. Investment and Asset Management In this category, governments leverage their holdings and financial assets to generate revenue. Whether through state-owned enterprises or financial market investments, the focus here is on wealth accumulation and offsetting public expenditures. State-Owned Enterprises (SOEs): Governments may own businesses in sectors like utilities, natural resources, or transportation. Revenue from these enterprises can be substantial and helps offset government expenditures. Sovereign Wealth Funds: Some governments manage large investment pools, often funded by natural resources like oil. These funds generate income through a variety of investments that can be used for public needs. Dividends and Interest: Financial investments made by the government can yield dividends or interest, serving as another income stream. For instance, governments might invest in bonds that pay periodic interest. Endowments and Trusts: These are funds set aside to be invested, with the profits to be used for a specific purpose, often educational or social. Rights and Licenses Governments often possess valuable natural or intellectual assets. By allowing private entities to exploit these resources under specific terms, they can secure another stream of income in the form of royalties, leases, and concession fees. Licenses and Royalties: Companies may pay for the right to use public goods or intellectual property owned by the government. These payments take the form of licenses and royalties, and they can be a significant income source. Leases and Land Sales: Governments often own land and natural resources. Leasing these assets, or selling them outright, can be a source of revenue. Concession Agreements: Governments may grant concessions to private firms for the right to operate a specific business within certain premises or for a particular service, often in return for a share of the revenue or a fixed fee. Asset Sales and Privatization: Governments can also generate funds by selling off public assets, or by privatizing services and industries. This is usually a one-time revenue source but can be significant. Financial Contributions and Aid This involves the interplay of grants, donations, and financial aid, both within a country and internationally. While these can be significant for budget support, they often come with specific conditions or obligations, making them a somewhat restricted revenue source. Domestic Grants and Donations: Federal governments often offer grants to local or state governments for specific projects. Conversely, they may also receive grants or donations from private entities or citizens for public initiatives. International Financial Aid: Governments often receive financial assistance from international organizations or more affluent countries to fund specific projects or support their budget. This financial support can be crucial for achieving developmental or emergency relief goals. However, aid often comes with conditions that the receiving country must meet. Repayable Aid and Reimbursements: When a country receives financial aid, there are often terms for payback. These repayments become a form of income for the country that provided the aid, offering a flexible tool for revenue generation. Inheritance and Gifts: Governments may be beneficiaries of large estates or gifts, contributing to revenue, although this is less common. Community Contributions: This includes unconventional but emerging sources like crowdfunding and community funding for local government projects. Unclaimed Property: This captures revenue from dormant accounts, uncollected insurance policies, and other unclaimed assets that revert to the government after a certain period. Monetary Policy Monetary policy tools, while primarily used to control inflation and stabilize the economy, can also generate revenue for the government. Whether through seigniorage or the profitable trading of financial instruments, these are specialized yet important revenue streams. However, it’s crucial to note that revenue generation through monetary policy is generally a side effect and should not be the primary objective. The primary goals are often macroeconomic stability, such as controlling inflation and influencing employment. The separation of monetary policy from political influence is particularly important in this context. When monetary policy becomes a tool for revenue generation influenced by political objectives, it risks being misaligned with the economy’s actual needs. Politicians, often focused on short-term gains or electoral cycles, could be tempted to manipulate these tools for immediate advantage, compromising long-term economic stability. This is why many countries place the responsibility for monetary policy in the hands of independent central banks. Seigniorage: This is the profit made by the government from issuing currency. For example, if it costs 1 cent to produce a $10 bill, the government gains $9.99. This is an aspect of monetary policy that can also serve as a revenue source. Central Bank Profits: Central banks often make profits from their various operations, including lending to commercial banks. These profits are usually remitted back to the government treasury. Open Market Operations (OMO): While the primary purpose is to control money supply, the government can earn interest from purchasing and holding securities. Profits can be realized if the securities are sold at higher prices than the purchase prices. Foreign Exchange Reserves: Governments hold foreign currency reserves, and any appreciation in these reserves relative to the domestic currency could be realized as profit if sold. Special Drawing Rights (SDR) Allocations: Some governments hold SDRs allocated by the International Monetary Fund (IMF). The interest difference between the SDR and what the government pays on its liabilities can be considered a form of income. Forward Contracts and Currency Swaps: Central banks may engage in forward contracts or currency swaps and earn a profit from the difference between the buying and selling rates, although this is less common. By understanding these additional sources of revenue, one can better appreciate the diverse fiscal tools available to governments for funding operations, achieving policy goals, and redistributing wealth. 10.5.5 Data Collection In the United States, various institutions play a crucial role in the collection and dissemination of fiscal data. The Department of the Treasury is responsible for federal financial management, including collecting revenue and issuing public debt. The Congressional Budget Office (CBO) provides non-partisan analyses of budgetary and economic issues, and their reports often serve as a basis for legislative decision-making. The Bureau of Economic Analysis (BEA) consolidates fiscal and economic data into the broader framework of national accounts. The U.S. Census Bureau also collects pertinent data, particularly regarding state and local governments, through its Census of Governments conducted every five years since 1957, for years ending in “2” and “7.” Internationally, organizations like the Organisation for Economic Co-operation and Development (OECD) gather fiscal statistics that allow for cross-country comparisons. The International Monetary Fund (IMF) similarly collects and disseminates international fiscal data. 10.5.6 Resources For those interested in deepening their understanding of Fiscal Accounts, several key resources are available. The “Government Finance Statistics Manual” by the International Monetary Fund (IMF, 2014) serves as a comprehensive guide to understanding government financial statistics globally. The Congressional Budget Office (CBO, 2023) publishes the “Budget and Economic Outlook” reports, providing in-depth analyses and projections of the U.S. federal budget. Additionally, the U.S. Census Bureau’s (2022) “Census of Governments” offers valuable insights into the organization and finances of state and local governments. These resources collectively offer a multi-faceted view of fiscal accounts from the municipal to the international level. 10.6 Monetary and Financial Accounts Monetary Accounts provide a structured framework for analyzing the money supply, including monetary aggregates like M1 and M2, and the implementation of monetary policy in an economy. These accounts are vital for understanding the dynamics of financial markets and institutions, as well as the efficacy of monetary policy instruments. Financial Accounts, distinct from Balance of Payments (BoP) Financial Accounts, focus on the domestic financial structure. They encompass a range of financial assets and liabilities, such as loans, securities, and equity, held by various sectors like households, corporations, and the government. Unlike the BoP Financial Accounts, which deal with international transactions and capital flows, these Financial Accounts offer insights into domestic financial relationships and conditions. By examining these two types of accounts, one gains a multi-dimensional view of the financial ecosystem. 10.6.1 Monetary Aggregates Money has three main roles: it serves as a medium of exchange, a unit of account, and a store of value. In modern economies, money exists as currency, such as coins and paper notes, and as various forms of bank deposits. Money is the most liquid asset, easily converted into other assets, goods, or services. The supply of money is controlled by the central bank through monetary policy actions that affect cash and bank reserves. Measuring money is complex because various liquid assets can also function as money, leading to ambiguity between monetary and non-monetary assets. While not a perfect solution, commonly used monetary aggregates like the Monetary Base, M1, and M2 measure the total amount of money in an economy. These categories are defined as follows: \\[ \\begin{aligned} \\text{Monetary Base} &amp;= \\text{Currency in Circulation} + \\text{Reserve Balances} \\\\ \\text{M1} &amp;= \\text{Currency} + \\text{Demand Deposits} \\\\ \\text{M2} &amp;= \\text{M1} + \\text{Savings Deposits} + \\text{Time Deposits} \\end{aligned} \\] Here: “Currency in Circulation” represents physical cash not held by financial institutions. “Reserve Balances” are the reserves held by depository institutions at the central bank. “Currency” refers to all physical cash, including that which is held by financial institutions. “Demand Deposits” are deposits that can be withdrawn at any time without notice. “Savings Deposits” are accounts that generally earn interest but have limitations on withdrawals. “Time Deposits” are accounts with a fixed term, often with a minimum time period, during which the money cannot be withdrawn without penalty. M1 is comprised of the most liquid assets used for daily transactions, and M2 also includes monetary assets that are less liquid. While M1 is a subset of M2, it’s important to note that the Monetary Base is not a subset of either M1 or M2. This is because M1 and M2 do not include “Reserve Balances” held by financial institutions at the central bank, as M1 and M2 focus on the forms of money directly used by households and firms. The Monetary Base is sometimes referred to as high-powered money. This term reflects its role as the base upon which the banking system can expand the money supply through lending activities. This amplification is feasible due to the practice of fractional-reserve banking, where banks are mandated to maintain only a fraction of their total deposits as reserves. The remaining amount is available for lending, leading to new deposits and an enlarged money supply. This phenomenon is known as the multiplier effect. Consider an illustrative example with a reserve requirement of 10%. A deposit of $1000 in Bank A mandates a reserve of $100, freeing $900 for lending. If this $900 is subsequently deposited in Bank B, it holds $90 in reserves and lends out $810. This chain reaction of lending and depositing continues across various banks, cumulatively multiplying the initial deposit. The Money Multiplier is the reciprocal of the reserve requirement. In this example, with a reserve requirement of 10%, the money multiplier is \\(\\frac{1}{0.10} = 10\\). Thus, the maximum quantum of new money that can emerge from the original $1000 deposit is \\(10 \\times \\$1000 = \\$10,000\\). This exemplifies the Monetary Base’s capacity to exponentially impact the broader money supply through a single deposit. 10.6.2 Interest Rates Interest rates are a cornerstone of monetary policy, serving as a powerful tool for central banks to steer economic activity. They influence a range of economic variables, from borrowing costs to investment decisions. Consider the following types of interest rates: Policy Rates: Central banks, such as the Federal Reserve in the United States, establish a policy rate to guide short-term interest rates. For example, the federal funds rate is the U.S. policy rate, which is the interest rate at which banks lend money to other banks overnight. While this rate is determined by the supply and demand for such overnight loans, the Federal Reserve can influence it through various tools like open market operations and the discount rate, which is discussed next. Discount Rate: This is the rate at which banks borrow directly from the central bank. It generally acts as an upper limit for the policy rate, as banks usually prefer interbank borrowing at lower rates. Central banks employ the discount rate to affect policy rates, which in turn influence both short-term and long-term interest rates. Short-term and Long-term Rates: Interest rates can be categorized based on their maturity. Short-term rates, such as Treasury bill rates, have a time to maturity of less than a year and usually follow the policy rate closely. Long-term rates, like 10-year Treasury yields, are more influenced by market expectations about future economic conditions. Nominal and Real Rates: The nominal rate is unadjusted, whereas the real rate accounts for inflation. They are related by the Fisher equation: \\[ \\begin{aligned} (1 + \\text{Nominal Rate}) \\ = &amp;\\ (1 + \\text{Real Rate}) \\times (1 + \\text{Inflation Rate}) \\\\ \\text{Nominal Rate} \\ \\approx &amp;\\ \\text{Real Rate} + \\text{Inflation Rate} \\end{aligned} \\] Real rates offer a truer cost of borrowing, useful for evaluating investment returns. The ex-ante real interest rate adjusts for expected inflation and signifies the anticipated cost of borrowing at the time the loan is made. The ex-post real interest rate adjusts for actual inflation, reflecting the true cost of borrowing after the fact. Interest rates have a stark influence on the economy. Higher rates increase borrowing costs, reducing consumer spending and business investment. Lower rates generally boost asset values. An increase in domestic rates can also strengthen the domestic currency, affecting trade. Central banks employ a variety of instruments to manage interest rates. Open market operations involve the buying or selling of government securities to sway the federal funds rate. The discount rate is what commercial banks pay to borrow from the central bank and essentially serves as an upper bound for the policy rate. Altering the banks’ reserve requirements also influences interest rates by shifting money demand. 10.6.3 Banking Reserves Banking reserves serve as a critical buffer for financial institutions, providing liquidity and ensuring system stability. Commercial banks hold these reserves either in their own vaults or with the central bank. They have specific components and functions: Required Reserves: These are the minimum reserves a bank must hold, dictated by the central bank through the reserve requirement ratio. \\[ \\text{Required Reserves} = \\text{Reserve Requirement Ratio} \\times \\text{Deposits} \\] Excess Reserves: These are reserves held beyond the minimum requirement, serving as an additional liquidity cushion. \\[ \\text{Excess Reserves} = \\text{Total Reserves} - \\text{Required Reserves} \\] Functions of reserves include: Liquidity Management: Reserves are crucial for managing short-term liquidity needs and meeting withdrawal demands from depositors. Monetary Policy: The central bank can adjust reserve requirements to influence money supply and, indirectly, interest rates. Financial Stability: Adequate reserve levels reduce the risk of bank runs and add to the resilience of the financial system. 10.6.4 Credit Aggregates Credit aggregates capture various forms of lending provided by financial institutions and are key indicators of financial system health and potential risks. They include: Consumer Credit: Includes credit card debt, auto loans, and other personal loans. Mortgage Credit: Loans secured by real estate properties. Corporate Credit: Includes loans to businesses, often in the form of bonds or direct loans. Public Sector Credit: Loans and securities related to government and quasi-government entities. Understanding credit aggregates is crucial for several reasons: Economic Activity: Credit conditions can stimulate or hinder economic actions like consumption and investment. Financial Stability: Analyzing these aggregates helps identify credit bubbles and measure systemic risk. Monetary Policy: The state of credit often reflects the impact of monetary policy interventions. 10.6.5 Exchange Rates Exchange rates are essentially the prices at which one currency can be exchanged for another. They play a pivotal role in international commerce, investment, and capital movements. They can also affect domestic monetary policy and price stability. Several types of exchange rates are analyzed in monetary accounts: Nominal Exchange Rate: Think of this as the sticker price for currencies. For instance, if 1 US Dollar equals 0.85 Euros, then \\(E = 0.85\\). \\[ E = \\frac{\\text{Units of Domestic Currency}}{\\text{Units of Foreign Currency}} \\] Real Exchange Rate: This rate accounts for purchasing power. If a hamburger costs 5 USD in the US and 4 Euros in Europe, then the real exchange rate between the hamburger in the US and Europe will consider both the nominal rate and the price levels. \\[ \\text{Real Exchange Rate (RER)} = E \\times \\frac{P_{\\text{domestic}}}{P_{\\text{foreign}}} \\] Effective Exchange Rate: Imagine the US trades mainly with Europe and Japan. The effective exchange rate will consider how the dollar stands against the Euro and the Yen, weighted by the size of trade with each region. \\[ \\text{Effective Exchange Rate (EER)} = \\sum_{i=1}^{n} w_i \\times E_i \\] Exchange Rate Regimes: Countries may adopt different exchange rate regimes, such as fixed (or pegged) exchange rates, where the currency value is fixed to another currency or a basket of currencies, or floating exchange rates, where the currency value is allowed to fluctuate according to the foreign exchange market. 10.6.6 Parity Conditions The concept of no arbitrage states that it should not be possible to make risk-free profits in financial markets. In simpler terms, you shouldn’t be able to buy low in one market and sell high in another without some form of risk. These conditions lead to what are known as parity conditions, which are equations that help us understand how prices should relate to each other to prevent such arbitrage. Exchange rates are theoretically tied to the following parity conditions, but these often deviate from theory due to market frictions. Purchasing Power Parity (PPP): According to this condition, identical goods should cost the same when priced in a common currency, assuming no transaction costs and barriers like tariffs. If there were a deviation from PPP, one could purchase the good in the cheaper market and sell it in the more expensive market, making a risk-free profit. The mathematical expression for PPP is: \\[ E = \\frac{P_{\\text{domestic}}}{P_{\\text{foreign}}} \\] Interest Rate Parity (IRP): This parity condition states that the difference in interest rates between two countries should be equal to the expected change in the exchange rate between their respective currencies. If this condition did not hold, one could borrow in the currency with the lower interest rate and invest in the currency with the higher interest rate, thereby making a risk-free profit. The mathematical representation for IRP is: \\[ E_{\\text{expected future}} = E \\times \\frac{(1 + \\text{Interest Rate}_{\\text{domestic}})}{(1 + \\text{Interest Rate}_{\\text{foreign}})} \\] Failure to adhere to these parity conditions would open up opportunities for arbitrage, thus violating the no-arbitrage principle. 10.6.7 Industrial Production and Capacity Utilization In addition to monetary and financial statistics, institutions that release these data often also provide measures of output at higher frequencies than the quarterly frequency commonly found in national accounts. Two such measures are Industrial Production (IP) and Capacity Utilization, which serve as useful indicators of economic activity, typically at a monthly frequency. Industrial Production (IP) is an index that measures the real output of the industrial sectors of the economy, including manufacturing, mining, and utilities. It is often expressed as a weighted average of the production outputs from these sectors. The mathematical form of the IP index can be expressed as: \\[ IP = \\sum_{i=1}^{n} (w_i \\times O_i) \\] Where: \\(IP\\) is the Industrial Production index. \\(w_i\\) is the weight of sector \\(i\\), reflecting its importance in the overall economy. \\(O_i\\) is the real output of sector \\(i\\), often measured in physical units or adjusted for inflation to be in real terms. The weights can be revised periodically to account for changes in the economic landscape, and the real output \\(O_i\\) is usually adjusted for seasonal variations and inflation. Capacity Utilization is a metric that measures the extent to which an enterprise uses its installed productive capacity. It is usually expressed as a percentage: \\[ \\text{Capacity Utilization} = \\left( \\frac{\\text{Actual Output}}{\\text{Potential Output}} \\right) \\times 100 \\] Where: Actual Output is the real output produced by the industrial sectors. Potential Output is the maximum possible output that could be produced if all resources were fully utilized. High levels of Capacity Utilization indicate that resources are being used efficiently, but extremely high levels may signal inflationary pressure due to supply constraints. Low levels may indicate underutilized resources and economic slack. By offering real measures of output and resource utilization at a higher frequency, both IP and Capacity Utilization complement the data available in monetary and financial accounts, allowing for more timely assessments of economic activity. 10.6.8 Data Collection In the United States, several government institutions collect monetary and financial accounts data: Federal Reserve System: Also known simply as the Fed, this is the central bank of the United States. It publishes data on money supply, interest rates, and other monetary policy variables. An overview of their data is available at www.federalreserve.gov/data.htm. The Fed categorizes this data into tables such as H.6 and G.20, which can be found at www.federalreserve.gov/releases/h6 and /g20, among others. The Z.1 Financial Accounts of the United States, formerly known as the Flow of Funds report, complements these tables by providing a broad financial overview of the U.S. economy every quarter. Below are the data tables relevant for this section and their publication frequencies: Money Stock Measures: H.3: Aggregate Reserves of Depository Institutions and the Monetary Base (Weekly) H.6: Money Stock Measures (Monthly) Interest Rates: H.15: Selected Interest Rates (Daily) Banking Reserves: H.3: Aggregate Reserves of Depository Institutions and the Monetary Base (Weekly) Credit Aggregates: G.19: Consumer Credit (Monthly) G.20: Finance Companies (Monthly) Exchange Rates: H.10: Foreign Exchange Rates (Weekly) Industrial Production and Capacity Utilization: G.17: Industrial Production and Capacity Utilization (Monthly) U.S. Department of the Treasury: Provides information on federal finances, including treasury securities and their interest rates. For more information visit their website at home.treasury.gov. Internationally, the following institutions are notable for collecting monetary and financial accounts data: International Monetary Fund (IMF): A key source for monetary and financial data, offering a range of data from monetary aggregates to exchange rates for cross-country analyses. Visit their website at www.imf.org. Bank for International Settlements (BIS): Specializes in global financial system data, including international banking activities, securities markets, policy rates, exchange rates, and payment systems. Visit their website at www.bis.org. Organisation for Economic Co-operation and Development (OECD): Provides various financial statistics, including banking, insurance, and investment data for cross-country comparisons. Visit their website at www.oecd.org. World Bank: Offers key financial and monetary indicators through databases like the World Development Indicators (WDI). Visit their website at www.worldbank.org. Financial Stability Board (FSB): Focuses on monitoring the global financial system and provides relevant financial statistics and reports. Visit their website at www.fsb.org. Asian Development Bank (ADB): Provides financial and monetary statistics specific to Asian countries. Visit their website at www.adb.org. The European Central Bank (ECB): Offers comprehensive monetary statistics for the Eurozone. Visit their website at www.ecb.europa.eu. 10.6.9 Resources For a deeper understanding of the Financial Accounts of the United States, the following readings are recommended: Albert M. Teplin (2001) provides an overview of U.S. flow of funds accounts in his paper “The U.S. Flow of Funds Accounts and Their Uses”. For an interactive and searchable guide on how to navigate and interpret the “Z.1 Financial Accounts of the United States,” the Board of Governors of the Federal Reserve System (2023a) provides a web-based guide at www.federalreserve.gov/apps/fof. The website allows users to explore data series, account structures, and the underlying source data. To supplement the above resources, the “Explanatory Notes” section of the latest Z.1 report by the Fed Board (2023b) is a valuable resource. Additionally, for insights into international monetary and financial accounts, refer to the International Monetary Fund’s (IMF, 2016) “Monetary and Financial Statistics Manual and Compilation Guide”. 10.7 Price Level and Inflation Price levels indicate the average cost of goods and services in an economy, while inflation represents the rate at which these price levels increase over time. Various methods and indices quantify these concepts, as detailed in subsequent subsections. 10.7.1 Implicit Measurement Implicit Price Deflators quantify the price level by taking the ratio of nominal to real measures, as elaborated in Chapter 10.2.6. National Accounts provide both nominal and real output values. These can be employed to compute various implicit price deflators, including the GDP Deflator: \\[ \\text{GDP Deflator} = \\left( \\frac{\\text{Nominal GDP}}{\\text{Real GDP}} \\right) \\times 100 \\] Here, “Nominal GDP” refers to the value of all goods and services produced within a country during a specific period, evaluated at current market prices. “Real GDP” measures output using constant prices from a specific base year. National Accounts also generate other deflators that offer specialized views of prices in different sectors. For example, the Personal Consumption Expenditures (PCE) Deflator focuses on prices of consumer goods and services: \\[ \\text{PCE Deflator} = \\left( \\frac{\\text{Nominal PCE}}{\\text{Real PCE}} \\right) \\times 100 \\] In this formula, “Nominal PCE” is the value of consumer goods and services at current prices, whereas “Real PCE” values them at a constant set of prices from a specified base year. Another example is the Export Deflator: \\[ \\text{Export Deflator} = \\left( \\frac{\\text{Nominal Exports}}{\\text{Real Exports}} \\right) \\times 100 \\] Here, “Nominal Exports” represent the value of all exported goods and services at current prices during a specific period. “Real Exports” adjust this value to constant prices from a chosen base year. These are implicit measures of price levels. Explicit measures, which directly observe prices, are discussed in the following sections. 10.7.2 Direct Measurement The Consumer Price Index (CPI) measures the average prices paid by consumers on a representative basket of goods and services. The formula to calculate CPI is: \\[ \\text{CPI} = \\left( \\frac{\\text{Cost of Basket in Current Year}}{\\text{Cost of Basket in Base Year}} \\right) \\times 100 \\] “Basket” here refers to a predefined set of goods and services that exemplifies typical consumption behavior. The CPI and the PCE Deflator serve similar functions: they measure the price level of consumer spending. However, they differ in methodology, scope, and timeliness. The PCE Deflator is more inclusive, covering all consumer goods and services, while the CPI uses a fixed basket. Additionally, the CPI is released more quickly than the PCE Deflator, making it a more immediate tool for inflation analysis. Finally, the difference in methodology makes the two indices useful complements for cross-validation. The Core CPI excludes volatile components like food and energy from the CPI: \\[ \\text{Core CPI} = \\left( \\frac{\\text{Cost of Core Basket in Current Year}}{\\text{Cost of Core Basket in Base Year}} \\right) \\times 100 \\] Here, the “Core Basket” consists of a selected range of goods and services, excluding food and energy, that represents typical consumption patterns. The Producer Price Index (PPI) measures the average prices that domestic producers receive for their output. The formula is: \\[ \\text{PPI} = \\left( \\frac{\\text{Cost of Basket of Produced Goods in Current Year}}{\\text{Cost of Basket of Produced Goods in Base Year}} \\right) \\times 100 \\] A rise in the PPI is often viewed as a precursor to an increase in the CPI, as higher production costs are typically passed on to consumers in the form of elevated retail prices. The Export Price Index (EPI) captures prices in exports: \\[ \\text{EPI} = \\left( \\frac{\\text{Cost of Export Basket in Current Year}}{\\text{Cost of Export Basket in Base Year}} \\right) \\times 100 \\] Note that the EPI and the Export Deflator differ in breadth. The latter accounts for all exported goods and services, while EPI uses a fixed basket. The Import Price Index (IPI) monitors the cost of imported goods: \\[ \\text{IPI} = \\left( \\frac{\\text{Cost of Import Basket in Current Year}}{\\text{Cost of Import Basket in Base Year}} \\right) \\times 100 \\] Real Prices adjust nominal prices for changes in the overall price level, providing a more accurate measure of relative value. For instance, a real IPI can be calculated as: \\[ \\text{Real IPI} = \\left(\\frac{\\text{IPI}}{\\text{GDP Deflator}} \\right) \\times 100 \\] The Real IPI provides an insightful measure of the true cost of imported goods over time. For instance, should the domestic currency appreciate, imported goods might become relatively cheaper even if nominal prices increase. In such a scenario, the IPI may rise due to general inflation, but the Real IPI could actually decline, thereby uncovering this nuanced economic behavior. 10.7.3 Inflation Inflation measures the percentage change in the price level over a specific time period. For example, CPI Inflation is defined as: \\[ \\begin{aligned} \\text{CPI Inflation} \\ = &amp; \\ \\left( \\frac{\\text{CPI in Current Period} - \\text{CPI in Previous Period}}{\\text{CPI in Previous Period}} \\right) \\times 100 \\\\ \\approx &amp;\\ \\ \\Big(\\ln (\\text{CPI in Current Period}) - \\ln (\\text{CPI in Previous Period})\\Big) \\times 100 \\end{aligned} \\] Inflation serves as a key indicator for assessing price stability of an economy. Deflation, or negative inflation, can be harmful. Anticipating lower future prices, consumers may delay spending, potentially triggering economic contraction. Moreover, deflation increases the real value of debt, exacerbating financial distress. 10.7.4 Data Collection In the United States, the Bureau of Labor Statistics (BLS) is the main agency responsible for collecting data on price levels. Internationally, organizations like the International Monetary Fund (IMF) and World Bank compile and publish price level data. 10.7.5 Resources For more details on price levels and their calculations, consult the Bureau of Labor Statistics’ (BLS, 2023) “Handbook of Methods” and the International Monetary Fund’s (IMF, 2016) “Monetary and Financial Statistics Manual and Compilation Guide.” References "],["financial-market-indicators.html", "Chapter 11 Financial Market Indicators 11.1 Interest Rates", " Chapter 11 Financial Market Indicators This chapter provides a comprehensive overview of key financial market indicators that offer insights into the performance and stability of financial markets. It covers the financial theories behind these indicators, their relevance, and the methodologies used to measure them. By examining interest rates, stock market indices, commodity prices, yield curves, and credit spreads, readers will understand how these indicators are determined and interpreted, and how they can be used to forecast future economic conditions. 11.1 Interest Rates This chapter demonstrates how to import Treasury yield curve rates data from a CSV file. Treasury yield curve data represents interest rates on U.S. government loans for different maturities. By comparing rates at various maturities, such as 1-month, 1-year, 5-year, 10-year, and 30-year, we gain insights into market expectations. An upward sloping yield curve, where interest rates for longer-term loans (30-year) are significantly higher than those for short-term loans (1-month), often indicates economic growth. Conversely, a downward sloping or flat curve may suggest the possibility of a recession. This data is crucial in making informed decisions regarding borrowing, lending, and understanding the broader economic landscape. To obtain the yield curve data, follow these steps: Visit the U.S. Treasury’s data center by clicking here. Click on “Data” in the menu bar, then select “Daily Treasury Par Yield Curve Rates.” On the data page, select “Download CSV” to obtain the yield curve data for the current year. To access all the yield curve data since 1990, choose “All” under the “Select Time Period” option, and click “Apply.” Please note that when selecting all periods, the “Download CSV” button may not be available. If the “Download CSV” button is not available, click on the link called Download interest rates data archive. Then, select yield-curve-rates-1990-2021.csv to download the daily yield curve data from 1990-2021. To add the more recent yield curve data since 2022, go back to the previous page, choose the desired years (e.g., “2022”, “2023”) under the “Select Time Period,” and click “Apply.” Manually copy the additional rows of yield curve data and paste them into the yield-curve-rates-1990-2021.csv file. Save the file as “yieldcurve.csv” in a location of your choice, ensuring that it is saved in a familiar folder for easy access. Following these steps will allow you to obtain the yield curve data, including both historical and recent data, in a single CSV file named “yieldcurve.csv.” 11.1.1 Import CSV File CSV (Comma Separated Values) is a common file format used to store tabular data. As the name suggests, the values in each row of a CSV file are separated by commas. Here’s an example of how data is stored in a CSV file: Male,8,100,3 Female,9,20,3 To import the ‘yieldcurve.csv’ CSV file in R, install and load the readr package. Run install.packages(\"readr\") in the console and include the package at the top of your R script. You can then use the read_csv() or read_delim() function to import the yield curve data: # Load the package library(&quot;readr&quot;) # Import CSV file yc &lt;- read_csv(file = &quot;files/yieldcurve.csv&quot;, col_names = TRUE) # Import CSV file using the read_delim() function yc &lt;- read_delim(file = &quot;files/yieldcurve.csv&quot;, col_names = TRUE, delim = &quot;,&quot;) In the code snippets above, the read_csv() and read_delim() functions from the readr package are used to import a CSV file named “yieldcurve.csv”. The col_names = TRUE argument indicates that the first row of the CSV file contains column names. The delim = \",\" argument specifies that the columns are separated by commas, which is the standard delimiter for CSV (Comma Separated Values) files. Either one of the two functions can be used to read the CSV file and store the data in the variable yc for further analysis. To inspect the first few rows of the data, print the yc object in the console. For an overview of the entire dataset, use the View() function, which provides an interface similar to viewing the CSV file in Microsoft Excel: # Display the data yc ## # A tibble: 8,382 × 14 ## Date `1 Mo` `2 Mo` `3 Mo` `4 Mo` `6 Mo` `1 Yr` `2 Yr` `3 Yr` `5 Yr` `7 Yr` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01/02/… N/A N/A 7.83 N/A 7.89 7.81 7.87 7.9 7.87 7.98 ## 2 01/03/… N/A N/A 7.89 N/A 7.94 7.85 7.94 7.96 7.92 8.04 ## 3 01/04/… N/A N/A 7.84 N/A 7.9 7.82 7.92 7.93 7.91 8.02 ## 4 01/05/… N/A N/A 7.79 N/A 7.85 7.79 7.9 7.94 7.92 8.03 ## 5 01/08/… N/A N/A 7.79 N/A 7.88 7.81 7.9 7.95 7.92 8.05 ## 6 01/09/… N/A N/A 7.8 N/A 7.82 7.78 7.91 7.94 7.92 8.05 ## 7 01/10/… N/A N/A 7.75 N/A 7.78 7.77 7.91 7.95 7.92 8 ## 8 01/11/… N/A N/A 7.8 N/A 7.8 7.77 7.91 7.95 7.94 8.01 ## 9 01/12/… N/A N/A 7.74 N/A 7.81 7.76 7.93 7.98 7.99 8.07 ## 10 01/16/… N/A N/A 7.89 N/A 7.99 7.92 8.1 8.13 8.11 8.18 ## # ℹ 8,372 more rows ## # ℹ 3 more variables: `10 Yr` &lt;dbl&gt;, `20 Yr` &lt;chr&gt;, `30 Yr` &lt;chr&gt; # Display the data in a spreadsheet-like format View(yc) Both the read_csv() and read_delim() functions convert the CSV file into a tibble (tbl_df), a modern version of the R data frame discussed in Chapter 4.6. Remember, a data frame stores data in separate columns, each of which must be of the same data type. Use the class(yc) function to check the data type of the entire dataset, and sapply(yc, class) to check the data type of each column: # Check the data type of the entire dataset class(yc) ## [1] &quot;spec_tbl_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; # Check the data type of each column sapply(yc, class) ## Date 1 Mo 2 Mo 3 Mo 4 Mo 6 Mo ## &quot;character&quot; &quot;character&quot; &quot;character&quot; &quot;numeric&quot; &quot;character&quot; &quot;numeric&quot; ## 1 Yr 2 Yr 3 Yr 5 Yr 7 Yr 10 Yr ## &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; ## 20 Yr 30 Yr ## &quot;character&quot; &quot;character&quot; When importing data in R, it’s possible that R assigns incorrect data types to some columns. For example, the Date column is treated as a character column even though it contains dates, and the 30 Yr column is treated as a character column even though it contains interest rates. To address this issue, you can convert the first column to a date type and the remaining columns to numeric data types using the following three steps: Replace “N/A” with NA, which represents missing values in R. This step is necessary because R doesn’t recognize “N/A”, and if a column includes “N/A”, R will consider it as a character vector instead of a numeric vector. yc[yc == &quot;N/A&quot;] &lt;- NA Convert all yield columns to numeric data types: yc[, -1] &lt;- sapply(yc[, -1], as.numeric) The as.numeric() function converts a data object into a numeric type. In this case, it converts columns with character values like “3” and “4” into the numeric values 3 and 4. The sapply() function applies the as.numeric() function to each of the selected columns. This converts all the interest rates to numeric data types. Convert the date column to a date object, recognizing that the date format is Month/Day/Year or %m/%d/%Y: # Check the date format head(yc$Date) ## [1] &quot;01/02/90&quot; &quot;01/03/90&quot; &quot;01/04/90&quot; &quot;01/05/90&quot; &quot;01/08/90&quot; &quot;01/09/90&quot; # Convert to date format yc$Date &lt;- as.Date(yc$Date, format = &quot;%m/%d/%y&quot;) # Sort data according to date yc &lt;- yc[order(yc$Date), ] # Print first 5 observations of date column head(yc$Date) ## [1] &quot;1990-01-02&quot; &quot;1990-01-03&quot; &quot;1990-01-04&quot; &quot;1990-01-05&quot; &quot;1990-01-08&quot; ## [6] &quot;1990-01-09&quot; # Print last 5 observations of date column tail(yc$Date) ## [1] &quot;2023-06-23&quot; &quot;2023-06-26&quot; &quot;2023-06-27&quot; &quot;2023-06-28&quot; &quot;2023-06-29&quot; ## [6] &quot;2023-06-30&quot; Hence, we have successfully imported the yield curve data and performed the necessary conversions to ensure that all columns are in their correct formats: yc ## # A tibble: 8,382 × 14 ## Date `1 Mo` `2 Mo` `3 Mo` `4 Mo` `6 Mo` `1 Yr` `2 Yr` `3 Yr` `5 Yr` ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1990-01-02 NA NA 7.83 NA 7.89 7.81 7.87 7.9 7.87 ## 2 1990-01-03 NA NA 7.89 NA 7.94 7.85 7.94 7.96 7.92 ## 3 1990-01-04 NA NA 7.84 NA 7.9 7.82 7.92 7.93 7.91 ## 4 1990-01-05 NA NA 7.79 NA 7.85 7.79 7.9 7.94 7.92 ## 5 1990-01-08 NA NA 7.79 NA 7.88 7.81 7.9 7.95 7.92 ## 6 1990-01-09 NA NA 7.8 NA 7.82 7.78 7.91 7.94 7.92 ## 7 1990-01-10 NA NA 7.75 NA 7.78 7.77 7.91 7.95 7.92 ## 8 1990-01-11 NA NA 7.8 NA 7.8 7.77 7.91 7.95 7.94 ## 9 1990-01-12 NA NA 7.74 NA 7.81 7.76 7.93 7.98 7.99 ## 10 1990-01-16 NA NA 7.89 NA 7.99 7.92 8.1 8.13 8.11 ## # ℹ 8,372 more rows ## # ℹ 4 more variables: `7 Yr` &lt;dbl&gt;, `10 Yr` &lt;dbl&gt;, `20 Yr` &lt;dbl&gt;, `30 Yr` &lt;dbl&gt; Here, &lt;dbl&gt; stands for double, which is the R data type for decimal numbers, also known as numeric type. Converting the yield columns to dbl ensures that the values are treated as numeric and can be used for calculations, analysis, and visualization. 11.1.2 Plotting Historical Yields Let’s use the plot() function to visualize the imported yield curve data. In this case, we will plot the 3-month Treasury rate over time, using the Date column as the x-axis and the 3 Mo column as the y-axis: # Plot the 3-month Treasury rate over time plot(x = yc$Date, y = yc$`3 Mo`, type = &quot;l&quot;, xlab = &quot;Date&quot;, ylab = &quot;%&quot;, main = &quot;3-Month Treasury Rate&quot;) In the code snippet above, plot() is the R function used to create the plot. It takes several arguments to customize the appearance and behavior of the plot: x represents the data to be plotted on the x-axis. In this case, it corresponds to the Date column from the yield curve data. y represents the data to be plotted on the y-axis. Here, it corresponds to the 3 Mo column, which represents the 3-month Treasury rate. type = \"l\" specifies the type of plot to create. In this case, we use \"l\" to create a line plot. xlab = \"Date\" sets the label for the x-axis to “Date”. ylab = \"%\" sets the label for the y-axis to “%”. main = \"3-Month Treasury Rate\" sets the title of the plot to “3-Month Treasury Rate”. Figure 11.1: 3-Month Treasury Rate The resulting plot, shown in Figure 11.1, displays the historical evolution of the 3-month Treasury rate since 1990. It allows us to observe how interest rates have changed over time, with low rates often observed during recessions and high rates during boom periods. Recessions are typically characterized by reduced borrowing and investment activities, leading to decreased demand for credit and lower interest rates. Conversely, boom periods are associated with strong economic growth and increased credit demand, which can drive interest rates upward. Furthermore, inflation plays a significant role in influencing interest rates through the Fisher effect. When inflation is high, lenders and investors are concerned about the diminishing value of money over time. To compensate for the erosion of purchasing power, lenders typically demand higher interest rates on loans. These higher interest rates reflect the expectation of future inflation and act as a safeguard against the declining value of the money lent. Conversely, when inflation is low, lenders may offer lower interest rates due to reduced concerns about the erosion of purchasing power. 11.1.3 Plotting Yield Curve Next, let’s plot the yield curve. The yield curve is a graphical representation of the relationship between the interest rates (yields) and the time to maturity of a bond. It provides insights into market expectations regarding future interest rates and economic conditions. To plot the yield curve, we will select the most recently available data from the dataset, which corresponds to the last row. We will extract the interest rates as a numeric vector and the column names (representing the time to maturity) as labels for the x-axis: # Extract the interest rates of the last row yc_most_recent_data &lt;- as.numeric(tail(yc[, -1], 1)) yc_most_recent_data ## [1] 5.24 5.39 5.43 5.50 5.47 5.40 4.87 4.49 4.13 3.97 3.81 4.06 3.85 # Extract the column names of the last row yc_most_recent_labels &lt;- colnames(tail(yc[, -1], 1)) yc_most_recent_labels ## [1] &quot;1 Mo&quot; &quot;2 Mo&quot; &quot;3 Mo&quot; &quot;4 Mo&quot; &quot;6 Mo&quot; &quot;1 Yr&quot; &quot;2 Yr&quot; &quot;3 Yr&quot; &quot;5 Yr&quot; ## [10] &quot;7 Yr&quot; &quot;10 Yr&quot; &quot;20 Yr&quot; &quot;30 Yr&quot; # Plot the yield curve plot(x = yc_most_recent_data, xaxt = &#39;n&#39;, type = &quot;o&quot;, pch = 19, xlab = &quot;Time to Maturity&quot;, ylab = &quot;Treasury Rate in %&quot;, main = paste(&quot;Yield Curve on&quot;, format(tail(yc$Date, 1), format = &#39;%B %d, %Y&#39;))) axis(side = 1, at = seq(1, length(yc_most_recent_labels), 1), labels = yc_most_recent_labels) In the code snippet above, plot() is the R function used to create the yield curve plot. Here are the key inputs and arguments used in the function: x = yc_most_recent_data represents the interest rates of the most recent yield curve data, which will be plotted on the x-axis. xaxt = 'n' specifies that no x-axis tick labels should be displayed initially. This is useful because we will customize the x-axis tick labels separately using the axis() function. type = \"o\" specifies that the plot should be created as a line plot with points. This will display the yield curve as a connected line with markers at each data point. pch = 19 sets the plot symbol to a solid circle, which will be used as markers for the data points on the yield curve. xlab = \"Time to Maturity\" sets the label for the x-axis to “Time to Maturity”, indicating the variable represented on the x-axis. ylab = \"Treasury Rate in %\" sets the label for the y-axis to “Treasury Rate in %”, indicating the variable represented on the y-axis. main = paste(\"Yield Curve on\", format(tail(yc$Date, 1), format = '%B %d, %Y')) sets the title of the plot to “Yield Curve on” followed by the date of the most recent yield curve data. Additionally, the axis() function is used to customize the x-axis tick labels. It sets the tick locations using at = seq(1, length(yc_most_recent_labels), 1) to evenly space the ticks along the x-axis. The labels = yc_most_recent_labels argument assigns the column names of the last row (representing maturities) as the tick labels on the x-axis. Figure 11.2: Yield Curve on June 30, 2023 The resulting plot, shown in Figure 11.2, depicts the yield curve based on the most recent available data, allowing us to visualize the relationship between interest rates and the time to maturity. The x-axis represents the different maturities of the bonds, while the y-axis represents the corresponding treasury rates. Analyzing the shape of the yield curve can provide insights into market expectations and can be useful for assessing economic conditions and making investment decisions. The yield curve can take different shapes, such as upward-sloping (normal), downward-sloping (inverted), or flat, each indicating different market conditions and expectations for future interest rates. An upward-sloping yield curve, where longer-term interest rates are higher than shorter-term rates, is often seen during periods of economic expansion. This shape suggests that investors expect higher interest rates in the future as the economy grows and inflationary pressures increase. It reflects an optimistic outlook for economic conditions, as borrowing and lending activity are expected to be robust. In contrast, a downward-sloping or inverted yield curve, where shorter-term interest rates are higher than longer-term rates, is often considered a predictor of economic slowdown or recession. This shape suggests that investors anticipate lower interest rates in the future as economic growth slows and inflationary pressures decrease. It reflects a more cautious outlook for the economy, as investors seek the safety of longer-term bonds amid expectations of lower returns and potential economic downturn. Inflation expectations also influence the shape of the yield curve. When there are high inflation expectations for the long term, the yield curve tends to slope upwards. This occurs because lenders demand higher interest rates for longer maturities to compensate for anticipated inflation. However, when there is currently high inflation but expectations are that the central bank will successfully control inflation in the long term, the yield curve may slope downwards. In this case, long-term interest rates are lower than short-term rates, as average inflation over the long term is expected to be lower than in the short term. "],["part-iii.html", "Part III: Economic Data Processing", " Part III: Economic Data Processing Part III sheds light on crucial techniques for transforming economic and financial data. This includes methods to derive growth rates, adjust values for real terms, and calculate per capita figures. It also guides on aggregating data across frequencies, for example, transitioning from monthly to yearly values. The section also clarifies common data types, differentiating between stock and flow variables, levels of measurement, and data structures like cross-sectional and time-series. Core statistical metrics such as mean, median, and variance are also explained. Included chapters: Chapter 12: “Data Categorization” delves into different ways to classify and categorize economic data. Chapter 13: “Data Transformation” explores methods for modifying and manipulating data, to make it more suitable for analysis or to reveal hidden patterns. Chapter 14: “Statistical Measures” introduces statistics common to Finance and Economics, like measures of centrality, dispersion, and association. Chapter 15: “Data Aggregation” explains how to compile data from a lower frequency to a higher one, such as aggregating monthly data into quarterly or annual data. "],["data-categorization.html", "Chapter 12 Data Categorization 12.1 Qualitative vs. Quantitative 12.2 Discrete vs. Continuous 12.3 Levels of Measurement 12.4 Index vs. Absolute Data 12.5 Stock vs. Flow 12.6 Data Dimensions 12.7 Conclusion", " Chapter 12 Data Categorization Before we delve into various economic indicators, it is essential to build a vocabulary of how data is categorized. This categorization is primarily based on the type of measurement and the dimensions of data: Qualitative vs. Quantitative: This refers to the nature of the data being collected, whether it’s descriptive (qualitative) or numerical (quantitative). Discrete vs. Continuous: This classification is based on the quantifiable values that the data can take on, either as distinct and countable (discrete) or as an infinite range of values within a certain scale (continuous). Levels of Measurement: Data can also be classified based on the scale of measure it uses, such as nominal, ordinal, interval, or ratio. Index vs. Absolute Data: This categorization depends on whether the data values are meaningful in relative terms or absolute terms. Index data is meaningful in relation to other values of the same index, whereas absolute data values have meaning on their own, independently of other values. Stock vs. Flow: This dichotomy classifies data based on whether they represent a quantity at a specific point in time (stock) or a rate over a period (flow). Data Dimensions: This categorization refers to the way data is organized across temporal and spatial dimensions, including cross-sectional, time-series, panel, spatial, and clustered types of data. Understanding these categories will allow us to more effectively interpret and analyze economic indicators. 12.1 Qualitative vs. Quantitative In the world of data analysis and research, data is typically classified into two broad categories: Qualitative and Quantitative. Qualitative data, often called categorical data, refers to non-numerical information that expresses the descriptive, subjective, or explanatory attributes of the variables under study. This data type includes factors such as colors, gender, nationalities, opinions, or any other attribute that does not have a natural numerical representation. In other words, qualitative data deals with characteristics and descriptors that can’t be easily measured, but can be observed subjectively. On the other hand, quantitative data is numerical and lends itself to mathematical and statistical manipulation. This data type involves numbers and things measurable in a quantifiable way. Examples of quantitative data include height, weight, age, income, temperature, or any variable that can be measured or counted. Quantitative data forms the backbone of any statistical analysis and tends to be more structured than qualitative data. As an example, we can consider the Affairs dataset from the AER (Applied Econometrics with R) package, which is utilized for teaching and learning applied econometrics. The Affairs dataset comes from a study by Ray C. Fair in 1978, published in the Journal of Political Economy, titled “A Theory of Extramarital Affairs”. The study attempted to model the time spent in extramarital affairs as a function of certain background variables. # Load the AER package library(&quot;AER&quot;) # Load the Affairs dataset data(&quot;Affairs&quot;, package = &quot;AER&quot;) # Display a few rows of the Affairs dataset tail(Affairs) ## affairs gender age yearsmarried children religiousness education ## 1935 7 male 47 15.0 yes 3 16 ## 1938 1 male 22 1.5 yes 1 12 ## 1941 7 female 32 10.0 yes 2 18 ## 1954 2 male 32 10.0 yes 2 17 ## 1959 2 male 22 7.0 yes 3 18 ## 9010 1 female 32 15.0 yes 3 14 ## occupation rating ## 1935 4 2 ## 1938 2 5 ## 1941 5 4 ## 1954 6 5 ## 1959 6 2 ## 9010 1 5 This dataset comprises 601 observations and nine variables, with a mixture of four qualitative and five quantitative variables: affairs: Quantitative - how much time is spent in extramarital affairs. gender: Qualitative - gender of the respondents. age: Quantitative - age of the respondents. yearsmarried: Quantitative - number of years married. children: Qualitative - whether the respondents have children or not. religiousness: Qualitative - rating of religiousness, 1 being not religious at all, and 5 being very religious. education: Quantitative - level of education in years of schooling. occupation: Qualitative - category of the respondent’s occupation. rating: Quantitative - a self-rating of the marriage, 1 indicates very unhappy, and 5 indicates very happy. Although certain qualitative variables are numerically coded, they remain qualitative in nature. They signify varying categories instead of directly measuring an attribute. 12.2 Discrete vs. Continuous When dealing with quantitative data, it is further divided into two categories: Discrete and Continuous. The distinction between these two lies in the nature of the values or observations that the variables can assume. Discrete data, as the name suggests, can only take certain, separated values. These values are distinct and countable, and there are no intermediate values between any two given values. Examples of discrete data include the number of students in a class, the number of cars in a parking lot, or any other count that cannot be divided into smaller parts. Contrarily, continuous data can take any value within a given range or interval. Here, the data points are not countable as they may exist at any point along a continuum within a given range. This means that the variables have an infinite number of possibilities. Examples of continuous data include the height of a person, the weight of an animal, or the time it takes to run a race. In these examples, the measurements can be broken down into smaller units, and a great number of precise values are possible. For instance, let’s examine the mtcars dataset that comes with the R programming language. This dataset provides information on various attributes of 32 automobiles from the 1970s. # View the first few rows of the mtcars dataset head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 One of the variables in this dataset, mpg (miles per gallon), is an example of a continuous variable. Continuous variables, such as mpg, can take on any value within a range. In this context, a car’s fuel efficiency (measured in miles per gallon) can theoretically take any non-negative value. This makes mpg a continuous variable. On the other hand, the cyl variable, representing the number of cylinders in a car’s engine, is a discrete variable. Discrete variables can only take certain, distinct values. In the case of cyl, cars can have a whole number of cylinders—usually 4, 6, or 8. This characteristic makes cyl a discrete variable. 12.3 Levels of Measurement Qualitative and quantitative data can be classified into four levels of measurement: nominal, ordinal, interval, and ratio These levels represent an increasing degree of precision in measurement. The first two levels, nominal and ordinal, are applicable to qualitative data, while the last two levels, interval and ratio, are relevant to (discrete or continuous) quantitative data. 12.3.1 Nominal Scale Variables Nominal scale variables are a type of categorical variable that represent distinct groups or categories without an inherent order or ranking. These variables essentially “name” the attribute and don’t possess any quantitative significance. In the field of economics, an example of a nominal variable could be the industry sector of a company, such as technology, healthcare, manufacturing, and so on. These are distinct categories with no implied order or priority. In R, nominal variables can be represented using the factor() function. Here’s an example of how this can be done: # Create a factor vector to represent industry sectors sectors &lt;- factor(c(&quot;technology&quot;, &quot;healthcare&quot;, &quot;manufacturing&quot;, &quot;technology&quot;, &quot;manufacturing&quot;), levels = c(&quot;technology&quot;, &quot;healthcare&quot;, &quot;manufacturing&quot;)) # Display the created factor vector print(sectors) ## [1] technology healthcare manufacturing technology manufacturing ## Levels: technology healthcare manufacturing In the above code snippet, the factor() function is used to convert a character vector into a factor vector, with the distinct industry sectors specified as the levels. This approach can enhance data analysis efficiency, as R internally assigns a distinct integer to each level. Hence, when dealing with large datasets, R can quickly refer to these numerical assignments instead of the corresponding character strings, improving processing speed. The levels() function can be used to extract the defined levels of a factor vector, while the as.numeric() function can convert the factor levels to their corresponding numeric codes, as shown below: # Extract defined levels of the factor vector levels(sectors) ## [1] &quot;technology&quot; &quot;healthcare&quot; &quot;manufacturing&quot; # Convert factor levels to their corresponding numeric codes as.numeric(sectors) ## [1] 1 2 3 1 3 In the output of this code, the levels() function will display the distinct industry sectors, while the as.numeric() function will present the numerical code assigned to each sector in the original vector. 12.3.2 Ordinal Scale Variables Ordinal scale variables, like nominal variables, are categorical. However, ordinal scale variables have a clear ordering of the variables. An example in economics might be a credit rating (e.g., AAA, AA, A, BBB, BB). Here’s an example: # Creating a factor with ordered levels credit_rating &lt;- factor(c(&quot;AAA&quot;, &quot;AA&quot;, &quot;AAA&quot;, &quot;BBB&quot;, &quot;BB&quot;), levels = c(&quot;C&quot;, &quot;B&quot;, &quot;BB&quot;, &quot;BBB&quot;, &quot;A&quot;, &quot;AA&quot;, &quot;AAA&quot;), ordered = TRUE) print(credit_rating) ## [1] AAA AA AAA BBB BB ## Levels: C &lt; B &lt; BB &lt; BBB &lt; A &lt; AA &lt; AAA In this code, the ordered = TRUE argument tells R that the levels should be considered in a specific order. 12.3.3 Interval Scale Variables Interval scale variables are a type of numerical variable where the distance between two values is meaningful. However, they lack a true zero point. An example in economics is the difference between two credit scores. The difference between a score of 800 and 700 is the same as the difference between 700 and 600, but a score of 0 does not imply the absence of creditworthiness, it’s just a lower bound. Interval data lacks the absolute zero point, which makes direct comparisons of magnitude impossible. A person with a credit score of 800 is not twice as creditworthy as someone with a score of 400. # Creating a numeric vector of credit scores credit_scores &lt;- c(800, 750, 700, 650, 600) print(credit_scores) ## [1] 800 750 700 650 600 Another example of an interval scale variable is temperature when measured in Celsius or Fahrenheit. For instance, 20 degrees Celsius is not twice as hot as 10 degrees Celsius. Furthermore, 0 degrees Celsius doesn’t imply the absence of temperature. Thus, the Celsius scale is an interval scale where the differences between values are meaningful, but it doesn’t have a true zero point or a ratio relationship between different temperatures. 12.3.4 Ratio Scale Variables Ratio scale variables, like interval scale variables, are numeric, but they have a clear definition of zero. When the value of a ratio scale variable is zero, it means the absence of the quantity. Examples in economics include income, annual sales, market share, unemployment rate, and GDP. When GDP is zero it means that no output is produced in that country during the year. # Creating a numeric vector of GDP values in billions of dollars gdp_billions &lt;- c(19362, 21195, 22675, 21433, 22770) print(gdp_billions) ## [1] 19362 21195 22675 21433 22770 The presence of a meaningful zero point allows us to make valid comparisons using ratios. For instance, one country’s GDP being twice as large as another’s is a meaningful statement. Under normal circumstances, ratio scale variables cannot assume negative values, as this would imply a quantity less than nothing, which is nonsensical. Variables such as age, height, weight, or income are all examples of ratio scale variables as they can be compared using ratios (someone can be twice as old or earn thrice as much income as someone else), and their zero points are meaningful (zero age means no time has passed since birth, zero height or weight means an absence of these attributes, and zero income means no money has been earned). Therefore, negative values for these variables would not make sense. However, there are certain exceptions where variables typically considered ratio scale variables can indeed be negative. These exceptions include instances such as net income, inflation, and interest rates. Here, a “negative” value does not imply less than nothing but rather denotes a particular condition or state. Negative net income reflects a state of indebtedness, negative inflation indicates a deflation, and negative interest rates imply a penalty for storing money at a bank. Despite being negative, these values can still be meaningfully compared to each other using ratios. For instance, 4% inflation is twice as high as 2% inflation, and deflation of 2% (equivalent to -2% inflation) is twice as high as deflation of 1% (or -1% inflation). However, one cannot meaningfully compare positive inflation to deflation using ratios. For example, 4% inflation cannot be compared using ratios to 2% deflation. 12.4 Index vs. Absolute Data The field of economics and finance often uses two types of data for analysis: index data and absolute data. Unlike absolute data, which provides valuable insights on its own, index data is only meaningful when considered in relative terms. 12.4.1 Index Data Index data, often referred to as indices or indexes, and not to be confused with indicators, are measures which values have no meaning on their own. An index value only becomes meaningful as comparison to a value at a different time period. Stock market indices are prime examples of index data. These indices combine various stock prices into a normalized index. As part of this process, an arbitrary value, such as 100, is assigned to the index at a given point in time (e.g., 2010). If the index subsequently rises to 110, this indicates a 10% increase in the total price of the underlying stocks. However, without the context provided by the previous index value (100), the new value (110) lacks meaningful interpretation. The reason for creating an index lies in the complexity of making raw values meaningful on their own. Simply adding all share prices together wouldn’t provide valuable insights, as the share price of a particular company may be much higher than others due to arbitrary factors like the number of shares they’ve issued. Instead, stock market indices typically aggregate share prices by normalizing their contributions to the index based on factors such as the size of the company. During this aggregation process, the relative weights of the stocks carry importance, but the sum of these weights — whether it’s 1, 100, or 34 — is irrelevant. This results in an index that lacks inherent meaning on its own, and thus, it only carries significance in relative terms. After completing this aggregation process, the index is standardized to an arbitrary value, often set at 100 at a specific moment in time. 12.4.2 Absolute Data On the other hand, absolute data refers to data that has inherent meaning on its own, without needing comparison to other data. A fitting example of absolute data is the Gross Domestic Product (GDP), which measures the total market value of all finished goods and services produced within a country’s borders over a specific time period. A GDP value, for instance, $24,000 billion USD, provides a self-explanatory measurement. It indicates that, during that year, the country produced goods and services worth $24,000 billion USD, irrespective of previous values. In summary, while index data offers relative comparisons that highlight trends and changes over time, absolute data also provides measurements that are meaningful on their own. 12.5 Stock vs. Flow Economic and financial data can be broadly categorized into two types of variables: stock and flow. 12.5.1 Stock Variables A stock variable represents a quantity measured at a single specific point in time. In a sense, they provide a snapshot of a specific moment. Here are examples of stock variables: Wealth: The total accumulation of assets a person owns at a given point in time. Debt: The total amount owed by a person, business, or country at a certain moment. Population: The total number of people in a country or region at a given time. Capital Stock: The total value of assets held by a firm at a point in time. Unemployment Level: The total number of people unemployed at a specific point in time. Inventory: The total amount of goods a company has in stock at a given time. Reserves: The total amount of currency held by a central bank at a specific time. Households’ savings: The total amount of unconsumed income of a household at a point in time. 12.5.2 Flow Variables A flow variable, on the other hand, is measured over an interval of time. Therefore, a flow would be measured per unit of time (say a year or a month). Here are examples of flow variables: Income: Money that an individual or business receives over a period of time. Spending: Money spent by individuals or businesses over a certain period. Production: The total amount of goods and services produced over a time period. Consumption: The total goods and services consumed over a period of time. Investment: Money invested by a business over a time period. Imports and Exports: Goods and services brought into or sent out of a country over a time period. Government spending: Total expenditure by the government over a certain period. Changes in inventory: The difference in inventory levels between two points in time. 12.5.3 Accounting The concepts of stock and flow are also evident in the financial statements of companies. The Balance Sheet presents the stock of what a company owns (assets) and owes (liabilities) at a specific point in time, while the Income Statement depicts the flow of revenues and expenses that result in net income or loss over a particular period. This clear separation aids in understanding a company’s financial position (stock) and performance (flow) separately, helping various stakeholders, including investors, creditors, and management, make informed decisions. In conclusion, understanding the difference between stock and flow variables is crucial in economics, finance, and accounting, as it provides insights into a wide range of issues, from personal financial planning to the evaluation of a country’s economic performance or a company’s financial health. 12.6 Data Dimensions Understanding the dimensions of a dataset is crucial in data analysis and econometrics. The dimensions of a dataset refer to how the observations are organized across different dimensions such as time and space. There are five principal arrangements of data across dimensions in economic data: Cross-Sectional Time Series Panel Spatial Clustered Each type of data arrangement presents its own unique characteristics, challenges, and opportunities for data analysis. 12.6.1 Cross-Sectional Data Cross-sectional data refer to information collected from multiple subjects at a specific point in time. In such data, each row represents a unique observation or individual. Examples of cross-sectional data are surveys and administrative records of persons, households, or firms. When analyzing this data, the standard assumption is that observations are independent, meaning that the ordering of observations does not matter. Consider the Ecdat package in R, which contains numerous datasets used in econometrics textbooks. A good example of cross-sectional data is the Wages dataset from the Ecdat package. The dataset provides information about workers’ wages and relevant characteristics. # Load the Ecdat package library(Ecdat) # Load the Wages dataset data(Wages) # Display the first few rows of the Wages dataset head(Wages) ## exp wks bluecol ind south smsa married sex union ed black lwage ## 1 3 32 no 0 yes no yes male no 9 no 5.56068 ## 2 4 43 no 0 yes no yes male no 9 no 5.72031 ## 3 5 40 no 0 yes no yes male no 9 no 5.99645 ## 4 6 39 no 0 yes no yes male no 9 no 5.99645 ## 5 7 42 no 1 yes no yes male no 9 no 6.06146 ## 6 8 35 no 1 yes no yes male no 9 no 6.17379 In the Wages dataset, each row corresponds to a unique worker, providing details such as their education, work experience, region, gender, and union membership. The order of workers does not affect the interpretation of the data, thereby affirming the dataset’s cross-sectional nature. Another example of cross-sectional data is the Hdma dataset from the Ecdat package. The Hdma dataset comprises of information regarding home loans in Boston, as gathered by the Home Mortgage Disclosure Act (HMDA). # Load the Hdma dataset data(Hdma) # Display the first few rows of the Hdma dataset head(Hdma) ## dir hir lvr ccs mcs pbcr dmi self single uria comdominiom black ## 1 0.221 0.221 0.8000000 5 2 no no no no 3.9 0 no ## 2 0.265 0.265 0.9218750 2 2 no no no yes 3.2 0 no ## 3 0.372 0.248 0.9203980 1 2 no no no no 3.2 0 no ## 4 0.320 0.250 0.8604651 1 2 no no no no 4.3 0 no ## 5 0.360 0.350 0.6000000 1 1 no no no no 3.2 0 no ## 6 0.240 0.170 0.5105263 1 1 no no no no 3.9 0 no ## deny ## 1 no ## 2 no ## 3 no ## 4 no ## 5 no ## 6 no The Hdma dataset presents each row as a loan applicant, including details about the applicant’s income, gender, race, loan specifications, and the decision made by the bank. As the ordering of applicants does not affect data interpretation, this dataset also qualifies as cross-sectional data. Finally, consider the built-in state.x77 dataset in R, sourced from the 1977 Statistical Abstract of the United States. The dataset consists of various economic and demographic measures of the U.S. states. # Display the US State Facts head(state.x77) ## Population Income Illiteracy Life Exp Murder HS Grad Frost Area ## Alabama 3615 3624 2.1 69.05 15.1 41.3 20 50708 ## Alaska 365 6315 1.5 69.31 11.3 66.7 152 566432 ## Arizona 2212 4530 1.8 70.55 7.8 58.1 15 113417 ## Arkansas 2110 3378 1.9 70.66 10.1 39.9 65 51945 ## California 21198 5114 1.1 71.71 10.3 62.6 20 156361 ## Colorado 2541 4884 0.7 72.06 6.8 63.9 166 103766 The state.x77 dataset comprises 50 rows and 8 columns, each row corresponding to one of the 50 US states. The columns represent a variety of measurements including the population, income, illiteracy rates, life expectancy, murder rates, high school graduation rates, frost frequency, and land area. The data is ordered alphabetically according to the state’s name, but changing the order wouldn’t affect interpretation, making this a cross-sectional dataset. Despite the geographic attributes of each state, the absence of specific geographic information (such as latitude and longitude) qualifies this dataset as cross-sectional rather than spatial data. 12.6.2 Time Series Data Time series data are observations that are indexed by time. Each row in a time series dataset typically represents an observation at a distinct time point. Examples of this type of data are annual GDP, daily interest rates, and the hourly share price of Tesla. The key characteristic of time series data is that the ordering of observations does matter, and observations are not assumed to be independent of one another. The datasets package in R, which is automatically installed and loaded with R, includes several built-in time series datasets that are commonly used for illustrative purposes. Consider the AirPassengers dataset as an example: # Air Passenger Monthly Totals (in thousands) from 1949 to 1960 AirPassengers ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec ## 1949 112 118 132 129 121 135 148 148 136 119 104 118 ## 1950 115 126 141 135 125 149 170 170 158 133 114 140 ## 1951 145 150 178 163 172 178 199 199 184 162 146 166 ## 1952 171 180 193 181 183 218 230 242 209 191 172 194 ## 1953 196 196 236 235 229 243 264 272 237 211 180 201 ## 1954 204 188 235 227 234 264 302 293 259 229 203 229 ## 1955 242 233 267 269 270 315 364 347 312 274 237 278 ## 1956 284 277 317 313 318 374 413 405 355 306 271 306 ## 1957 315 301 356 348 355 422 465 467 404 347 305 336 ## 1958 340 318 362 348 363 435 491 505 404 359 310 337 ## 1959 360 342 406 396 420 472 548 559 463 407 362 405 ## 1960 417 391 419 461 472 535 622 606 508 461 390 432 The AirPassengers dataset includes monthly totals of international airline passengers from 1949 to 1960. Each row represents a particular month, and the order of the data matters due to potential temporal dependencies, such as seasonality or trends. Another example of a time series dataset is EuStockMarkets which includes daily closing prices of major European stock indices from 1991 to 1998: # Load EuStockMarkets dataset data(EuStockMarkets) # Display the first few rows, and include the Date variable head(data.frame(Date = time(EuStockMarkets), EuStockMarkets)) ## Date DAX SMI CAC FTSE ## 1 1991.496 1628.75 1678.1 1772.8 2443.6 ## 2 1991.500 1613.63 1688.5 1750.5 2460.2 ## 3 1991.504 1606.51 1678.6 1718.0 2448.2 ## 4 1991.508 1621.04 1684.1 1708.1 2470.4 ## 5 1991.512 1618.16 1686.6 1723.1 2484.7 ## 6 1991.515 1610.61 1671.6 1714.3 2466.8 In this dataset, each row represents a trading day, and the order of the days is crucial to understanding trends and volatilities in the stock markets. Lastly, consider the longley dataset, available in R’s built-in datasets: # Longley&#39;s Economic Regression Data (built-in dataset) head(longley) ## GNP.deflator GNP Unemployed Armed.Forces Population Year Employed ## 1947 83.0 234.289 235.6 159.0 107.608 1947 60.323 ## 1948 88.5 259.426 232.5 145.6 108.632 1948 61.122 ## 1949 88.2 258.054 368.2 161.6 109.773 1949 60.171 ## 1950 89.5 284.599 335.1 165.0 110.929 1950 61.187 ## 1951 96.2 328.975 209.9 309.9 112.075 1951 63.221 ## 1952 98.1 346.999 193.2 359.4 113.270 1952 63.639 The longley dataset includes annual observations on six macroeconomic variables for the United States, spanning from 1947 to 1962. Each row stands for a year, and the ordering of the years carries substantial information about the economic progression of the country. Aggregate economic data such as the longley dataset is typically available at low frequency (annual, quarterly, or perhaps monthly) so the sample size is quite small, while financial data such as the EuStockMarkets dataset is typically available at high frequency (weekly, daily, hourly, or per transaction). In all of these examples, the order of observations matters significantly because it allows for the analysis of trends, cycles, and other time-dependent structures within the data. Proper handling of these elements is crucial for effective time series analysis. This often involves the application of statistical methods designed specifically for time series data, such as autoregressive integrated moving average (ARIMA) models and vector autoregression (VAR) models. These models take into account the temporal dependencies and provide a more reliable analysis of the data. 12.6.3 Panel Data Panel data, also known as longitudinal data or cross-sectional time series data, combines elements of both cross-sectional and time series data. In panel data, observations are collected on multiple entities (such as individuals, households, firms, or countries) over multiple time periods. This data structure provides unique opportunities to analyze both individual and time effects, as well as their interaction. To illustrate panel data, we can explore some examples using R datasets. One example is the Produc dataset, which is available in the plm package: # Load the plm package library(&quot;plm&quot;) # Load the Produc dataset data(&quot;Produc&quot;, package = &quot;plm&quot;) # Display a few rows of the Produc dataset Produc[15:20, ] ## state year region pcap hwy water util pc gsp emp ## 15 ALABAMA 1984 6 19257.47 8655.94 2235.16 8366.37 59446.86 45118 1387.7 ## 16 ALABAMA 1985 6 19433.36 8726.24 2253.03 8454.09 60688.04 46849 1427.1 ## 17 ALABAMA 1986 6 19723.37 8813.24 2308.99 8601.14 61628.88 48409 1463.3 ## 18 ARIZONA 1970 8 10148.42 4556.81 1627.87 3963.75 23585.99 19288 547.4 ## 19 ARIZONA 1971 8 10560.54 4701.97 1627.34 4231.23 24924.82 21040 581.4 ## 20 ARIZONA 1972 8 10977.53 4847.84 1614.58 4515.11 26058.65 23289 646.3 ## unemp ## 15 11.0 ## 16 8.9 ## 17 9.8 ## 18 4.4 ## 19 4.7 ## 20 4.2 The Produc dataset contains state-level data for the United States from 1970 to 1986. It includes variables such as state GDP, capital stock, employment, and wages. With data from 48 states, each state having 17 yearly observations, this dataset combines both cross-sectional and time-series components. This panel structure allows for analyzing the relationship between these variables over time, while also accounting for heterogeneity across states. Another example is the EmplUK dataset from the plm package: # Load the EmplUK dataset data(&quot;EmplUK&quot;, package = &quot;plm&quot;) # Display a few rows of the EmplUK dataset EmplUK[19:24, ] ## firm year sector emp wage capital output ## 19 3 1981 7 19.570 24.8714 6.2136 99.5581 ## 20 3 1982 7 18.125 24.8447 5.7146 98.6151 ## 21 3 1983 7 16.850 28.9077 7.3431 100.0301 ## 22 4 1977 8 26.160 14.8283 8.4902 118.2223 ## 23 4 1978 8 26.740 14.8379 8.7420 120.1551 ## 24 4 1979 8 27.280 14.8756 9.1869 118.8319 The EmplUK dataset contains information on employment in different industries in the United Kingdom from 1977 to 1983. It includes variables such as employment, wages, and output. This dataset allows for analyzing how employment and wages vary across industries and over time. In panel data analysis, various econometric techniques can be applied to account for time-varying effects, individual-specific heterogeneity, and potential endogeneity. Common methods include fixed effects models, random effects models, and dynamic panel data models. These approaches help uncover relationships that may be obscured in cross-sectional or time series analysis alone. 12.6.4 Spatial Data Spatial data are observations that are indexed by geographical locations or coordinates. Each row in a spatial dataset typically represents an observation at a distinct spatial unit. Examples of this type of data are housing prices across different districts, climate data from different weather stations, and population densities across various regions. The key characteristic of spatial data is that the geographical arrangement of observations does matter, and observations are not assumed to be independent of one another, often exhibiting spatial autocorrelation where observations close in space tend to be more similar. The house dataset, available in the spdep package, serves as a fitting example for spatial data analysis: # Load the spdep package library(&quot;spdep&quot;) # Load the house dataset data(house) # Display a few rows and columns of the house dataset head(house[, 1:6]) ## coordinates price yrbuilt stories TLA wall beds ## 1 (484668.1, 195270.3) 303000 1978 one+half 3273 partbrk 4 ## 2 (484875.6, 195301.3) 92000 1957 one 920 metlvnyl 2 ## 3 (485248.4, 195353.8) 90000 1937 two 1956 stucdrvt 3 ## 4 (485764.2, 196177.5) 330000 1887 one+half 1430 wood 4 ## 5 (488149.8, 196591.4) 185000 1978 two 2208 partbrk 3 ## 6 (485525.7, 196660) 100000 1989 one 1232 wood 1 The house dataset consists of details pertaining to 25,357 single-family homes sold in Lucas County, Ohio, between 1993 and 1998. It includes particulars such as sale prices, locations, and other relevant features. Additionally, the dataset reveals the GPS coordinates of each house in the coordinates column. The first value in the coordinates column denotes the northern coordinate, while the second one refers to the western coordinate. Researchers can employ these GPS coordinates to conduct spatial analyses, examining housing market dynamics in Lucas County while taking into account spatial autocorrelation and spatial dependence. It’s important to note that the regional datasets previously discussed in the context of cross-sectional and panel data can also be classified as spatial data. To perform spatial data analysis on these regional datasets, one needs to merge them with another dataset that includes GPS or longitude and latitude data for each region. This geographical data is often available online; for instance, one can find the longitude and latitude for each U.S. state or county with relative ease. By integrating geographic information and taking into account spatial dependencies, economists can improve the accuracy of their empirical results, gaining deeper insights into the influence of geographical factors on economic outcomes. Common methods include spatial lag models, spatial error models, and spatial autoregressive models. These approaches help reveal relationships that may be hidden when using cross-sectional or time series analysis alone. 12.6.5 Clustered Data Clustered data is a form of data that may resemble panel or spatial data, as it groups observations based on criteria such as geographical location, time period, or subject. This clustering implies that observations within each group are likely to exhibit a degree of correlation, meaning they are more likely to resemble each other than observations in different clusters. However, the approach to clustered data contrasts with panel or spatial data in an important way: in clustered data, the specific nature of the correlation within each cluster is not typically explicitly modeled. For example, while we might expect houses sold within the same county (a cluster) to have more similar characteristics than houses sold across different counties, we don’t explicitly model the specific relationships among the houses within each county so that the order of observations within clusters doesn’t matter. On the other hand, panel data has a time-dependent structure as it involves repeated observations collected from the same subjects over time. Similarly, spatial data has a location-dependent structure, where observations are based on their geographical locations. In both these cases, the dependencies within the data are explicitly incorporated into the data analysis. Consider the EmplUK dataset as an example of clustered data. This dataset contains panel data on employment and wages for different industries in the United Kingdom from 1977 to 1987, as discussed earlier in the panel data section: # Display a few rows of the EmplUK dataset EmplUK[19:25, ] ## firm year sector emp wage capital output ## 19 3 1981 7 19.570 24.8714 6.2136 99.5581 ## 20 3 1982 7 18.125 24.8447 5.7146 98.6151 ## 21 3 1983 7 16.850 28.9077 7.3431 100.0301 ## 22 4 1977 8 26.160 14.8283 8.4902 118.2223 ## 23 4 1978 8 26.740 14.8379 8.7420 120.1551 ## 24 4 1979 8 27.280 14.8756 9.1869 118.8319 ## 25 4 1980 8 27.830 15.2332 9.4036 111.9164 The EmplUK dataset is clustered by industry, suggesting that observations within the same industry may exhibit similar employment patterns or be influenced by industry-specific factors. Depending on the research question and context, the EmplUK dataset could be treated as either panel data or clustered data. When analyzing the EmplUK dataset to understand employment and wage variations across different industries over time, treating it as panel data is appropriate. In this context, each industry is a distinct unit in the panel data framework, and using panel data models allows researchers to study variations in employment and wages over time and across industries, accounting for both time effects and industry-specific effects. In a different context, the EmplUK dataset could be treated as clustered data. Here, the interest lies in understanding the variation and potential correlations within each industry. The clusters here are the industries themselves, and observations within each industry might be more similar to each other due to industry-specific factors such as common economic conditions, regulations, or labor market dynamics. Treating the data as clustered acknowledges the correlation within each cluster (industry) but does not model this correlation explicitly. Hence, the choice between treating the EmplUK dataset as panel data or clustered data depends on the research question. When working with clustered data, it is vital to use statistical techniques that account for the correlation within clusters. Common techniques include the use of cluster-robust standard errors, fixed effects models, or random effects models. 12.7 Conclusion In this chapter, we’ve developed a comprehensive vocabulary for understanding and categorizing data. The classifications include whether the data is described in words or numbers (qualitative or quantitative), the quantifiable values the data can assume (discrete or continuous), the scale of measurement (nominal, ordinal, interval, or ratio), its relational form (index vs. absolute data), its temporal nature (stock or flow), and its dimensions (cross-sectional, time-series, panel, spatial, and clustered). Together, these classifications provide us with a robust framework to approach and interpret different types of data that we encounter in economic analysis. Each of these categories has unique characteristics that require specific analytical methods and considerations. For example, time-series data are analyzed differently from cross-sectional data, considering their inherent temporal order and potential autocorrelation. Similarly, data transformations, such as logarithmic or difference transformations, may be more applicable or meaningful to certain types of data than others. In the following chapters, we will apply this vocabulary to economic indicators and the transformations thereof. We will discuss how each economic indicator fits into these categories and how specific data transformations can enhance our understanding of these indicators. Remember, the appropriate transformation depends on the specific type of data and the research question at hand, so our newly established vocabulary will prove invaluable. "],["data-transformation.html", "Chapter 13 Data Transformation 13.1 Example Data 13.2 Growth 13.3 Differentiation 13.4 Natural Logarithm 13.5 Ratio 13.6 Gap 13.7 Filtering", " Chapter 13 Data Transformation In Economics and Finance, it is common to transform raw data into meaningful indicators. One common transformation is per capita GDP, where a country’s GDP is divided by its population. This gives a more comparable measure of economic performance across different countries. Similarly, inflation transforms the price index into a growth rate of prices, which helps us understand price distortions, interest rates, and evaluate monetary policy. While these transformations enhance data comparability, they can also introduce complexity. This chapter explores common data transformations in Economics and Finance and offers insights on their interpretation. Throughout this chapter, we will explore the following transformed measures: Growth: This term refers to how an economic variable changes compared to its original value over a set period. Often, it’s expressed as a percentage. Differentiation: Differentiation measures the change between an economic variable’s current value and its preceding value. This offers insights into the degree and direction of change. Natural Logarithm (Log): Logarithms have many uses, like simplifying data that shows exponential growth or quantifying proportional changes. Ratio: Ratio measures calculate the ratio between two variables, such as per capita GDP or unemployment rate. They provide information on the relative size or magnitude of one variable in relation to another. Gap: Gap measures calculate the distance between two variables, highlighting disparities. Filtering: These measures remove noise or specific fluctuations from the data, providing a clearer view of specific patterns. By the end of this chapter, you will gain a solid understanding of these transformations and learn how to correctly interpret each one. But before we dive into the details, let’s first prepare some example data that we’ll use to illustrate these transformations throughout this chapter. 13.1 Example Data For our example data, we leverage key macroeconomic indicators from the U.S. economy, which we source from the FRED database maintained by the Federal Reserve Bank of St. Louis. These indicators provide critical insights into the economic state of the nation, informing various financial, investment, and policy decisions. The following indicators will be used as examples, where the symbols in parenthesis represent the FRED symbols for downloading the data in R: Gross Domestic Product (GDP) (GDP): This metric denotes the total market value of all the finished goods and services produced in the United States in a specific time period. Unemployment Rate (UNRATE): This measures the percentage of the total U.S. labor force that is unemployed but actively seeking employment and willing to work. GDP Deflator (Aggregate Price Level) (GDPDEF): This is a measure of the price level of all domestically produced, final goods and services in the United States. 3-Month Treasury Bill Rate (TB3MS): This represents the return on investment for the U.S. government’s short-term debt obligation that matures in three months. NASDAQ Composite Index (NASDAQCOM): This encompasses a broad range of the securities listed on the NASDAQ stock market and is typically viewed as an indicator of the performance of technology and growth companies. Population (B230RC0Q173SBEA): This refers to the total number of individuals residing within the United States. We employ the getSymbols() function from the quantmod package in R to access the data. This function enables direct data download as delineated in Chapter ??. To use the getSymbols() function for data download, set the Symbols parameter of the function as \"GDP\" or \"UNRATE\" (the characters enclosed within the parentheses) and the src (source) parameter as \"FRED\", corresponding to the FRED website: # Load quantmod package library(&quot;quantmod&quot;) # Start date start_date &lt;- as.Date(&quot;1971-02-05&quot;) # Download data getSymbols(&quot;GDP&quot;, src = &quot;FRED&quot;, from = start_date) getSymbols(&quot;UNRATE&quot;, src = &quot;FRED&quot;, from = start_date) getSymbols(&quot;GDPDEF&quot;, src = &quot;FRED&quot;, from = start_date) getSymbols(&quot;TB3MS&quot;, src = &quot;FRED&quot;, from = start_date) getSymbols(&quot;NASDAQCOM&quot;, src = &quot;FRED&quot;, from = start_date) getSymbols(&quot;B230RC0Q173SBEA&quot;, src = &quot;FRED&quot;, from = start_date) # De-annualize GDP &amp; use quarterly instead of monthly index GDP &lt;- GDP / 4 index(GDP) &lt;- as.yearqtr(index(GDP)) # Remove missing NASDAQCOM values NASDAQCOM &lt;- na.omit(NASDAQCOM) ## [1] &quot;GDP&quot; ## [1] &quot;UNRATE&quot; ## [1] &quot;GDPDEF&quot; ## [1] &quot;TB3MS&quot; ## [1] &quot;NASDAQCOM&quot; ## [1] &quot;B230RC0Q173SBEA&quot; The code block above, as discussed in Chapter ??, is responsible for data acquisition. The line start_date &lt;- as.Date(\"1971-02-05\") initializes a variable start_date with the date value of February 5, 1971. The function as.Date() converts the string input \"1971-02-05\" into a format that R can interpret and manipulate as a date object. This conversion process is explained in Chapter ??. Within the getSymbols() function, the argument from = start_date stipulates that data should be retrieved starting from the date encapsulated in the start_date variable, which in this instance is from February 5, 1971 onward. The data fetched by this function is returned as xts objects, a time series data format described in Chapter 4.8. The following code block visualizes the data: # Put all plots into one par(mfrow = c(rows = 3, columns = 2), mar = c(b = 2, l = 4, t = 2, r = 1)) # Plot U.S. macro indicators, SA = seasonally adjusted plot.zoo(x = GDP, main = &quot;Gross Domestic Product (GDP)&quot;, xlab = &quot;&quot;, ylab = &quot;Billion-USD, SA&quot;) plot.zoo(x = UNRATE, main = &quot;Unemployment Rate&quot;, xlab = &quot;&quot;, ylab = &quot;Percent, SA&quot;) plot.zoo(x = GDPDEF, main = &quot;GDP Deflator (Aggregate Price Level)&quot;, xlab = &quot;&quot;, ylab = &quot;Index, 2012 = 100, SA&quot;) plot.zoo(x = TB3MS, main = &quot;3-Month Treasury Rate&quot;, xlab = &quot;&quot;, ylab = &quot;Percent&quot;) plot.zoo(x = NASDAQCOM, main = &quot;Nasdaq Composite Index&quot;, xlab = &quot;&quot;, ylab = &quot;Index, Feb 2, 1971 = 100&quot;) plot.zoo(x = B230RC0Q173SBEA, main = &quot;Population&quot;, xlab = &quot;&quot;, ylab = &quot;Thousands&quot;) The above code block uses two main functions: par(): This is a base R function used to set or query graphical parameters. Parameters can be set for the duration of a single R session and influence the visual aspect of your plots. Here, it’s being used in two ways: mfrow = c(rows = 3, columns = 2): This sets up the plotting area as a matrix of 3 rows and 2 columns. This means up to six plots can be displayed on the same plotting area, arranged in three rows and two columns. mar = c(b = 2, l = 4, t = 2, r = 1): This is setting the margin sizes around the plots. The mar parameter takes a numeric vector of length 4, indicating the margin size for the bottom, left, top, and right sides of the plot. The numbers are the line widths for the margins. plot.zoo(): This function comes from the zoo package in R, which is a package for working with ordered observations like time series data such as xts objects. The plot.zoo() function is used to create time series plots. x =: This parameter is used to pass the time series data you want to plot. For example, x = GDP means it’s plotting the GDP data. main =: This is used to set the title of each plot. xlab = \"\" and ylab =: These are used to set the labels for the x and y axes. In this case, the x-axis labels are left blank (\"\"), and the y-axis labels are set to various units of measurement relevant to the data being plotted. In summary, this script is creating a \\(3\\times 2\\)-matrix of time series plots for six different macroeconomic indicators with specific title and y-axis labels, and specific margin sizes around each plot. Figure 13.1: Example Data Figure 13.1 displays the six economic indicators selected to demonstrate the various transformations covered in this chapter. The Gross Domestic Product (GDP) quantifies the U.S. total economic output in billions of U.S. dollars. This GDP series undergoes seasonal adjustment (SA), a transformation discussed in Chapter 13.7.3. The Unemployment Rate (UNRATE), depicted as a percentage, signifies the fraction of the U.S. workforce actively seeking employment but currently unemployed; this series is also seasonally adjusted (SA). The GDP deflator (GDPDEF) measures the aggregate price level in the U.S. and is conveyed as an index number standardized to the average of 2012 Q1 - Q4 equating to 100, which is also seasonally adjusted (SA). See Chapter @ref(index-vs.-absolute-data) for an overview on index measures. The 3-Month Treasury Bill Rate (TB3MS), measured as an annual percentage rate, represents the yield on investments in short-term U.S. government debt obligations. The NASDAQ Composite Index (NASDAQCOM) captures the performance of over 3,000 listed equities on the NASDAQ stock market, reflected through a market capitalization-weighted index value, where the index is normalized to 100 on February 2, 1971. Lastly, Population (B230RC0Q173SBEA) lists the number of people living in the United States, reported in thousands of people. 13.2 Growth 13.2.1 Definition The growth rate or %-change is the relative increase or decrease in a variable from one time period to the next, typically expressed as a percentage: \\[ \\%\\Delta x_t =\\frac{x_t-x_{t-1}}{x_{t-1}} =100 \\left( \\frac{x_t-x_{t-1}}{x_{t-1}} \\right) \\% \\] Here, \\(t\\) represents the current period, and \\(t-1\\) denotes the preceding period. Consider the U.S. GDP growth rate in 2024 Q1 as an example. It’s calculated as follows: \\[ \\begin{aligned} \\text{GDP Growth}_{2024 Q1} = &amp;\\ \\frac{ \\text{GDP}_{2024 Q1} - \\text{GDP}_{2023 Q4} }{ \\text{GDP}_{2023 Q4} } \\\\ = &amp;\\ \\frac{ 7064 \\ \\text{Billion USD} - 6989 \\ \\text{Billion USD} }{ 6989 \\ \\text{Billion USD} } \\\\ = &amp;\\ 1.07 \\% \\end{aligned} \\] This illustrates that growth rates aren’t influenced by the units of measurement: billion USD, a point we’ll delve into later in Chapter 13.2.3. 13.2.2 Log Approximation Growth rates can be approximated using the difference in the natural logarithm of the series: \\[ \\frac{x_t-x_{t-1}}{x_{t-1}} \\approx \\ln(x_{t})- \\ln(x_{t-1}) = 100 \\Big( \\ln(x_{t})- \\ln(x_{t-1}) \\Big) \\% \\] This approximation is accurate as long as growth rates remain small. However, the approximation loses precision when growth rates exceed 20% or -20%. To illustrate this, let’s plot the growth rate of GDP alongside the difference in its log values in percent: # Compute GDP growth in percent and its log approximation GDP_growth &lt;- 100 * (GDP - lag.xts(GDP)) / lag.xts(GDP) GDP_logdiff &lt;- 100 * (log(GDP) - lag.xts(log(GDP))) # Plot the two series plot.zoo(x = merge(GDP_growth, GDP_logdiff), plot.type = &quot;single&quot;, col = c(5, 1), lwd = c(5, 1.5), lty = c(1, 2), xlab = &quot;&quot;, ylab = &quot;%&quot;) legend(&quot;topleft&quot;, legend = c(&quot;Real GDP Growth&quot;, &quot;Log Difference&quot;), col = c(5, 1), lwd = c(5, 1.5), lty = c(1, 2), horiz = TRUE, bty = &#39;n&#39;) This code block computes two different forms of growth for the GDP time series and plots them together for comparison: GDP_growth &lt;- 100 * (GDP - lag.xts(GDP)) / lag.xts(GDP): This line of code computes the percentage growth of the GDP, which is the difference between the current and previous GDP values, divided by the previous GDP value. The lag.xts(GDP) function generates a new series where each data point is shifted one time unit into the future, effectively getting the GDP value from the previous time period. This is then multiplied by 100 to convert it into a percentage. The result is stored in the variable GDP_growth. GDP_logdiff &lt;- 100 * (log(GDP) - lag.xts(log(GDP))): This line of code calculates the log difference of GDP. It first applies a natural logarithm transformation to the GDP (log(GDP)) and then computes the difference with the lagged log-transformed GDP (lag.xts(log(GDP))). This log difference approximates the growth rate when the changes in GDP are relatively small. The result is then multiplied by 100 to convert it into a percentage. The result is stored in the variable GDP_logdiff. plot.zoo(): This function is used to create a time series plot. The x = merge(GDP_growth, GDP_logdiff) argument tells the function to plot the two series together. The plot.type = \"single\" argument makes both series appear on a single plot. The col = c(5, 1) argument sets the color of the two lines (5 for magenta and 1 for black), lwd = c(5, 1.5) sets the line width, and lty = c(1, 2) sets the line type (1 for solid, 2 for dashed). The xlab and ylab arguments are used to label the x and y-axes, respectively. legend(): This function adds a legend to the plot. The \"topleft\" argument places the legend at the top left corner of the plot. The legend = c(\"Real GDP Growth\", \"Log Difference\") argument specifies the names of the series. The remaining arguments (col, lwd, lty, horiz, and bty) set the color, line width, line type, horizontal layout, and box type of the legend, respectively. Figure 13.2: GDP Growth and Log Approximation Figure 13.2 generated from this code block shows the real GDP growth and the log difference of GDP over time, providing a visual comparison of these two methods of computing growth. As quarterly GDP growth in the U.S. has remained relatively stable, the log approximation aligns almost perfectly with the actual growth rate. 13.2.3 Relativity Growth rates eliminate the units of measurement, making it easier to compare variables of different scales or units. When we examine economic performance using GDP growth, for example, we don’t need to standardize each country’s GDP into a single currency. This is because growth rates focus on the proportional changes, not the absolute values. Thus, regardless of whether the GDP is measured in US dollars, Euros, or any other currency, the GDP growth rate can be directly compared across countries. Additionally, growth rates provide a relative measure, measuring the change with respect to an initial level. This feature makes it an effective tool for comparing economies of different sizes. For instance, a small economy might have a lower absolute GDP than a larger one, but it could be growing at a significantly faster pace. Therefore, using growth rates allows us to observe this relative performance more clearly, irrespective of the initial level of GDP. 13.2.4 Return and Inflation Note that when the concept of “growth rate” is applied to prices, it’s often referred to as “return” or “inflation.” The choice of terminology depends on the context. In the financial world, when we discuss the percentage increase in the price of a financial asset such as a stock or a bond over time, we usually refer to the growth rate as a return. This usage reflects the perspective of investors who buy assets with the expectation that their value will increase over time, thus yielding a positive return on their investment. The term inflation refers to the overall increase in prices in an economy. When economists calculate the rate at which the general level of prices for goods and services is rising, and subsequently, the purchasing power of currency is falling, they refer to this as the inflation rate. Despite this terminology, both return and inflation fundamentally represent a form of growth rate, but applied in different contexts. Now, let’s compute and plot the growth rates of the prices from the example data using their log approximation: # Compute growth rates of prices using log approximation Inflation &lt;- 100 * (log(GDPDEF) - lag.xts(log(GDPDEF))) Nasdaq_return &lt;- 100 * (log(NASDAQCOM) - lag.xts(log(NASDAQCOM))) # Put all plots into one par(mfrow = c(rows = 2, columns = 1), mar = c(b = 2, l = 4, t = 2, r = 1)) # Plot growth rates of prices plot.zoo(x = Inflation, main = &quot;U.S. Inflation&quot;, xlab = &quot;&quot;, ylab = &quot;%&quot;) plot.zoo(x = Nasdaq_return, main = &quot;Nasdaq Stock Market Return&quot;, xlab = &quot;&quot;, ylab = &quot;%&quot;) The functions used in the above code chunk have been explained earlier in Chapters 13.2.1 and 13.2.2. Figure 13.3: Growth Rate of Prices Figure 13.3 is the output of this code block. It displays the growth rate of two price indices: the GDP deflator and the Nasdaq composite index. Since the GDP deflator measures overall price changes in an economy, its growth rate is referred to as inflation. Since the Nasdaq indices represent assets that can be invested in, the growth rates of these prices are termed as returns. 13.2.5 Applicability Not all data are suitable for transformation into growth rates. Growth rates are most meaningful for ratio scale variables, which measure quantities that have a clear, absolute zero and uniform intervals between numbers (see Chapter 12.3 for an overview on nominal, ordinal, interval, and ratio scale variables). In such cases, growth rates offer insights about the pace of a variable’s change over time. This is why we commonly see growth rates for economic output (GDP), prices, population, etc., quantities for which it makes sense to ask, “By what percentage did this quantity change?” Conversely, some variables, often percentages or rates themselves, are not meaningful to describe in terms of growth rates. Their movements over time are better described in terms of differentiation. This includes variables like unemployment rates, interest rates, or inflation rates. Specifically, if variables exhibit one or more of the following characteristics, computing growth rates may lead to more confusion than insight: They are expressed as a ratio or percentage. They don’t have a meaningful absolute zero. The difference between two values isn’t meaningful. The ratio of two values is usually not meaningful or not interpreted. This is a general guideline, and exceptions certainly exist, particularly when delving into specialized domains or specific use cases. Since the U.S. unemployment rate and the 3-month Treasury bill (T-bill) rate from the example data are expressed in percentages, their growth rates are not calculated here. However, we will compute their first differences in Chapter 13.3. Interestingly, the 3-month T-bill rate is, in fact, already a growth rate: the values in the time series represent the (annualized) returns of holding a 3-month T-bill until maturity at different dates. 13.3 Differentiation 13.3.1 Definition The first difference signifies the variation in a variable from one time point to the subsequent one: \\[ \\Delta x_t =x_t-x_{t-1} \\] In this equation, \\(t\\) symbolizes the current period, while \\(t-1\\) signifies the preceding one. Take the first difference of U.S. GDP in 2024 Q1 for instance. It is computed as follows: \\[ \\begin{aligned} \\Delta \\text{GDP}_{2024 Q1} = &amp;\\ \\text{GDP}_{2024 Q1} - \\text{GDP}_{2023 Q4} \\\\ = &amp;\\ 7064 \\ \\text{Billion USD} - 6989 \\ \\text{Billion USD} \\\\ = &amp;\\ 74.73 \\ \\text{Billion USD} \\end{aligned} \\] This shows that in contrast with growth rates, differentiation does not remove the units of measurement, with the first difference still presented in billion USD. This could pose some challenges when the underlying unit is a percentage, as outlined in Chapter 13.3.2. The second difference is the variation of the first difference: \\[ \\Delta^2 x_t =\\Delta x_t-\\Delta x_{t-1} \\] and the \\(k\\)th difference is the difference of the \\((k-1)\\)th difference: \\[ \\Delta^k x_t =\\Delta^{k-1} x_t-\\Delta^{k-1} x_{t-1} \\] Differentiation beyond the second difference is not commonly observed in Economics and Finance. Differentiation is frequently employed to eliminate trends in data to concentrate on business cycles. 13.3.2 Percentage Points (pp) As noted, the units of measurements do not disappear when differentiating. This could create confusion when the underlying unit of measurement is a percentage, for example, when obtaining the first difference of an unemployment rate. This confusion occurs because if the unemployment rate increases by 5%, it implies that the growth is 5%, rather than the difference being 5%. That’s why the term percentage points was devised, to avert this confusion. Adding “points” to “percentage” clarifies that it is not 5% growth but rather the first difference is 5%. Formally, a percent change (%-change) is a relative measure, demonstrating the change in one value relative to its initial value (see Chapter 13.2): \\[ \\% \\Delta Y_t =100 \\left( \\frac{Y_t-Y_{t-1}}{Y_{t-1}} \\right) \\% =100 \\left( \\frac{5\\%-4\\%}{4\\%} \\right) \\% =25\\% \\] while percentage point change (pp-change) is an absolute measure, reflecting the difference between two percentage values: \\[ \\Delta Y_t =Y_t-Y_{t-1} =5\\%-4\\%=1pp \\] In this context, \\(Y_t\\) is a measure represented in percentages, like the unemployment rate, inflation, GDP growth, stock return, or interest rate. The example illustrates that moving from 4% to 5% is a 25% increase, but only a 1pp increase; therefore, confusing these two units of measurements can cause significant errors. To illustrate, let’s calculate both the %-change and pp-change of the unemployment rate from the example-data, and compare the two measures. Note, however, that as per Chapter 13.2, it is unusual to compute growth rates (%-change) of measures expressed in percent; hence, when writing about %-change of unemployment, the reader will be confused, and likely believe you mixed up %-change with pp-change; thus, I do not recommend actually calculating the %-change of the unemployment rate in practice. This is purely for illustrative purposes: # Compute %-change of unemployment rate UNRATE_percent_pp &lt;- 100 * (UNRATE - lag.xts(UNRATE)) / UNRATE names(UNRATE_percent_pp) &lt;- &quot;percent&quot; # Compute pp-change of unemployment rate UNRATE_percent_pp$pp &lt;- UNRATE - lag.xts(UNRATE) # Put all plots into one par(mfrow = c(rows = 2, columns = 1), mar = c(b = 2, l = 4, t = 2, r = 1)) # Plot %-change and pp-change of unemployment rate plot.zoo(x = UNRATE_percent_pp, plot.type = &quot;single&quot;, xlab = &quot;&quot;, ylab = &quot;% vs. pp&quot;, main = &quot;Change in Unemployment Rate&quot;, ylim = c(-22, 22), col = c(5, 1), lwd = c(1.5, 1)) legend(x = &quot;topleft&quot;, legend = c(&quot;%-change&quot;, &quot;pp-change&quot;), col = c(5, 1), lwd = c(1.5, 1), horiz = TRUE) # Zoom in using the ylim-input plot.zoo(x = UNRATE_percent_pp, plot.type = &quot;single&quot;, xlab = &quot;&quot;, ylab = &quot;% vs. pp&quot;, main = &quot;(Zoomed In)&quot;, ylim = c(-2.2, 2.2), col = c(5, 1), lwd = c(1.5, 1)) legend(x = &quot;topleft&quot;, legend = c(&quot;%-change&quot;, &quot;pp-change&quot;), col = c(5, 1), lwd = c(1.5, 1), horiz = TRUE) The functions used in the above code chunk have been explained earlier in Chapters 13.2.1 and 13.2.2. What we haven’t used before is the ylim input, here ylim = c(-22, 22), which defines the limits of the y-axis and allows to zoom in. Note that the xlim input does the same for limiting the x-axis. Figure 13.4: Percent vs. Percentage Point Change of Unemployment Rate Figure 13.4 plots the %-change and pp-change of the unemployment rate. The variance of the %-change is significantly larger than that of the pp-change because the unemployment rates are one or two-digit numbers, so an increase results in a larger relative increase than an absolute increase. Note that if the underlying time series were three or higher digit numbers, it would be reversed: the variance of the pp-change would be larger than that of the %-change. 13.3.3 Basis Points (bp) Changes in economic and financial data, particularly at a high frequency, often appear in small increments. For instance, monthly changes in interest rates often involve subtle shifts like 0.04pp or -0.24pp. For clarity and ease of reading, these changes are commonly expressed in basis points (bp) rather than percentage points (pp), with 1pp equivalent to 100bp. To formalize, a basis point (bp) change is calculated by multiplying the difference between two percentage values by 100: \\[ \\Delta Y_t = 100 \\left(Y_t-Y_{t-1}\\right) =5\\%-4\\%=1pp=100bp \\] In this formula, \\(Y_t\\) represents a measure in percentages, like stock returns or interest rates. Now, let’s compute the basis point change for the 3-month Treasury bill rate and visualize it: # Calculate basis point (bp) change in 3-month T-bill rate dTB3MS_bp &lt;- 100 * (TB3MS - lag.xts(TB3MS)) # Plot the time series plot.zoo(x = dTB3MS_bp[&quot;1990/&quot;], plot.type = &quot;single&quot;, xlab = &quot;&quot;, ylab = &quot;Basis Point (bp)&quot;, ylim = c(-150, 150), main = &quot;Change in 3-Month Treasury Bill Rate&quot;) Figure 13.5: Basis Point Changes in 3-Month Treasury Bill Rate As shown in Figure 13.5, the bp-change of the 3-month T-bill rate demonstrates that U.S. interest rates typically exhibit gradual increases during economic boom periods and sharp declines during recessions, leading to larger negative changes. 13.4 Natural Logarithm 13.4.1 Definition The natural logarithm, often written as \\(\\ln(x)\\), \\(\\log_e(x)\\), or \\(\\log(x)\\), is the inverse operation to exponentiation with base \\(e\\), where \\(e\\) is the natural base or Euler’s number and approximately equal to \\(2.7182818\\). In other words, if \\(y = e^x\\), then \\(x = \\ln(y)\\), which defines the natural logarithm. This relationship can be expressed as: \\[ \\ln(e^x) = x \\] and \\[ e^{\\ln(x)} = x \\] These equations hold true for any positive number \\(x\\). The natural logarithm function \\(\\ln(x)\\) is the power to which \\(e\\) must be raised to get \\(x\\). Here’s an example of how you can calculate the natural logarithm in R: # Calculate the natural logarithm of e (which should be 1) log(exp(1)) ## [1] 1 And here’s how you could calculate the natural logarithm of an array of numbers: # Create an array of numbers numbers &lt;- c(1, 2, 3, 4, 5) # Calculate the natural logarithm of the array of numbers log(numbers) ## [1] 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379 Now, consider trying to find the natural logarithm of zero, \\(\\ln(0)\\). Is there a power to which you can raise \\(e\\) to get zero? The answer is no. Even if we raise \\(e\\) to a very large negative power, the result approaches zero, but never quite gets there. This is why \\(\\ln(0)\\) is considered undefined or negative infinity if such number is defined: # Calculate the natural logarithm of zero log(0) ## [1] -Inf Next, consider trying to find the natural logarithm of a negative number. The constant \\(e\\) raised to any real number always results in a positive value. Therefore, there’s no real number that you can raise \\(e\\) to that would result in a negative number. Because of this, the natural logarithm of a negative number is considered undefined when dealing with real numbers: # Calculate the natural logarithm of a negative number log(-5) ## Warning in log(-5): NaNs produced ## [1] NaN The natural logarithm has many useful properties. For example, one such property is that the natural logarithm of a product equals the sum of the natural logarithms of each individual number: \\[ \\ln(ab) = \\ln(a) + \\ln(b) \\] # ln(ab) = ln(a) + ln(b) log(3 * 4) log(3) + log(4) ## [1] 2.484907 ## [1] 2.484907 Another property is that the natural logarithm of a number raised to a power is the product of the power and the natural logarithm of the number: \\[ \\ln(a^n) = n \\ln(a) \\] # ln(a^n) = n * ln(a) log(3^4) 4 * log(3) ## [1] 4.394449 ## [1] 4.394449 These unique properties of natural logarithm make it particularly valuable in simplifying mathematical expressions and solving exponential and logarithmic equations. 13.4.2 Motivation Logarithmic measures aid in making the visualization and interpretation of exponentially growing data simpler. For instance, stock prices, which often exhibit exponential growth, can make long-term historical data appear flat when graphed. Using a logarithmic transformation can convert this exponential growth into linear growth, thus making patterns and shifts over time more evident. Logarithmic measures are also easy to interpret as changes in the log approximate the growth rate (as discussed in Chapter 13.2.2). To illustrate this, let’s graph the Nasdaq composite index from the example-data and its natural log: # Calculate natural logarithm of Nasdaq composite index Nasdaq_log &lt;- log(NASDAQCOM) # Set up subplots par(mfrow = c(rows = 2, columns = 1), mar = c(b = 2, l = 4, t = 2, r = 1)) # Plot original time series plot.zoo(x = NASDAQCOM, xlab = &quot;&quot;, ylab = &quot;Index, Feb 2, 1971 = 100&quot;, main = &quot;Nasdaq Composite Index&quot;) # Plot natural log of time series plot.zoo(x = Nasdaq_log, xlab = &quot;&quot;, ylab = &quot;Log&quot;, main = &quot;Log of Nasdaq Composite Index&quot;) Figure 13.6: Log Transformation of Nasdaq Composite Index Figure 13.6 shows that the Nasdaq stock market index appears stagnant in the early sample. However, the logarithmic transformation uncovers significant growth and contraction periods throughout the sample, highlighting the changes in the earlier years. This is because stock prices often demonstrate exponential growth, with value increasing proportional to the current value. Therefore, even substantial %-changes early in a stock’s history may appear small due to a lower starting price. In contrast, smaller %-changes later on appear large because they started from a higher price. This can distort how we view a stock’s past ups and downs. The second panel of Figure 13.6 presents the natural logarithm of the Nasdaq stock market index. This transformation helps visualize the exponential growth in stock prices as a linear increase, with the slope of the line representing the growth rate. Crucially, changes in the log of the price correspond directly to percentage changes in the price. For instance, a 0.01 increase in the log price equates to a 1% rise in the original price (see Chapter 13.2.2). This characteristic enables a direct comparison of price changes over time, aiding in the interpretation of these shifts in terms of relative growth or contraction. You might wonder why we plot the log of a series, which interprets change as growth rate, instead of directly plotting the growth rate. This is because the natural logarithm of a variable often offers more insight into long-term trends than the growth rate itself. To understand this better, use the fact that changes in the log approximate the growth rate. As a result, the log series embodies the cumulated growth rates – the sum of all growth rates from the beginning of the dataset to the present. When the growth rate fluctuates extensively, it can be tough to discern whether positive or negative growth rates predominate over time. However, when you plot the logarithm, these growth rates are cumulative, making it clear whether a variable is growing or not. Let’s compare the growth rate and the natural logarithm of the Nasdaq composite index from the example-data to demonstrate this: # Calculate growth rate of Nasdaq composite index Nasdaq_return &lt;- 100 * (log(NASDAQCOM) - lag.xts(log(NASDAQCOM))) # Set up subplots par(mfrow = c(rows = 2, columns = 1), mar = c(b = 2, l = 4, t = 2, r = 1)) # Plot growth rate plot.zoo(x = Nasdaq_return, xlab = &quot;&quot;, ylab = &quot;%&quot;, main = &quot;Nasdaq Stock Market Return (Growth Rate)&quot;) # Plot natural log plot.zoo(x = Nasdaq_log, xlab = &quot;&quot;, ylab = &quot;Log&quot;, main = &quot;Log of Nasdaq Composite Index (Cumulated Growth Rate)&quot;) Figure 13.7: Growth vs. Log of Nasdaq Composite Index The first panel of figure 13.7 shows the highly volatile stock market returns (growth rate), making it difficult to assess overall performance. The second panel, on the other hand, depicts the log transformation of the Nasdaq stock market index. This transformation accumulates the stock market returns from the first panel from the beginning to the current period on the x-axis. It clearly shows whether stock prices grow or contract over time, providing a more reliable perspective on the market’s performance. 13.4.3 Applicability Not all data are suitable for transformation into natural logarithm. Essentially, if the concept of a growth rate is nonsensical or irrelevant for the data at hand, then it would also be inappropriate to compute its natural logarithm. This is because the natural logarithm essentially represents cumulative growth rates, and therefore presupposes that a concept of ‘growth’ is meaningful for the data in question. Specific cases where the application of the growth rate may or may not be appropriate are discussed in Chapter 13.2.5. 13.5 Ratio 13.5.1 Definition A ratio is a mathematical expression that represents the relationship between two quantities or variables, represented as a fraction. It’s often used to provide insights into relative magnitudes or proportions between the two variables. Ratio measures can be calculated as follows: \\[ \\text{Ratio Measure} = \\frac{\\text{Variable 1}}{\\text{Variable 2}} \\] Ratio measures can provide useful insights into how the two variables relate to each other. For instance, a company’s debt-to-equity ratio compares its total debt to its total equity, offering a sense of the company’s financial leverage. A high debt-to-equity ratio might indicate a risky financial situation, whereas a lower ratio may suggest a more stable financial position. In the financial sector, another common ratio measure is the price-to-earnings (P/E) ratio. This ratio compares a company’s stock price to its earnings per share, giving investors a sense of how much they’re paying for each dollar of earnings. It’s important to clarify that the term “ratio” defined in this context is not to be confused with “ratio scale” variables defined in Chapter 12.3.4. The “ratio” in the current context refers to the mathematical comparison of two quantities, “a to b”. On the other hand, a “ratio scale” variable refers to a type of variable where the difference between any two values is meaningful and it has a true zero point. This means that zero indicates the absence of the quantity being measured, and ratios between numbers on the scale are meaningful. Therefore, despite sharing a common term, these two concepts have fundamental differences. However, calculating a ratio measure only makes sense if the underlying variables are indeed ratio scale variables. 13.5.2 Per Capita Per capita measures are a specific type of ratio measure that calculate the average value of a particular variable per person within a given population. These measures are typically obtained by dividing a macroeconomic variable, such as Gross Domestic Product (GDP) or income, by the population size of a region: \\[ \\text{Per Capita Measure} = \\frac{\\text{Variable}}{\\text{Population Size}} \\] Per capita measures play a significant role in economics and finance. One prominent example is GDP per capita, which calculates the average economic output per person in a country. This measure enables meaningful comparisons of the average standard of living and economic well-being across nations, regardless of population size. It provides insights into the relative well-being of individuals within different countries. In the field of finance, per capita measures are also valuable. For instance, credit card debt per capita offers insights into the average debt burden carried by individuals in a specific region. By dividing the total credit card debt by the population, analysts can assess the average level of indebtedness and make comparisons across different areas. This measure helps in understanding the credit behavior and financial health of individuals within various regions. To calculate GDP per capita in the U.S., we divide the GDP series (GDP) of the example-data by the population (B230RC0Q173SBEA) of the country: # Compute GDP per capita GDP_per_capita &lt;- ((10^9) * GDP) / (1000 * B230RC0Q173SBEA) For example, U.S. GDP per capita in 2024 Q1 is calculated as follows: \\[ \\begin{aligned} \\text{GDP Per Capita}_{2024 Q1} = &amp;\\ \\frac{ \\text{GDP}_{2024 Q1} }{ \\text{Population}_{2024 Q1} } \\\\ = &amp;\\ \\frac{ 7,064 \\ \\text{Billion USD} }{ 336,308 \\ \\text{Thousands} } \\\\ = &amp;\\ \\frac{ 7,063,982,000,000 \\ \\text{USD} }{ 336,308,000 } \\\\ = &amp;\\ 21,004.50 \\ \\text{USD} \\end{aligned} \\] This division yields the average economic output per person in USD. Therefore, in the U.S., the average income per person was 21,004.50 USD in 2024 Q1, taking into account all sources of income, including wages, dividends, and profits. Next, let’s visualize GDP per capita: # Plot GDP per capita plot.zoo(x = GDP_per_capita, xlab = &quot;Date&quot;, ylab = &quot;USD&quot;, main = &quot;GDP Per Capita in the United States&quot;) Figure 13.8: GDP Per Capita in the United States Figure 13.8 illustrates GDP per capita in the United States, representing the average income per person in USD over time. The plot shows a consistent increase in average income, indicating economic growth. However, it’s important to note that prices have also steadily increased during this period. Therefore, to accurately evaluate changes in the standard of living, it becomes necessary to consider these price changes. In the next chapter, we will delve into the concept of real measures, which account for such changes and provide a more accurate representation of living standards. 13.5.3 Real vs. Nominal Real measures are ratio measures that adjust for changes in prices by dividing the variable by the price index. In contrast, nominal measures are the counterparts of real measures, remaining unadjusted for inflation: \\[ \\text{Real Measure} = 100 \\left(\\frac{\\text{Nominal Measure}}{\\text{Price Index}} \\right) \\] In this equation, the ratio is multiplied by 100 due to the normalization of price indices at 100 during a specific period. For example, if the price index is normalized at 100 in 2012, then the real measure is interpreted as the nominal measure in terms of 2012 prices. Real variables measure the physical quantity of goods and services by accounting for changes in the price level. They provide a more accurate representation of economic trends. Examples include real GDP, real income, and real wages. These measures eliminate the impact of price changes, enabling comparisons of economic performance over time or across regions. On the other hand, nominal measures, which are not adjusted for price changes, represent the monetary value of goods and services at current prices. Hence, unlike real values that adjust for inflation, nominal values do not. It’s essential to distinguish between the “nominal measure” discussed here and the “nominal scale” variables outlined in Chapter 12.3.1. In this context, “nominal measure” refers to values not adjusted for price changes, whereas “nominal scale” variables are a type of categorical data where different categories do not indicate any order or hierarchy. To calculate real GDP in the U.S., the GDP series (GDP) of the example-data is divided by a price index - specifically, the U.S. GDP deflator index (GDPDEF): # Compute real GDP RGDP &lt;- 100 * GDP / GDPDEF The resulting data is returned as an xts object. You can compare the real GDP measure RGDP with the nominal measure GDP, which is the GDP measure unadjusted for price changes: # Merge nominal and real GDP GDP_nominal_real &lt;- merge(GDP, RGDP) # Print merged data tail(GDP_nominal_real) ## GDP GDP.1 ## 2022 Q4 6602.101 5497.490 ## 2023 Q1 6703.400 5528.076 ## 2023 Q2 6765.753 5556.356 ## 2023 Q3 6902.532 5622.694 ## 2023 Q4 6989.249 5669.825 ## 2024 Q1 7063.982 5687.449 Visualize the two GDP series using the plot.zoo() function: # Plot nominal vs. real GDP plot.zoo(x = GDP_nominal_real, plot.type = &quot;single&quot;, col = c(1, 2), lwd = 2, lty = c(1, 2), main = &quot;Nominal vs. Real GDP in the United States&quot;, xlab = &quot;Date&quot;, ylab = &quot;Billions of USD&quot;) legend(x = &quot;topleft&quot;, legend = c(&quot;Nominal GDP&quot;, &quot;Real GDP&quot;), col = c(1, 2), lwd = 2, lty = c(1, 2)) Figure 13.9: Nominal vs. Real GDP in the United States Figure 13.9 displayes U.S. GDP for each quarter since 1947, contrasting the nominal and real measures. In 2012, nominal and real GDP align because the real GDP is defined in terms of 2012 prices, and the nominal GDP (measured in current prices) is also based on 2012 prices for that year. The fact that nominal GDP has grown more than real GDP reflects the influence of inflation, which causes nominal GDP to inflate while real GDP provides a more accurate measure of economic growth. In summary, nominal GDP quantifies the total value of goods and services produced within an economy, calculated using the market prices at the time of measurement. As such, if prices increase due to inflation, nominal GDP might rise even without an actual increase in goods and services produced. Conversely, real GDP adjusts for inflation and represents economic output in terms of “constant prices” from a chosen base year - like calculating today’s economic output as if prices had remained at their 2012 levels. This inflation-adjusted measure enables more accurate comparisons of economic growth over different periods. 13.5.4 Growth of Ratios The growth rate of a ratio can be calculated based on the growth rate of the variables that form it. Define \\(x_t = y_t/p_t\\) as the ratio of interest (e.g., real GDP), \\(y_t\\) as the numerator (e.g., nominal GDP), and \\(p_t\\) as the denominator (e.g., the price level). The growth rate of the ratio can then be formulated as follows: \\[ \\begin{aligned} \\%\\Delta x_{t} = &amp;\\ \\frac{x_{t} -x_{t-1}}{x_{t-1}} =\\frac{\\frac{y_{t}}{p_{t}} -\\frac{y_{t-1}}{p_{t-1}}}{\\frac{y_{t-1}}{p_{t-1}}} =\\frac{p_{t-1}}{p_{t}}\\frac{y_{t}}{y_{t-1}} -1=\\frac{1+\\%\\Delta y_{t}}{1+\\%\\Delta p_{t}} -1 \\end{aligned} \\] This formula links the growth rate of the ratio measure \\(\\%\\Delta x_t\\) with the growth rates of the constituent variables \\(\\%\\Delta y_t\\) and \\(\\%\\Delta p_t\\). Consider the case of real versus nominal growth, yielding the following equation: \\[ \\begin{aligned} 1+\\text{Real Growth}_{t} = &amp;\\ \\frac{1+\\text{Nominal Growth}_{t}}{1+\\text{Inflation}_{t}} \\end{aligned} \\] Another approach to approximate the growth rate of a ratio is through the use of logarithms: \\[ \\begin{aligned} \\%\\Delta x_{t} \\approx &amp;\\ \\ln( x_{t}) -\\ln( x_{t-1})\\\\ = &amp;\\ \\ln\\left(\\frac{y_{t}}{p_{t}}\\right) -\\ln\\left(\\frac{y_{t-1}}{p_{t-1}}\\right)\\\\ = &amp;\\ \\Bigl(\\ln( y_{t}) -\\ln( y_{t-1})\\Bigr) -\\Bigl(\\ln( p_{t}) -\\ln( p_{t-1})\\Bigr)\\\\ = &amp;\\ \\%\\Delta y_{t} -\\%\\Delta p_{t} \\end{aligned} \\] The growth rate of the ratio measure is approximately the difference between the growth rates of the two underlying variables. In terms of real growth, this yields the following approximation: \\[ \\begin{aligned} \\text{Real Growth}_{t} \\approx &amp;\\ \\text{Nominal Growth}_{t} -\\text{Inflation}_{t} \\end{aligned} \\] To illustrate, consider the interest rate as a nominal growth variable. The interest rate is a growth rate when it serves as a discount rate or yield to maturity, as illustrated by the 3-month Treasury bill data from the example-data section, which captures the total return on a security over a year. However, if the interest rate is understood as a coupon rate that doesn’t account for capital gain, then the interest rate is not a growth rate. The above formula can then be used to calculate the (ex-post) real interest rate, representing the return on the security in real terms: \\[ r_t \\approx i_t - \\pi_t \\] Where \\(r_t\\) is the real interest rate, \\(i_t\\) is the (nominal) interest rate, and \\(\\pi_t\\) is the inflation rate. The real interest rate is essentially the quantity of goods and services one could purchase in a year by holding this security for that duration. Applying this to data: # Compute ex post real interest rates Inflation_annual_rate &lt;- 4 * Inflation EXPOSTREAL &lt;- TB3MS - Inflation_annual_rate # Plot ex post real interest rate plot.zoo(x = na.omit(merge(TB3MS, Inflation_annual_rate, EXPOSTREAL)), plot.type = &quot;single&quot;, ylim = c(-16, 16), lwd = c(4, 2, 1.5), lty = c(1, 3, 1), col = c(5, 2, 1), ylab = &quot;&quot;, xlab = &quot;&quot;, main = &quot;Real 3-Month Treasury Bill Rate&quot;) legend(x = &quot;bottomleft&quot;, legend = c(&quot;Nominal Rate&quot;, &quot;Inflation&quot;, &quot;Real Rate&quot;), lwd = c(4, 2, 1.5), lty = c(1, 3, 1), col = c(5, 2, 1), bty = &#39;n&#39;, horiz = TRUE) abline(h = 0, lty = 2) Figure 13.10: Real 3-Month Treasury Bill Rate Figure 13.10 shows that real and nominal rates typically move in tandem, but not always. For example, in the 1970s, even though the nominal rates in the U.S. were high, the real rates were surprisingly low, and sometimes even negative. If you just looked at the high nominal rates, you might think it was tough to get credit because it was costly to borrow. However, the negative real rates tell us that this was not actually the case. 13.6 Gap A gap is a term that refers to the difference between two variables. You can calculate a gap measure as follows: \\[ \\text{Gap Measure} = \\text{Variable 1} - \\text{Variable 2} \\] This gap measure might be referred to by different names, depending on the context. For example, in Finance, the term spread is used to indicate the gap between two interest rates. In the field of Economics, the term disparity is often used to describe the gap between the highest and lowest income levels within a population or group, while the gap between revenues and expenditures is known as net revenue. 13.7 Filtering Filtering techniques play a vital role in the fields of economics and finance by helping to extract meaningful insights from noisy data. They are primarily used to isolate certain components of time-series data such as trends, cycles, or seasonal patterns. This chapter will introduce and delve into the concepts of filtering specific frequencies, detrending, and seasonal adjustment. 13.7.1 Frequency Filtering In some instances, analysts are interested in isolating components of a time series that oscillate at specific frequencies. For example, business cycle analysis often focuses on fluctuations that recur every 2 to 8 years. To isolate these components, one can apply filters that attenuate (reduce the amplitude of) frequencies outside this range. A well-known filter in economics is the band-pass filter, which only allows frequencies within a certain band to pass through. The Baxter-King filter, for instance, is a widely used band-pass filter in economics. Another popular approach is the wavelet transform, which can provide a time-varying view of different frequencies. 13.7.2 Detrending Detrending is another important tool for analyzing time series data. In many economic series, there is often a long-term trend or direction in which the series is headed, such as the general upward trend of GDP or stock market indices. Detrending is the process of removing this underlying trend to study the cyclical and irregular components of the series. Several methods exist for detrending data, from simple approaches such as subtracting the linear trend estimated by least squares, to more sophisticated methods like the Hodrick-Prescott (HP) filter, which estimates and removes a smooth trend component. 13.7.3 Seasonal Adjustment In many economic time series, patterns tend to repeat at regular intervals. This repetition is referred to as seasonality. Examples include increased retail sales during the holiday season, or fluctuations in employment rates due to seasonal industries. Seasonal adjustment is the process of removing these recurring patterns to better understand the underlying trend and cyclical behavior of the series. Methods for seasonal adjustment include moving-average methods, like the Census Bureau’s X-13ARIMA-SEATS, and model-based methods such as the popular STL (Seasonal and Trend decomposition using Loess) procedure. By implementing these techniques, researchers and policymakers can make more accurate comparisons over time and across different series, free from the distortion of seasonal effects. For instance, consider seasonally adjusted (SA) GDP. It is a modified GDP measure that eliminates seasonal variation effects like holidays, weather patterns, or specific events. These adjustments allow economists to focus on analyzing underlying economic trends and business cycles. For example, in a seasonally adjusted GDP series, a significant increase in output from Q3 to Q4 can be attributed to actual economic growth rather than the higher consumption typically associated with holiday seasons. Seasonally adjusted GDP data can be obtained from various sources, including government statistical agencies or international organizations. In the United States, the Bureau of Economic Analysis (BEA) provides seasonally adjusted GDP data, which can be accessed through the BEA website or economic data platforms like FRED. In R, there are packages available that perform such seasonal adjustment. The stats package offers functions such as decompose() and stl() for seasonal decomposition and filtering, while the seasonal package provides tools for estimating and removing seasonal components from time series data. These packages enable users to perform seasonal adjustment by analyzing the time series patterns of the same series or incorporating information from related series, such as export or import data. However, it is recommended to first check for pre-existing seasonally adjusted data available from the BEA or other reliable sources before attempting to create your own adjustments, as they may have access to more advanced tools and methodologies. In the example data, the GDP series (GDP) is already seasonally adjusted, the common default option, as most economists focus more on business cycles than seasonal cycles. To access non-seasonally adjusted U.S. GDP data, visit FRED and search “U.S. GDP”. The first suggestion would be “Gross Domestic Product” subtitled “Billions of Dollars, Quarterly, Seasonally Adjusted Annual Rate”. Although this is the correct variable, we require the original, not seasonally adjusted (NSA) GDP series. To retrieve this, click on “6 other formats”, and select “Quarterly, Millions of Dollars, Not Seasonally Adjusted”. The resulting graph will be titled “Gross Domestic Product (NA000334Q)”. To download the data, use the getSymbols() function with Symbols parameter as \"NA000334Q\" and the src (source) parameter as \"FRED\": # Download GDP: Quarterly, Millions of Dollars, Not Seasonally Adjusted getSymbols(&quot;NA000334Q&quot;, src = &quot;FRED&quot;) ## [1] &quot;NA000334Q&quot; After rescaling the non-seasonally adjusted GDP to billions, we can compare the seasonally adjusted (SA) GDP measure GDP with the non-seasonally adjusted (NSA) measure NA000334Q: # Rescale the not seasonally adjusted GDP to billions GDP_nsa &lt;- NA000334Q / 1000 # Merge seasonally adjusted and not seasonally adjusted GDP GDP_sa_nsa &lt;- merge(GDP, GDP_nsa) # Print merged data tail(GDP_sa_nsa) ## GDP NA000334Q ## 2022 Q4 6602.101 6701.519 ## 2023 Q1 6703.400 6546.655 ## 2023 Q2 6765.753 6802.375 ## 2023 Q3 6902.532 6927.581 ## 2023 Q4 6989.249 7081.776 ## 2024 Q1 7063.982 6918.699 Visualization of both GDP series: # Plot U.S. seasonally adjusted vs. not seasonally adjusted GDP plot.zoo(x = GDP_sa_nsa[&quot;1970/&quot;], plot.type = &quot;single&quot;, col = c(5, 1), lwd = c(4, 1), main = &quot;SA vs. NSA U.S. GDP&quot;, xlab = &quot;Date&quot;, ylab = &quot;Billions of USD&quot;) legend(x = &quot;topleft&quot;, legend = c(&quot;Seasonally Adjusted (SA)&quot;, &quot;Not Seasonally Adjusted (NSA)&quot;), col = c(5, 1), lwd = c(4, 1)) Figure 13.11: SA vs. NSA U.S. GDP Figure 13.11 displays U.S. GDP for each quarter since 1970, comparing the seasonally adjusted with the not seasonally adjusted measure. The seasonally adjusted GDP series removes the effects of seasonal variations, allowing for a clearer analysis of underlying trends and business cycles. On the other hand, the not seasonally adjusted GDP series reflects the raw, unadjusted data and includes the impact of seasonal variations. This series provides a more detailed view of the quarterly fluctuations in economic activity, which can be influenced by factors like holiday spending or seasonal industries. "],["statistical-measures.html", "Chapter 14 Statistical Measures", " Chapter 14 Statistical Measures This chapter focuses on the statistical measures used to analyze economic and financial data. Content Coming Soon. "],["data-aggregation.html", "Chapter 15 Data Aggregation 15.1 Annualization 15.2 Year over Year (YoY)", " Chapter 15 Data Aggregation 15.1 Annualization 15.1.1 Motivation Economic indicators are often measured at varying periods - hourly, daily, weekly, monthly, quarterly, or yearly. For example, the U.S. macro indicators in our example data have different periodicities: # Extract periodicity of example data periodicity(GDP) ## Quarterly periodicity from 1971 Q2 to 2024 Q1 periodicity(UNRATE) ## Monthly periodicity from 1971-03-01 to 2024-04-01 periodicity(GDPDEF) ## Quarterly periodicity from 1971-04-01 to 2024-01-01 periodicity(TB3MS) ## Monthly periodicity from 1971-03-01 to 2024-04-01 periodicity(NASDAQCOM) ## Daily periodicity from 1971-02-05 to 2024-05-30 periodicity(B230RC0Q173SBEA) ## Quarterly periodicity from 1971-04-01 to 2024-01-01 Annualization normalizes these variables so that average values stay consistent when the periodicity is changed to an annual frequency. This process enables a simpler comparison of time series across various periodicities. For example, a quarterly growth rate of \\(1.5\\%\\) implies an annualized growth rate of \\(4 \\times 1.5\\% = 6\\%\\), or precisely \\(6.14 \\%\\) when compounding is considered. Conversely, a higher frequency data point like a monthly growth rate of \\(0.5\\%\\) annualizes to a rate of around \\(12 \\times 0.5\\%=6\\%\\). Despite the raw growth rate appearing smaller with higher frequency data: \\(0.5\\% &lt; 1.5\\%\\), they would both grow at the same rate when growth rates remain the same over the year: \\(4 \\times 1.5\\% = 12 \\times 0.5\\%\\). Hence, annualizing ensures consistent interpretation across different periodicities. The interest rates of financial contracts are often expressed as annual rates. For example, suppose you’re a small business owner and need to cover some unexpected expenses. You might take out a short-term loan that needs to be paid back in six months. If the loan amount is \\(\\$20,000\\) and the lender charges an annual interest rate of \\(8\\%\\), a.k.a. \\(8\\%\\) per annum, the interest for this six-month period would be approximately \\(\\$20,000 \\times 8\\% \\times (6/12) = \\$800\\). More accurately, the interest would be calculated as \\(\\$20,000 \\times (1+8\\%)^{1/2}-1 \\approx \\$784.61\\), when considering compound interest. The annual rate helps you understand the cost of the loan, but the actual interest paid is prorated for the shorter period. Also, note that the 3-month Treasury bill (T-bill) rate from the example data is expressed as an annual rate. Suppose you purchased a 90-day T-bill with a face value of \\(\\$1,000\\) on April 1, 2024. The annualized return on that day was \\(5.24\\%\\); hence, the price of that T-bill would be calculated using the formula: \\(\\$1,000 / (1+ 5.24\\%)^{90/365}\\). This formula adjusts the annual interest rate to reflect the shorter time frame of 90 days. The result will be approximately equal to \\(\\$ 987.49\\), which is the amount you would have paid for the \\(\\$1,000\\) T-bill on April 1, 2024. 15.1.2 Annualized Flow Variables The process of annualization depends on the characteristics of the underlying variable and may not be suitable for all types of data. Errors often occur when trying to annualize certain economic indicators. Understanding when it’s suitable to use annualization requires the terminology of stock and flow variables of Chapter @ref(stock-vs.-flow). A stock variable represents a quantity measured at a specific point in time, like population or unemployment, while a flow variable is measured over an interval of time, like quarterly immigration or GDP. The process of annualization isn’t appropriate for stock variables. For example, when converting population data from quarterly to monthly frequency, the values wouldn’t decrease, as the population remains the same regardless of the time unit used. Conversely, when the frequency of immigration data changes from quarterly to monthly, the monthly figures will typically be lower, as there’s more immigration over an entire quarter than within a single month. Annualizing flow variables requires understanding how to aggregate these variables from higher to lower frequencies. To convert a monthly variable \\(x^{(12)}_t\\) to a quarterly frequency \\(x^{(4)}_t\\), you sum up the data points from three consecutive months. To convert the data further to an annual frequency \\(x^{(1)}_t\\), you would then sum up the four quarterly values: \\[ \\begin{aligned} \\text{Monthly to quarterly aggregation}: &amp;&amp; x^{(4)}_t = &amp;\\ x^{(12)}_t+x^{(12)}_{t+\\frac{1}{12}}+x^{(12)}_{t+\\frac{2}{12}} \\\\ \\text{Quarterly to annual aggregation}: &amp;&amp; x^{(1)}_t = &amp;\\ x^{(4)}_t+x^{(4)}_{t+\\frac{1}{4}}+x^{(4)}_{t+\\frac{2}{4}}+x^{(4)}_{t+\\frac{3}{4}} \\end{aligned} \\] where \\(t\\) represents years (e.g., \\(t=2020\\), \\(t+1=2021\\), etc.). The goal of annualization is to normalize variables such that the average values stay constant when changing the frequency to annual. Hence, to compute the annualized measure of a flow variable, you multiply the variable by the frequency \\(m\\) of the data: \\[ \\text{Annualization}: \\quad y^{(m)}_t = m x^{(m)}_t \\] where \\(y^{(m)}_t\\) is the annualized measure of \\(x^{(m)}_t\\), \\(m=12\\) for monthly data, \\(m=4\\) for quarterly, and \\(m=1\\) for annual data, and so forth. In doing so, the yearly average (arithmetic mean) of the annualized flow variables remains constant when increasing the frequency, as demonstrated below: \\[ \\begin{aligned} &amp;\\text{Yearly average of quarterly series}: \\\\ &amp;\\ \\frac{1}{4}\\left( y^{(4)}_t+y^{(4)}_{t+\\frac{1}{4}}+y^{(4)}_{t+\\frac{2}{4}}+y^{(4)}_{t+\\frac{3}{4}} \\right) = \\frac{1}{4}\\left( 4x^{(4)}_t+4x^{(4)}_{t+\\frac{1}{4}}+4x^{(4)}_{t+\\frac{2}{4}}+4x^{(4)}_{t+\\frac{3}{4}} \\right)=x_t^{(1)} \\\\ &amp; \\text{Yearly average of monthly series}: \\\\ &amp;\\ \\frac{1}{12}\\left( y^{(12)}_t+y^{(12)}_{t+\\frac{1}{12}}+\\ldots + y^{(12)}_{t+\\frac{11}{12}} \\right) = \\frac{1}{12}\\left( 12x^{(12)}_t+12x^{(12)}_{t+\\frac{1}{12}}+\\ldots + 12x^{(12)}_{t+\\frac{11}{12}} \\right)= x_t^{(1)} \\end{aligned} \\] One can also interpret an annualized flow variable as an adjusted measure that reflects what the value would be if the flow continued at the same rate for an entire year. The quarterly U.S. GDP series from the example data is a flow variable. Therefore, we can compute the annualized rate by multiplying the series by four: # Compute annualized GDP GDP_annualized &lt;- 4 * GDP names(GDP_annualized) &lt;- &quot;GDP_annualized&quot; # Print data tail(merge(GDP, GDP_annualized)) ## GDP GDP_annualized ## 2022 Q4 6602.101 26408.40 ## 2023 Q1 6703.400 26813.60 ## 2023 Q2 6765.753 27063.01 ## 2023 Q3 6902.532 27610.13 ## 2023 Q4 6989.249 27957.00 ## 2024 Q1 7063.982 28255.93 Note that the original data labeled as GDP on the FRED website was already annualized. As specified on their website: fred.stlouisfed.org/series/GDP, the units are described as “Billions of Dollars, Seasonally Adjusted Annual Rate”, where “Annual Rate” implies the quarterly GDP data was already multiplied by four. This is why I divided the original data by four in the example data section, ensuring the GDP series reflects the actual GDP produced in a single quarter. The fact that the raw data was already annualized shows how common annualization is in practice, and it’s essential to avoid inadvertently annualizing an already annualized series, as this would result in overestimated values. 15.1.3 Annualized Growth Rates Growth rate, unlike a flow variable, is a relative measure; it calculates the growth in relation to the initial value. Aggregating a growth rate to a lower frequency isn’t as straightforward as adding up the growth rates. This is because the base value changes continuously, and a 2% growth rate has different implications for each period due to the fluctuating base value. To accommodate for this continuous change in the initial values, growth rates are aggregated from higher to lower frequency using the following formula: \\[ \\begin{aligned} &amp;\\text{Monthly to quarterly aggregation}: \\\\ &amp; \\ 1+x^{(4)}_t = \\left( 1+x^{(12)}_t \\right) \\left( 1+x^{(12)}_{t+\\frac{1}{12}} \\right) \\left( 1+x^{(12)}_{t+\\frac{2}{12}} \\right) \\\\ &amp;\\text{Quarterly to annual aggregation}: \\\\ &amp; \\ 1+x^{(1)}_t = \\left(1+x^{(4)}_t \\right) \\left(1+x^{(4)}_{t+\\frac{1}{4}}\\right) \\left(1+x^{(4)}_{t+\\frac{2}{4}}\\right) \\left(1+x^{(4)}_{t+\\frac{3}{4}}\\right) \\end{aligned} \\] However, for small growth rates, the multiplication of growth rates \\(x^{(12)}_{s} x^{(12)}_{t}\\approx 0\\) because multiplying a small number with a small number results in a number that is approximately zero. Thus, we can approximate the growth rate aggregation as follows: \\[ \\begin{aligned} \\text{Monthly to quarterly aggregation}: &amp;&amp; x^{(4)}_t \\approx &amp;\\ x^{(12)}_t+x^{(12)}_{t+\\frac{1}{12}}+x^{(12)}_{t+\\frac{2}{12}} \\\\ \\text{Quarterly to annual aggregation}: &amp;&amp; x^{(1)}_t \\approx &amp;\\ x^{(4)}_t+x^{(4)}_{t+\\frac{1}{4}}+x^{(4)}_{t+\\frac{2}{4}}+x^{(4)}_{t+\\frac{3}{4}} \\end{aligned} \\] This aggregation mirrors that of flow variables. Therefore, growth rates can be annualized the same way as flow measures by multiplying the value by its frequency. However, a separate formula is necessary for exact annualization. Recall, the process of annualization aims to normalize variables so that their average values remain the same when the periodicity is changed to an annual frequency. Therefore, when computing the annualized measure of a growth rate, we raise the gross growth rate (1 plus the growth rate) to the power of the frequency \\(m\\), effectively applying the same growth rate \\(m\\) times: \\[ \\text{Annualization}: \\quad 1+y^{(m)}_t = \\left(1+ x^{(m)}_t \\right)^{m} \\] In this formula, \\(y^{(m)}_t\\) is defined as the annualized measure of \\(x^{(m)}_t\\), where \\(m=12\\) signifies a monthly periodicity of \\(x_t\\), \\(m=4\\) for quarterly, and \\(m=1\\) for annual, etc. This ensures that the yearly average (geometric mean) of the annualized growth rates stays the same when the frequency increases. As shown below, when the frequency is increased from quarterly to monthly, the geometric mean stays constant and is equal to \\(x_t^{(1)}\\): \\[ \\begin{aligned} &amp; \\text{Yearly average of quarterly series}: \\\\ &amp;\\ \\left[ \\left(1+y^{(4)}_t \\right) \\left(1+y^{(4)}_{t+\\frac{1}{4}}\\right) \\left(1+y^{(4)}_{t+\\frac{2}{4}}\\right) \\left(1+y^{(4)}_{t+\\frac{3}{4}}\\right) \\right]^{\\frac{1}{4}} \\\\ &amp;\\quad = \\left[ \\left(1+x^{(4)}_t \\right)^{4} \\left(1+x^{(4)}_{t+\\frac{1}{4}}\\right)^{4} \\left(1+x^{(4)}_{t+\\frac{2}{4}}\\right)^{4} \\left(1+x^{(4)}_{t+\\frac{3}{4}}\\right)^{4} \\right]^{\\frac{1}{4}} = 1+x^{(1)}_t \\\\ &amp;\\text{Yearly average of monthly series}:\\\\ &amp;\\ \\left[ \\left( 1+y^{(12)}_t \\right) \\left( 1+y^{(12)}_{t+\\frac{1}{12}} \\right) \\cdots \\left( 1+y^{(12)}_{t+\\frac{11}{12}} \\right) \\right]^{\\frac{1}{12}} \\\\ &amp;\\quad = \\left[ \\left( 1+x^{(12)}_t \\right)^{12} \\left( 1+x^{(12)}_{t+\\frac{1}{12}} \\right)^{12} \\cdots \\left( 1+x^{(12)}_{t+\\frac{11}{12}} \\right)^{12} \\right]^{\\frac{1}{12}} =1+ x_t^{(1)} \\end{aligned} \\] Another way to understand annualized growth rate is to consider it as an adjustment to reflect what the measure would look like if the growth rate continued at the same pace for a full year. The GDP growth rates computed in Chapter ?? are growth variables. Hence, we can compute the annualized rates as follows: # Approximate annualized GDP GDP_growth_appr_annualized &lt;- 4 * GDP_growth names(GDP_growth_appr_annualized) &lt;- &quot;GDP_growth_appr_annualized&quot; # Compute annualized GDP precisely GDP_growth_annualized &lt;- 100 * ((1 + GDP_growth / 100)^4 - 1) names(GDP_growth_annualized) &lt;- &quot;GDP_growth_annualized&quot; # Print data tail(merge(GDP_growth, GDP_growth_appr_annualized, GDP_growth_annualized)) ## GDP GDP_growth_appr_annualized GDP_growth_annualized ## 2022 Q4 1.591736 6.366944 6.520581 ## 2023 Q1 1.534345 6.137379 6.280083 ## 2023 Q2 0.930166 3.720664 3.772899 ## 2023 Q3 2.021638 8.086550 8.335093 ## 2023 Q4 1.256314 5.025257 5.120753 ## 2024 Q1 1.069249 4.276997 4.346085 Note that the growth rates, being expressed in percentages, require corresponding division and multiplication by 100. 15.1.4 Annualized Log-Difference When growth rates are approximated using log differences, as illustrated in Chapter 13.2.2, annualized growth rates can be computed in the same manner as flow variables, by multiplying the growth rate by the frequency of the data. The rationale behind this is that the aggregation to a lower frequency can be achieved by simply summing up the log-differences, which implies that it undergoes the same aggregation process as flow variables: \\[ \\begin{aligned} &amp;\\text{Monthly to quarterly aggregation}: \\\\ &amp; \\ \\begin{aligned} x^{(12)}_t+x^{(12)}_{t+\\frac{1}{12}}+x^{(12)}_{t+\\frac{2}{12}} = &amp;\\ \\left( \\ln z^{(4)}_t - \\ln z^{(4)}_{t-\\frac{1}{12}} \\right) +\\left( \\ln z^{(4)}_{t-\\frac{1}{12}} - \\ln z^{(4)}_{t-\\frac{2}{12}} \\right) +\\left( \\ln z^{(4)}_{t-\\frac{2}{12}} - \\ln z^{(4)}_{t-\\frac{3}{12}} \\right) \\\\ = &amp;\\ \\ln z^{(4)}_t - \\ln z^{(4)}_{t-\\frac{3}{12}} \\\\ = &amp;\\ x_t^{(4)} \\end{aligned} \\end{aligned} \\] We used log differences to approximate the growth rates of prices in Chapter ??. Thus, we can compute the annualized rates as follows: # Annualize monthly inflation Inflation_annualized &lt;- 12 * Inflation # Annualize daily Nasdaq returns (about 252 trading days a year) Nasdaq_return_annualized &lt;- 252 * Nasdaq_return Let’s now plot these annualized growth rates: # Put all plots into one par(mfrow = c(rows = 2, columns = 1), mar = c(b = 2, l = 4, t = 2, r = 1)) # Plot growth rates of prices plot.zoo(x = Inflation_annualized, main = &quot;U.S. Inflation, Annualized&quot;, xlab = &quot;&quot;, ylab = &quot;%&quot;) plot.zoo(x = Nasdaq_return_annualized, main = &quot;Nasdaq Stock Market Return, Annualized&quot;, xlab = &quot;&quot;, ylab = &quot;%&quot;) The functions used in the above code chunk have been elaborated on in Chapters 13.2.1 and 13.2.2. Figure 15.1: Annualized Growth Rate of Prices Figure 15.1 displays the annualized growth rate of two price indices: the GDP deflator and the Nasdaq composite index. The graphs make it clear that although computing annualized measures is sensible for inflation, it’s less so for the Nasdaq composite index, which can produce growth rates exceeding 2000%. For instance, stock returns can easily surpass 8% in a single day, and annualizing this return, considering 252 trading days, yields an exceedingly large growth rate of \\(252\\times 8\\% = 2016\\%\\). Consequently, annualization is not commonly used for variables with high frequency or high variance. 15.2 Year over Year (YoY) Year over Year (YoY) Growth Rates: represent the percentage change in a certain variable from one year to the same period in the next year. By comparing the same periods, YoY growth rates remove any effects of seasonality, making it easier to observe the long-term trends or performance of the variable. Moreover, unlike other growth rates, increasing the frequency of data doesn’t reduce the size of YoY growth rates, since YoY growth rates always measure growth over a one-year period, regardless the frequency. For example, if the GDP for Q2 2023 is \\(\\$20\\) trillion and for Q2 2022 it was \\(\\$18\\) trillion, the YoY growth rate would be \\[ \\frac{\\$20 \\text{ trillion} - \\$18 \\text{ trillion}}{ \\$18 \\text{ trillion}} \\times 100 = 11.1\\%, \\] indicating an \\(11.1\\%\\) increase in GDP from Q2 2022 to Q2 2023. Same as moving average if growth rate is measured using log-difference. When analyzing time series data, it’s common to use Year over Year (YoY) measures as a way to compare the performance of a variable during the same period from one year to the next. This form of measure provides a clearer picture of performance, as it negates the effects of seasonality. 15.2.1 Year over Year (YoY) Growth Rates YoY growth rates represent the percentage change of a certain variable from one year to the corresponding period in the next year. Unlike other growth rates, the frequency of data doesn’t impact the magnitude of YoY growth rates, as these always measure growth over a one-year period, regardless of the data’s frequency. For instance, if the GDP for Q2 2023 is $20 trillion and for Q2 2022 it was $18 trillion, the YoY growth rate would be calculated as: \\[ \\frac{\\$20 \\text{ trillion} - \\$18 \\text{ trillion}}{ \\$18 \\text{ trillion}} \\times 100 = 11.1\\%, \\] This would indicate an 11.1% increase in GDP from Q2 2022 to Q2 2023. 15.2.2 Comparing YoY Measures with Moving Averages Moving averages and YoY measures are both powerful tools for smoothing out data and revealing underlying trends, but they serve slightly different purposes. A moving average is a calculation that takes the arithmetic mean of a given set of values over a specified period, which slides over time. It is often used to identify trends in the short to medium term. YoY measures, on the other hand, are used to compare the same period across different years, which is useful for identifying long-term trends and eliminating the effects of seasonality. Interestingly, if growth rates are calculated using log differences, YoY measures will be identical to a 12-month moving average. This is because the logarithm of the growth rate is the difference in the logarithms, which can be added together, similar to how moving averages are calculated. 15.2.3 Sliding Window vs. Fixed Window Aggregation Figure 15.2: Time Windows Aggregation over a time window is a common operation in time series analysis, with sliding windows and fixed windows being the two main types. Sliding Window: In a sliding window, the window “slides” over time. That is, the start and end of the window change at each step, always incorporating the same number of periods. It provides a continuous stream of averages (or other aggregations) and is most useful when you’re interested in smooth trends over time. For example, a 12-month sliding window of monthly data would calculate the average for each successive 12-month period. Fixed Window: In contrast, a fixed window holds the start or end of the window constant and aggregates data up to or from that point. For example, calculating the average of all preceding data from a fixed point in time uses a fixed window. Fixed windows are typically used when you’re interested in the change from a specific time point. Each method has its advantages and can be selected based on the specific requirements of the analysis. "],["part-iv.html", "Part IV: Patterns in Economics", " Part IV: Patterns in Economics Part IV delves into the intricacies of economic patterns across time, space, and individual entities. This segment highlights how economies evolve temporally, including the nuances of business cycles and seasonal variations. It also examines regional disparities, weighing influences such as local demographics, industrial presence, and institutional frameworks. The discussion extends to the micro level, elucidating individual and firm decisions that shape market dynamics. The section culminates in understanding causation in economics, distinguishing between mere correlation and genuine causal relationships. Included chapters: Chapter 16: “Temporal Patterns” provides an insight into time series data’s features and characteristics from a macroeconomic lens, examining trends, business cycles, and seasonality. Chapter 17: “Regional Patterns” delves into the variations in economic performance across regions at various geographical levels, including county, state, and national levels, and delves into the underlying factors influencing these patterns. Chapter 18: “Microeconomic Patterns” explores microeconomic behaviors and decisions of consumers, firms, and industries, and how external factors like government policies and technological innovations influence market outcomes. Chapter 19: “Causal Relationships” transitions from mere patterns to establishing causal relationships, highlighting key economic causal measures like treatment effects, elasticities, and multipliers. "],["temporal-patterns.html", "Chapter 16 Temporal Patterns 16.1 Time Series Data 16.2 Trend 16.3 Seasonality 16.4 Business Cycle", " Chapter 16 Temporal Patterns 16.1 Time Series Data This chapter provides an overview of the features and characteristics of time series data from a macroeconomic perspective. In Macroeconomics, time-series data often exhibit three key features: a trend, a business cycle, and a seasonality. Understanding these features is crucial to analyzing and interpreting economic data. But before venturing into these features, the chapter begins with an examination of the concept of a time series. 16.1.1 Definition A time series is a sequence of numerical data points, measured over different points in time. In simple terms, these data points are pairs, including the time variable \\(t\\) and the variable of interest at that time \\(y_t\\). For instance, if \\(y_t\\) represents U.S. GDP, then \\(y_{2017 \\ Q3} = 4920\\), since U.S. GDP was \\(4,920\\) billion USD in the third quarter of 2017. The entire GDP time series \\(\\{y_t\\}\\) thus looks as follows: \\[ \\begin{aligned} \\{y_t\\}=&amp;\\ \\{58, 61, \\ldots, 4920, 5050,\\ldots\\} \\\\ \\{t\\}=&amp;\\ \\{1947 \\ Q1, 1947 \\ Q2, \\ldots, 2017 \\ Q3, 2017 \\ Q4, \\ldots\\} \\end{aligned} \\] where each value represents the GDP in billion dollars at a specific quarter, starting from \\(1947 \\ Q1\\). 16.1.2 Features Economic time series typically possess three primary characteristics: trends, seasonality, and cycles. Trend: This refers to the long-term direction of the time series. It could exhibit growth (an upward trend) or decay (downward trend). For instance, many developed countries have seen sustained economic growth, meaning their GDP has demonstrated a consistent upward trend over numerous decades (refer to Chapter 16.2 for more examples). Seasonality: Seasonal patterns are regular fluctuations in a time series due to calendar effects. These patterns may recur daily, monthly, quarterly, or annually. A prime example of seasonality is the performance of the retail sector, which usually sees a surge during the holiday season (Q4 - October, November, December) due to increased shopping for holiday gifts (refer to Chapter 16.3 for more examples). Cycles: Cycles refer to the fluctuations in time series data that don’t have a fixed period. Unlike the regular pattern of seasonality, these fluctuations rise and fall irregularly. In Macroeconomics, these cycles are known as business cycles, which refer to the periods of expansion (growth in real output) and recession (decline in real output). Chapter 16.4 delves deeper into the behavior of various economic indicators during these business cycles. A single time series can exhibit all three features: trends, cycles, and seasonality. For instance, the U.S. GDP showcases a general upward trend with cyclical deviations around this trend - sometimes above (expansions) and sometimes below (recessions), forming a business cycle. It also exhibits annual fluctuations, indicative of seasonal patterns. Before we delve into a detailed discussion on U.S. macroeconomic trends, business cycles, and seasonal patterns, let’s first establish how to effectively visualize a time series. 16.1.3 Visualization The first step in data analysis often involves visualizing the data. When it comes to time series data, it is common practice to plot time on the x-axis and the variable of interest on the y-axis. These plots enable the identification of temporal patterns and characteristics, including trends, business cycles, seasonal patterns, outliers, structural breaks, changes in volatility, and relationships between variables. To highlight business cycle movements, it’s typical to overlay recession shades on the time series plot. These shades mark periods of recession. In R, there are two common approaches for plotting time series data. The appropriate method depends on how the time series is stored. If the time series is stored as an xts (or zoo) object (as discussed in Chapter 4.8), then the plot.zoo() function is used. Conversely, if the time series is stored as a data.frame or a tibble object (topics discussed in Chapters @ref(data.frame) and 4.6), the plot.zoo() function isn’t applicable. In this case, the ggplot() function from the tidyverse package is the good choice. Using the plot.zoo() Function To illustrate the use of the plot.zoo() function, we’ll utilize a time series representing the quarterly GDP of the United States, obtained from the Bureau of Economic Analysis (BEA). You can access U.S. GDP data from the Federal Reserve Economic Data (FRED) database, maintained by the Federal Reserve Bank of St. Louis. The getSymbols() function from the quantmod package in R can be used to directly download this data, as explained in Chapter ??. Visit the FRED website at fred.stlouisfed.org and input “U.S. GDP” in the search bar. The first suggestion would be “Gross Domestic Product” subtitled “Billions of Dollars, Quarterly, Seasonally Adjusted Annual Rate”. Although this is the correct variable, we are interested in the original, non-seasonally adjusted GDP series. To retrieve this, click on “6 other formats”, and select “Quarterly, Millions of Dollars, Not Seasonally Adjusted”. The resulting graph will be titled “Gross Domestic Product (NA000334Q)”. To download this data with getSymbols(), use “NA000334Q” for the Symbols parameter and “FRED” for the src (source) parameter: # Import the quantmod package for using the getSymbols() function library(&quot;quantmod&quot;) # Download U.S. quarterly GDP time series getSymbols(Symbols = &quot;NA000334Q&quot;, src = &quot;FRED&quot;) # Convert GDP from millions to billions NA000334Q &lt;- NA000334Q / 1000 Following this, we generate a plot of the time series, setting time on the x-axis and GDP values on the y-axis: # Plot the data plot.zoo(x = NA000334Q, main = &quot;U.S. GDP&quot;, xlab = &quot;Date&quot;, ylab = &quot;Billions of Dollars&quot;, lwd = 2, col = 4) This code block creates a plot of U.S. GDP over time using the plot.zoo() function, a function designed to handle time series data. Here is what each argument does: x = NA000334Q: This specifies the data to be plotted. NA000334Q is an xts object that contains U.S. GDP data. main = \"U.S. GDP\": This sets the main title of the plot to “U.S. GDP”. xlab = \"Date\": This sets the label of the x-axis to “Date”. ylab = \"Billions of Dollars\": This sets the label of the y-axis to “Billions of Dollars”. lwd = 2: This sets the line width of the plot. A value of 2 indicates that the line will be twice as thick as the default width. col = 4: This sets the color of the plot line. In R, color is often specified by a number, and 4 corresponds to blue. Figure 16.1: U.S. GDP The resulting plot, shown in Figure 16.1, presents U.S. GDP from the first quarter of 1947 to the present. Observe the upward trend, indicative of the consistent growth of the U.S. economy over this period. The annual ups and downs signify seasonal patterns, whereas the fluctuations during periods of economic boom and recession denote the business cycles. To emphasize the business cycle, it is common to add shaded areas over the periods where the economy experienced a recession. In the U.S., the National Bureau of Economic Research (NBER), a private, non-profit research organization dedicated to the study of the economy, determines the official dates for the beginning and end of recessions in the U.S. Recessions are defined by the NBER as “a significant decline in economic activity that is spread across the economy and lasts more than a few months.” (see definition here). The NBER recession periods are stored as a monthly dummy variable, where 1 refers to a recession period and 0 to a non-recession period, which can be downloaded from the FRED website using the USREC symbol: # Load NBER based recession indicators for the United States getSymbols(Symbols = &quot;USREC&quot; , src = &quot;FRED&quot;) # Print data during the end of the Great Recession USREC[&quot;2009-04/2009-09&quot;] ## [1] &quot;USREC&quot; ## USREC ## 2009-04-01 1 ## 2009-05-01 1 ## 2009-06-01 1 ## 2009-07-01 0 ## 2009-08-01 0 ## 2009-09-01 0 The recession shaded can then be added to Figure 16.1 as follows: # Get the x- and y-coordinates of the polygon with recession shades x_poly &lt;- c(index(USREC), rev(index(USREC))) y_poly &lt;- c((coredata(USREC) * 2 - 1), rep(-1, nrow(USREC))) * 10^7 # Plot U.S. GDP plot.zoo(NA000334Q, main = &quot;U.S. GDP with Recession Shades&quot;, ylab = &quot;Billions of Dollars&quot;, xlab = &quot;Date&quot;, lwd = 2, col = 4) # Draw the polygon with recession shades polygon(x = x_poly, y = y_poly, col = rgb(0, 0, 0, alpha = 0.2), border = NA) # Plot U.S. GDP again so that the line is on top of the shades par(new = TRUE) plot.zoo(x = NA000334Q, lwd = 2, col = 4, xlab = &quot;&quot;, ylab = &quot;&quot;) # Legend legend(x = &quot;topleft&quot;, legend = c(&quot;U.S. GDP&quot;, &quot;NBER Recession Indicator&quot;), col = c(4, rgb(0, 0, 0, alpha = 0.2)), lwd = c(2, 10)) Figure 16.2: U.S. GDP with NBER Recession Shades Let’s dissect the code chunk line by line: The first line is creating the x-coordinates for the polygon that will draw the recessions shades on the plot. The index() function is used to get the date indices from the USREC series. These dates are used as the x-coordinates of the polygon, first in increasing order and then in decreasing order, to form a closed polygon. x_poly &lt;- c(index(USREC), rev(index(USREC))) The second line is responsible for generating the y-coordinates for the polygon. Here, the coredata() function is used to extract the raw data from the USREC xts object, which is comprised of zeros and ones. This data is then multiplied by 2 and subtracted by 1, yielding a series of positive ones during recessions and negative ones during expansions. Following this, the rep function is used to produce a vector of negative ones with a length equal to the number of rows in USREC. Both vectors are then concatenated to establish the y-coordinates. The design of the y-coordinates in this manner ensures that during recession periods (when USREC == 1), the polygon reaches a maximum height of \\(10^7\\), while during non-recession periods (when USREC == 0), the polygon descends to a minimum height of \\(-10^7\\). If the time series encompasses values greater than \\(10^7\\), this number should be adjusted to a higher value to cover all values of the data. y_poly &lt;- c((coredata(USREC) * 2 - 1), rep(-1, nrow(USREC))) * 10^7 The plot.zoo() function is used to plot the U.S. GDP (NA000334Q). The x-axis labels are suppressed with xlab=\"\", and the y-axis is labeled as “Billions of Dollars”. The plotted line is colored blue (as indicated by col = 4), and its width is set to 2. plot.zoo(NA000334Q, main = &quot;U.S. GDP with Recession Shades&quot;, ylab = &quot;Billions of Dollars&quot;, xlab = &quot;&quot;, lwd = 2, col = 4) The polygon() function is used to draw the polygons that represent recession periods. The color of the polygons is set to a semi-transparent black, created with rgb(0, 0, 0, alpha = 0.2), and the borders are suppressed with border = NA. polygon(x = x_poly, y = y_poly, col = rgb(0, 0, 0, alpha = 0.2), border = NA) The GDP series is re-plotted on top of the polygons to make the GDP line more visible. The par(new = TRUE) function is used to allow for this overlay. par(new = TRUE) plot.zoo(x = NA000334Q, lwd = 2, col = 4, xlab = &quot;&quot;, ylab = &quot;&quot;) Finally, a legend is added to the top-left of the plot to differentiate between the GDP line and the recession shading. legend(x = &quot;topleft&quot;, legend = c(&quot;U.S. GDP&quot;, &quot;NBER Recession Indicator&quot;), col = c(4, rgb(0, 0, 0, alpha = 0.2)), lwd = c(2, 10)) To streamline our plotting process, we can define a function that extends the plot.zoo() function, allowing it to automatically add the recession shades: #-------------------------------------- # plot.zoo.rec() function #-------------------------------------- # Load NBER based recession indicators for the United States quantmod::getSymbols(Symbols = &quot;USREC&quot; , src = &quot;FRED&quot;) # Get the x- and y-coordinates of the polygon with recession shades x_poly &lt;- c(index(USREC), rev(index(USREC))) y_poly &lt;- c((coredata(USREC) * 2 - 1), rep(-1, nrow(USREC))) * 10^7 # Function that plots time series with recession shades plot.zoo.rec &lt;- function(...) { # Initial plotting of the time series plot.zoo(...) # Add recession shades polygon(x = x_poly, y = y_poly, col = rgb(0, 0, 0, alpha = 0.2), border = NA) # Redraw the time series on top of the recession shades par(new = TRUE) plot.zoo(...) } In the function plot.zoo.rec(), the recession data (USREC) is first retrieved from the FRED database, and the x- and y-coordinates for the recession polygon are calculated, similar to the previous code. This function then plots the given time series, adds the recession shades as a polygon, and re-plots the time series on top of the recession shades. This code chunk can be placed at the beginning of your R script. Afterwards, you can use plot.zoo.rec() in place of plot.zoo() whenever you’re graphing time series data stored as xts or zoo objects. We can now use this newly defined function plot.zoo.rec() to plot the log of the Nasdaq Composite Index along with the NBER recession shades: # Retrieve the daily Nasdaq stock market index getSymbols(Symbols = &quot;NASDAQCOM&quot; , src = &quot;FRED&quot;) # Plot the index with recession shades using the previously defined function plot.zoo.rec(log(NASDAQCOM), main = &quot;Nasdaq Composite Index&quot;, ylab = &quot;Log&quot;, xlab = &quot;Date&quot;) Figure 16.3: Nasdaq Composite Index with NBER Recession Shades Figure 16.3 displays the log of the Nasdaq stock market index together with NBER recession shades. Note that the recession indicator USREC has a monthly frequency, so if the plotted time series has a higher frequency (like the daily Nasdaq Composite Index in this example), the recession shade starts on the first day of the month and ends on the last day of the month. As can be seen in the figure, around or even prior to the onset of recessions, there’s often a pre-recession rally in the stock market, marked by ascending prices fueled by optimism and strong investor confidence. However, as a recession becomes more probable, we observe an increase in volatility and a market correction. A key example visible in Figure 16.3 is the dot-com bubble, which was prevalent during the late 1990s and early 2000s. This period was defined by excessive speculation in internet-focused companies. During this phase, the Nasdaq stock market index, largely made up of technology stocks, experienced an exceptional rise in its value. Fuelled by unrealistic expectations of internet-based businesses, the Nasdaq index soared to an all-time high in March 2000. However, as the hype faded and many dot-com companies failed to deliver on profit expectations, the bubble burst, leading to a significant drop in the Nasdaq index and substantial financial losses for investors. This bursting of the dot-com bubble contributed to the onset of a recession, as reflected by the gray recession shade visible on the figure from April to November 2001. This recessionary period put even more downward pressure on the stock market. Using the ggplot() Function To illustrate the use of the ggplot() function for tibble objects, we’ll use the Treasury yield curve data discussed in Chapter ??. The data is downloaded from the U.S. Department of the Treasury as a CSV file called “yieldcurve.csv” in a folder called “files”. The CSV file is then imported into R following the same steps outlined in Chapter ??: # Load the package to import CSV file library(&quot;readr&quot;) # Import CSV file yc &lt;- read_csv(file = &quot;files/yieldcurve.csv&quot;, col_names = TRUE) # Replace &quot;N/A&quot; with NA yc[yc == &quot;N/A&quot;] &lt;- NA # Convert all yield columns to numeric data types yc[, -1] &lt;- sapply(yc[, -1], as.numeric) # Convert to date format yc$Date &lt;- as.Date(yc$Date, format = &quot;%m/%d/%y&quot;) # Sort data according to date yc &lt;- yc[order(yc$Date), ] # Print yc data yc ## # A tibble: 8,382 × 14 ## Date `1 Mo` `2 Mo` `3 Mo` `4 Mo` `6 Mo` `1 Yr` `2 Yr` `3 Yr` `5 Yr` ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1990-01-02 NA NA 7.83 NA 7.89 7.81 7.87 7.9 7.87 ## 2 1990-01-03 NA NA 7.89 NA 7.94 7.85 7.94 7.96 7.92 ## 3 1990-01-04 NA NA 7.84 NA 7.9 7.82 7.92 7.93 7.91 ## 4 1990-01-05 NA NA 7.79 NA 7.85 7.79 7.9 7.94 7.92 ## 5 1990-01-08 NA NA 7.79 NA 7.88 7.81 7.9 7.95 7.92 ## 6 1990-01-09 NA NA 7.8 NA 7.82 7.78 7.91 7.94 7.92 ## 7 1990-01-10 NA NA 7.75 NA 7.78 7.77 7.91 7.95 7.92 ## 8 1990-01-11 NA NA 7.8 NA 7.8 7.77 7.91 7.95 7.94 ## 9 1990-01-12 NA NA 7.74 NA 7.81 7.76 7.93 7.98 7.99 ## 10 1990-01-16 NA NA 7.89 NA 7.99 7.92 8.1 8.13 8.11 ## # ℹ 8,372 more rows ## # ℹ 4 more variables: `7 Yr` &lt;dbl&gt;, `10 Yr` &lt;dbl&gt;, `20 Yr` &lt;dbl&gt;, `30 Yr` &lt;dbl&gt; In the next step, we load the tidyverse package, which includes the ggplot2 package. The ggplot2 package contains the ggplot() function that we’ll use for data visualization. The ggplot() function requires data in long format as opposed to wide format, meaning that it cannot handle a data frame where different columns represent different variables. Instead, different variables need to be arranged as new rows, and an additional column should be added to store the variable names. This transformation can be achieved using the pivot_longer() function from the tidyverse package. The following code chunk demonstrates this procedure: # Load tidyverse package library(&quot;tidyverse&quot;) # Reshape yield curve data to long format yc_long &lt;- yc %&gt;% select(Date, `3 Mo`, `3 Yr`, `10 Yr`) %&gt;% pivot_longer(cols = -Date, names_to = &quot;ttm&quot;, values_to = &quot;yield&quot;) # Order time to maturity from shortest to longest yc_long$ttm &lt;- factor(yc_long$ttm, levels = c(&quot;3 Mo&quot;, &quot;3 Yr&quot;, &quot;10 Yr&quot;)) In the code above, we first load the tidyverse package. We then reshape the yield curve data to long format using the pivot_longer() function. In this process, we select the Date, 3 Mo, 3 Yr, and 10 Yr columns from the yc data frame and transform them into a long-format data frame where the Date column remains intact and the other columns are condensed into two columns - ttm (time to maturity) and yield. The ttm column represents the original column names from the yc data frame (i.e., 3 Mo, 3 Yr, 10 Yr), and the yield column contains the corresponding yield values. Lastly, we order the time to maturity (ttm) from the shortest to the longest using the factor() function. We can add recession shades to a ggplot() plot using a function we’ll call geom_recession_shades(). This function is similar to the plot.zoo.rec() function defined earlier, but it only adds recession shades as a ggplot() layer, not the entire plot. For plotting time series, we’ll still use the ggplot() function in conjunction with geom_recession_shades(). The function is defined as follows: #-------------------------------- # Recession shades #-------------------------------- # Load NBER based recession indicators for the United States quantmod::getSymbols(&quot;USRECM&quot;, src = &quot;FRED&quot;) # Create a data frame with dates referring to start and end of recessions REC &lt;- data.frame(index = zoo::index(USRECM), USRECM = zoo::coredata(USRECM)) REC &lt;- rbind(list(REC[1, &quot;index&quot;], 0), REC, list(REC[nrow(REC), &quot;index&quot;], 0)) REC$dUSRECM &lt;- REC$USRECM - c(NA, REC$USRECM[-nrow(REC)]) REC &lt;- data.frame(rec_start = REC$index[REC$dUSRECM == 1 &amp; !is.na(REC$dUSRECM)], rec_end = REC$index[REC$dUSRECM == -1 &amp; !is.na(REC$dUSRECM)]) # Add a ggplot() layer that draws rectangles for those recession periods geom_recession_shades &lt;- function(xlim = c(min(REC$rec_start), max(REC$rec_end))){ geom_rect(data = REC[REC$rec_start &gt;= xlim[1] &amp; REC$rec_end &lt;= xlim[2], ], inherit.aes = FALSE, aes(xmin = rec_start, xmax = rec_end, ymin = -Inf, ymax = +Inf), fill = &quot;black&quot;, alpha = .15) } This function should be included at the start of your R script, after which you can call geom_recession_shades() within the ggplot() function to include recession shades in your plots. The xlim parameter specifies the date range over which the recession shades are to be drawn. For example, the function geom_recession_shades(xlim = as.Date(\"1970-01-01\", \"2021-01-01\")) draws recession shades from 1970 to 2020. In this code chunk, the recession dummy variable is first downloaded from the FRED database. A data frame, REC, is then created which includes the start and end dates of the recessions in separate columns: # Print REC data tail(REC) ## rec_start rec_end ## 30 1980-01-01 1980-08-01 ## 31 1981-07-01 1982-12-01 ## 32 1990-07-01 1991-04-01 ## 33 2001-03-01 2001-12-01 ## 34 2007-12-01 2009-07-01 ## 35 2020-02-01 2020-05-01 Lastly, the function geom_recession_shades() is defined as a ggplot() layer that uses the geom_rect() function to overlay recession periods with gray rectangles. Let’s use the geom_recession_shades() function in conjunction with the ggplot() function to plot the Treasury yield curve data: # Plot Treasury yield curve rates with recession shades ggplot(data = yc_long, mapping = aes(x = Date, y = yield, color = ttm)) + geom_recession_shades(xlim = range(yc_long$Date)) + geom_line() + theme_classic() + labs(x = &quot;Date&quot;, y = &quot;%&quot;, color = &quot;Time to Maturity&quot;, title = &quot;Treasury Yield Curve Rates&quot;) This code chunk uses the ggplot() function to create a plot of the Treasury yield curve rates over time. The plot also features shaded regions representing economic recessions. Here’s a breakdown of what each line does: ggplot(data = yc_long, mapping = aes(x = Date, y = yield, color = ttm)): This line creates the base of the plot, specifying that the data comes from the yc_long dataframe. The aes() function maps the variables to the plot aesthetics: Date to the x-axis, yield to the y-axis, and ttm (Time to Maturity) to the color of the lines. geom_recession_shades(xlim = range(yc_long$Date)): This line adds the recession shades to the plot, using the function geom_recession_shades() defined earlier. The xlim parameter is set to the range of dates in the yc_long dataframe, meaning the recession shades will span the full date range of the data. geom_line(): This line adds line geometries to the plot. Each line represents one of the Time to Maturity categories (3-month, 3-year, 10-year), with its color set by the aes() mapping set up in the first line. theme_classic(): This sets the overall aesthetic theme of the plot to a “classic” style, which includes a white background and black axis lines. labs(x = \"Date\", y = \"%\", color = \"Time to Maturity\", title = \"Treasury Yield Curve Rates\"): This line customizes the labels of the plot. It sets the x-axis label to “Date”, the y-axis label to “%”, the legend title (for the color variable) to “Time to Maturity”, and the main plot title to “Treasury Yield Curve Rates”. Figure 16.4: Treasury Yield Curve Rates The resulting plot, shown in Figure 16.4, shows how the short-term (3-month), mid-term (3-year), and long-term (10-year) Treasury rates have evolved over time, with shaded areas indicating periods of economic recession. Treasury rates are procyclical, meaning they tend to rise during economic expansions and fall during recessions. This is because during periods of economic expansion, demand for money and credit increases, pushing interest rates higher. Conversely, during a recession, economic activity slows, the demand for money and credit decreases, and so do interest rates. In periods leading up to a recession, it’s not uncommon for the short-term, mid-term, and long-term Treasury rates to converge, as observed in the figure. This convergence, resulting in a flat yield curve, reflects market expectations of a slowdown in economic activity. The reason for this is that, in anticipating a recession, investors expect the interest rate to decline and therefore invest in long-term bonds, which drives up their prices and hence lowers their yields. During and following a recession, as represented by the recession shades in Figure 16.4, these rates frequently diverge. This divergence is generally a result of the market’s growing optimism about the future trajectory of the economy compared to the ongoing recession. Additionally, this divergence can be attributed to monetary policy interventions: in efforts to revitalize the economy during a recession, central banks often opt to reduce short-term interest rates more drastically than long-term rates, leading to a steepening of the yield curve. 16.2 Trend The trend in a time series refers to the long-term progression or directional movement in the data. This trend may be upwards, downwards, or a combination of both over different periods. To identify economic trends, we can plot the data over time and visually examine the overall pattern. However, for a more rigorous analysis, techniques such as using a moving average or applying regression methods can be utilized to estimate the trend line. This chapter discusses some notable trends observed in economic data. 16.2.1 Sustained Growth One prevailing long-term trend observed in many economies is sustained economic growth. Despite intermittent periods of recessions and downturns, the real GDP of numerous nations has demonstrated a steady upward trend over several decades, reflecting increased wealth and economic output. This long-term pattern can be discerned when plotting real GDP per capita for various nations. Real GDP per capita is an indicator of the economic output per person in a country, calculated by dividing the country’s GDP by its population. It is represented in current US dollars for all periods, meaning that it accounts for inflation over time. This indicator serves as a measure of the average income and standard of living within a country. The World Bank database provides this measure under the variable name NY.GDP.PCAP.CD. Figure 16.5 shows the real GDP per capita for selected countries (see the forthcoming code chunk for data download and plotting instructions). Figure 16.5: Real GDP per Capita As evident in Figure 16.5, most countries have witnessed a continuous rise in their economic output over time. Several factors contribute to this phenomenon. Firstly, technological progress plays a significant role. Technological improvements tend to be cumulative, implying that advancements made today continue to exist in the future, thereby improving the output per person forever. The ongoing nature of these improvements means that GDP tends to increase continuously as more technologies become available. Secondly, globalization, spurred by trade liberalization and political stability, allows for greater economic specialization, which subsequently enhances overall output. Thirdly, population growth and urbanization can amplify output, even on a per capita basis. As a country’s population and labor force grow, economies of scale come into play, potentially leading to an increase in output per worker. This effect is particularly prominent in urban areas where access to markets and services is readily available, and human capital is concentrated. Fourthly, investments in human capital (through education and healthcare) and infrastructure (such as transportation and communication systems) also contribute to the sustained economic growth observed in the data. Both education and healthcare enhance workforce productivity, while infrastructure investments stimulate economic activity by improving the flow of goods, services, and information. Finally, improvements in institutional factors, such as the rule of law, protection of property rights, and implementation of economic policies conducive to growth (like stable monetary and fiscal policies), contribue to sustained economic growth. These factors create a favorable business environment where companies can plan long-term, invest, and innovate, thereby driving growth in GDP per capita. The following R code will generate Figure 16.5, which displays the real GDP per capita for selected countries: # Load API to access World Bank data library(&quot;wbstats&quot;) # Select countries for which data will be downloaded countries &lt;- c(&quot;United States&quot;, &quot;Mexico&quot;, &quot;Brazil&quot;, &quot;Spain&quot;, &quot;Poland&quot;, &quot;Switzerland&quot;, &quot;Japan&quot;, &quot;Egypt, Arab Rep.&quot;, &quot;South Africa&quot;, &quot;India&quot;, &quot;China&quot;) # Get GDP per capita in current USD for the selected countries gdp_by_country &lt;- wb_data(indicator = &quot;NY.GDP.PCAP.CD&quot;, country = countries) # Convert date column to Date format gdp_by_country$date &lt;- as.Date(paste(gdp_by_country$date, &quot;1-1&quot;), format = &quot;%Y %m-%d&quot;) # Plot real GDP for each country over time ggplot(gdp_by_country) + aes(x = date, y = NY.GDP.PCAP.CD, color = country) + geom_recession_shades(xlim = range(gdp_by_country$date)) + geom_line() + theme_classic()+ labs(x = &quot;Date&quot;, y = &quot;Current USD&quot;, color = &quot;Country&quot;, title = &quot;Real GDP Per Capita&quot;) 16.2.2 Decrease in Poverty Another significant long-term trend evident across various economies is the decline in poverty rates. Over the past century, a persistent decrease in the proportion of populations living in extreme poverty has been observed in numerous countries. This trend is attributable not just to overall economic growth, but also to purposeful policies aimed at poverty reduction. Data on poverty can be sourced from the World Bank database under the variable SI.POV.DDAY. This variable represents the percentage of the population living below the international poverty line, defined as $1.90 per day in 2011 purchasing power parity (PPP) terms. This indicator measures the prevalence of extreme poverty within a country. Figure 16.6 illustrates this poverty rate for selected countries (refer to the below code chunk for data download and plotting instructions). Figure 16.6: Poverty Rate As demonstrated in Figure 16.6, countries worldwide have witnessed a steady decline in the proportion of their populations living in extreme poverty. This encouraging trend is influenced by various factors. Firstly, economic growth, as discussed in the Sustained Growth section, plays a critical role by leading to increased incomes, job creation, and improved living standards. With economic expansion, individuals and families are provided with more opportunities to break free from extreme poverty and enhance their quality of life. Secondly, echnological advancements also play a significant role by increasing productivity, creating new employment opportunities, and improving access to vital services. These advancements facilitate education, employment, and income generation, assisting individuals in escaping poverty. Thirdly, globalization and trade have also been influential in reducing poverty. The expansion of markets, economic integration, and the facilitated flow of goods, services, and capital that come with globalization and trade have been instrumental in poverty reduction. Trade liberalization and engagement in global value chains allow developing countries to access global markets, draw foreign investment, and stimulate economic growth, thereby diminishing poverty. Fourthly, specific policies and social safety net programs aimed at poverty reduction have been implemented across many nations. These include conditional cash transfer programs, social welfare initiatives, subsidies for education and healthcare, and targeted employment generation programs. These measures provide direct support to vulnerable populations, improve access to basic services, and create opportunities for income generation. Finally, international development efforts by organizations such as the World Bank and the IMF have also supported poverty reduction through financial aid, technical expertise, and policy advice. The R code below will generate Figure 16.6, depicting the poverty rate for selected countries: # Load API to access World Bank data library(&quot;wbstats&quot;) # Select countries for which data will be downloaded countries &lt;- c(&quot;United States&quot;, &quot;Mexico&quot;, &quot;Brazil&quot;, &quot;Spain&quot;, &quot;Poland&quot;, &quot;Switzerland&quot;, &quot;Japan&quot;, &quot;Egypt, Arab Rep.&quot;, &quot;South Africa&quot;, &quot;India&quot;, &quot;China&quot;) # Download poverty rate for selected countries world_poverty &lt;- wb_data(indicator = &quot;SI.POV.DDAY&quot;, country = countries) world_poverty &lt;- world_poverty[!is.na(world_poverty$SI.POV.DDAY), ] # Convert date column to Date format world_poverty$date &lt;- as.Date(paste(world_poverty$date, &quot;1-1&quot;), format = &quot;%Y %m-%d&quot;) # Plot poverty rate for each country over time ggplot(world_poverty) + aes(x = date, y = SI.POV.DDAY, color = country) + geom_recession_shades(xlim = range(world_poverty$date)) + geom_line() + theme_classic()+ labs(x = &quot;Date&quot;, y = &quot;Poverty Rate&quot;, color = &quot;Country&quot;, title = &quot;Worldwide Poverty&quot;) 16.2.3 Aging Population A remarkable long-term trend observed in many developed countries, including the United States, is the increase in the average age of the population. This trend, often referred to as an aging population or demographic aging, represents a shift in the age distribution of a population towards older ages. Data related to this trend can be obtained from the Federal Reserve Economic Data (FRED), under the variable SPPOPDPNDOLUSA. This variable measures the Age Dependency Ratio for the United States, defined as the ratio of dependents (people younger than 15 or older than 64) to the working-age population (those ages 15-64). An increase in this ratio typically signifies an increase in the proportion of older dependents, thus reflecting an aging population. Figure 16.7 illustrates the Age Dependency Ratio for the United States (refer to the code chunk below for data download and plotting instructions). Figure 16.7: Age Dependency Ratio for the United States As depicted in Figure 16.7, the United States, like many other developed nations, has experienced a steady increase in the Age Dependency Ratio over time. This trend of increasing average age is influenced by several factors. Firstly, advancements in healthcare have led to increased life expectancy, meaning people are living longer. This increase in longevity has resulted in a larger proportion of older adults in the population. Secondly, declining fertility rates have also contributed to demographic aging. As more individuals and families choose to have fewer children or delay childbearing, the proportion of younger individuals in the population decreases, thus raising the average age. Thirdly, the Baby Boomer generation (those born between 1946 and 1964) has reached retirement age, significantly increasing the proportion of elderly individuals in the population. This trend of an aging population has significant social, economic, and political implications. Economically, it can lead to labor shortages and increased public expenditures on healthcare and pensions, which can strain public finances. Socially, it can result in changes in family structures and increased demand for elder care. Politically, it can influence voting patterns and policy priorities. The R code below generates Figure 16.7, which illustrates the Age Dependency Ratio for the United States: # Load the quantmod package to downloaded data with getSymbols() library(&quot;quantmod&quot;) # Download age dependency ratio getSymbols(Symbols = &quot;SPPOPDPNDOLUSA&quot;, src = &quot;FRED&quot;) # Plot age dependency ratio plot.zoo.rec(x = SPPOPDPNDOLUSA, xlab = &quot;Date&quot;, ylab = &quot;% of Working Age Population&quot;, main = &quot;Age Dependency Ratio for the United States&quot;, lwd = 2) 16.2.4 Productivity-Labor Trade-Off Another important trend observable in many developed economies is the increase in labor productivity coupled with a decrease in average working hours per worker. These two factors represent significant shifts in the way work is performed and have substantial implications for both individuals and societies. Labor productivity, typically measured as output per hour worked, reflects how efficiently labor input is utilized in the production process. An increase in labor productivity means that more output is being produced for each hour of labor input. On the other hand, average working hours per worker refer to the average amount of time an individual spends working during a specific period. A decrease in this measure implies that individuals are spending less time working while still maintaining or even increasing output levels. These trends can be understood in the context of the equation defining output (\\(Y\\)) as a function of labor productivity (\\(A\\)), employment (\\(E\\)), and hours per worker (\\(H\\)): \\[ Y = A \\times E \\times H \\] Figure 16.8 plots these variables over time for the United States (see the code chunk below for data download and plotting instructions). Figure 16.8: Output and Hours per Worker in U.S. Nonfarm Business Sector As depicted in Figure 16.8, labor productivity (\\(A\\)), measured as real output per worker, has been steadily increasing. Several factors contribute to this increase. Technological advancements have been key, enabling workers to produce more with less effort. Increased education and skills training have also made workers more efficient. Moreover, improvements in managerial practices, organizational structures, and production processes have further boosted productivity. Figure 16.8 also reveals that the average hours worked per worker (\\(H\\)) in the United States have been decreasing. This trend can be attributed to various factors. Legislation and labor market regulations have played a crucial role, with many countries implementing policies to reduce the standard work week. Societal shifts, such as increased importance placed on work-life balance, have also contributed. Furthermore, increased productivity has allowed for more output to be produced in less time, reducing the need for long working hours. Note that the variables in Figure 16.8 indices. This means that the final point of real output per hour - 449 in 2024 Q1 - carries no meaning on its own. Its significance emerges only when compared with other values in the index series (refer to Chapter 12.4.1 for more on index data). Contrarily, the time series on employment and hours worked could potentially be represented using absolute data, where each number would carry inherent meaning (for instance, employment of 200 million would imply 200 million employed individuals). However, the U.S. Bureau of Labor Statistics collects these series as indices primarily because it is easier to measure growth rates of employment and hours worked rather than absolute numbers. Therefore, they are represented as indices. The following R code can be used to generate Figure 16.8, which illustrates labor productivity and hours worked per worker over time: # Load the quantmod package to downloaded data with getSymbols() library(&quot;quantmod&quot;) # Download Y, A, E, and H SYM &lt;- list(&quot;OUTNFB&quot;, &quot;OPHNFB&quot;, &quot;PRS85006013&quot;, &quot;PRS85006023&quot;) IND &lt;- do.call(merge, lapply(SYM, getSymbols, src = &quot;FRED&quot;, auto.assign = FALSE)) IND &lt;- 100 * sweep(IND, 2, STATS = IND[&quot;1950-01&quot;], FUN = &quot;/&quot;) # Plot Y, A, E, and H plot.zoo.rec(x = IND, plot.type = &quot;single&quot;, xlab = &quot;Date&quot;, ylab = &quot;Indices, 1950 Q1 = 100&quot;, main = &quot;Output and Hours per Worker in U.S. Nonfarm Business Sector&quot;, col = 1:4, lwd = 2) legend_text &lt;- paste0(c(&quot;Real Output&quot;, &quot;Real Output per Hour&quot;, &quot;Employment&quot;, &quot;Hours Per Worker&quot;), &quot; in &quot;, format(index(tail(IND, 1)), &quot;%Y&quot;), quarters(index(tail(IND, 1))), &quot;: &quot;, round(tail(IND, 1)), c(paste(&quot; =&quot;, paste(round(tail(IND[,-1], 1)), collapse = &quot;*&quot;)), rep(&quot;&quot;, 3))) legend(x = &quot;topleft&quot;, legend = legend_text, col = 1:4, lwd = 2, bg=&quot;white&quot;) 16.2.5 Trend Inflation A common economic trend is trend inflation, or the steady, long-term rise in the general level of prices for goods and services. In economic terms, the level of real output (\\(Y\\)) in an economy is defined as the nominal output (\\(N\\)) divided by the price level (\\(P\\)): \\[ Y = \\frac{N}{P} \\] In this equation, nominal output represents the total monetary value of all goods and services produced in an economy without adjusting for inflation. The price level refers to the average prices of goods and services across the economy. The quotient of these two values gives us the real output, which measures the value of all goods and services produced in an economy after removing the effects of inflation. These variables are illustrated over time in Figure 16.9 for the United States (refer to the code chunk below for data download and plotting instructions). Figure 16.9: Real vs. Nominal Output in U.S. Nonfarm Business Sector Figure 16.9 clearly shows a long-term trend of rising price levels in the U.S. economy. In fact, the increase in the price level contributes almost as much to the growth in nominal output as does the growth in real output. This trend inflation is primarily driven by the monetary policy of central banks, which usually target a low and stable rate of inflation, such as 2% per year. This policy aims to stimulate steady economic growth without causing overheating or excessive inflation. Trend inflation has significant implications for an economy and its participants. On one hand, moderate inflation can stimulate economic activity by encouraging spending and investment. On the other hand, high inflation can erode purchasing power, create economic uncertainty, and disrupt economic planning. It’s essential to note that while nominal output can grow due to both real economic growth (more goods and services being produced) and inflation (each good or service costing more), real output isolates the growth component by adjusting for inflation. This allows for a more accurate assessment of economic growth (see Chapter @ref(real-vs.-nominal) for a discussion on real vs. nominal variables). The R code provided below can be used to generate Figure 16.9, which represents the evolution of nominal output, price level, and real output over time in the United States: # Load the quantmod package to downloaded data with getSymbols() library(&quot;quantmod&quot;) # Download Y, N, and P SYM &lt;- list(&quot;OUTNFB&quot;, &quot;PRS85006053&quot;, &quot;IPDNBS&quot;) IND &lt;- do.call(merge, lapply(SYM, getSymbols, src = &quot;FRED&quot;, auto.assign = FALSE)) IND &lt;- 100 * sweep(IND, 2, STATS = IND[&quot;1950-01&quot;], FUN = &quot;/&quot;) # Plot Y, N, and P plot.zoo.rec(x = IND, plot.type = &quot;single&quot;, xlab = &quot;Date&quot;, ylab = &quot;Indices, 1950 Q1 = 100&quot;, main = &quot;Real vs. Nominal Output in U.S. Nonfarm Business Sector&quot;, col = 1:3, lwd = 2) legend_text &lt;- paste0(c(&quot;Real Output&quot;, &quot;Nominal Output&quot;, &quot;Price Level&quot;), &quot; in &quot;, format(index(tail(IND, 1)), &quot;%Y&quot;), quarters(index(tail(IND, 1))), &quot;: &quot;, round(tail(IND, 1)), c(paste(&quot; =&quot;, paste(round(tail(IND[,-1], 1)), collapse = &quot;*&quot;)), rep(&quot;&quot;, 2))) legend(x = &quot;topleft&quot;, legend = legend_text, col = 1:3, lwd = 2, bg=&quot;white&quot;) 16.2.6 Decline in Labor Share The labor share (\\(s_L\\)) and the capital share (\\(s_C\\)) in the total cost of production represent the proportions of total output that are allocated to labor and capital, respectively. According to the equation: \\[ 1 = s_L+s_C \\] the labor and capital shares should add up to 1 (or 100% if expressed in percentage terms), indicating that all output is distributed to either labor or capital. In recent decades, a significant shift in these shares has been observed in many economies. Labor’s share in income (\\(s_L\\)) has been trending downward, while the capital share (\\(s_C\\)) has been increasing. This phenomenon is illustrated in Figure 16.10, which can be created with the provided R code for data download and plotting. Figure 16.10: Share in Cost to Produce Output in U.S. Nonfarm Business Sector Figure 16.10 demonstrates a decline in labor share and a rise in capital share in the United States since 1987. Although the available data on labor share is annual and only recorded since 1987, a more detailed analysis is possible with a labor share index. The index is calculated using growth rates to measure the extent to which output growth can be attributed to labor income growth. The growth rates are then cumulated to form the index. Although the labor share itself is less than 100%, the index can exceed 100 because its starting period can be set arbitrarily. It’s crucial to remember that an index value for a single period isn’t inherently meaningful. The value of the index lies in its comparative capability over different periods. Therefore, the trend of the index - its slope and curvature - is significant, not the individual point values (see Chapter 12.4.1 for an overview on index data). The labor share index is displayed in Figure 16.11. Figure 16.11: Labor Share Index of U.S. Nonfarm Business Sector Figure 16.11 reveals that the labor share has been in decline since the 1950s, with a particularly steep drop in the 2000s, stabilizing after the Great Recession. The decline in labor’s share of income is a topic of considerable debate and research among economists and policymakers. Several explanations have been proposed, including technological change, globalization, changes in labor market institutions, and the rise of “superstar” firms. Technological progress, especially automation and digital technologies, is frequently cited. As firms adopt technologies that replace tasks previously performed by workers, demand for labor in those areas decreases, which puts downward pressure on wages and labor’s share of income. Globalization and increased competition from low-wage countries have also affected wages in developed economies, particularly for lower-skilled workers. This contributes to the decrease in labor’s share of income. At the same time, capital’s share has increased, potentially driven by the same factors causing labor’s share to decrease. New technologies often require significant capital investments, and as these technologies become more prevalent, capital returns (and thus its share of income) may rise. Additionally, changes in market structures, such as the rise of large “superstar” firms with substantial market power, may also contribute to capital’s increased share. These firms can earn high profits, which are capital returns, thus increasing capital’s share of income. Use the following R code to generate Figure 16.10, which visualizes the trends in labor and capital shares over time: # Load the quantmod package to downloaded data with getSymbols() library(&quot;quantmod&quot;) # Download labor and capital share SYM &lt;- c(&quot;MPU4910141&quot;, &quot;MPU4910131&quot;) SHA &lt;- do.call(merge, lapply(SYM, getSymbols, src = &quot;FRED&quot;, auto.assign = FALSE)) # Plot labor and capital share plot.zoo.rec(x = SHA, plot.type = &quot;single&quot;, xlab = &quot;Date&quot;, ylab = &quot;Indices, 1950 Q1 = 100&quot;, main = &quot;Share in Cost to Produce Output in U.S. Nonfarm Business Sector&quot;, col = 1:2, lwd = 2) legend_text &lt;- paste0(c(&quot;Labor&#39;s Share&quot;, &quot;Capital&#39;s Share&quot;),&quot;, &quot;, paste(format(range(index(SHA)), &quot;%Y&quot;), collapse = &quot; - &quot;), &quot;: &quot;, apply(100 * SHA[range(index(SHA))], 2, paste, collapse = &quot; to &quot;), &quot;%&quot;) legend(x = &quot;left&quot;, legend = legend_text, col = 1:2, lwd = 2, bg=&quot;white&quot;) This R code will produce Figure 16.11, which plots the labor share index: # Load the quantmod package to downloaded data with getSymbols() library(&quot;quantmod&quot;) # Download labor share index LAB &lt;- getSymbols(&quot;PRS85006173&quot;, src = &quot;FRED&quot;, auto.assign = FALSE) # Plot labor share index plot.zoo.rec(x = LAB, col = 1, lwd = 2, xlab = &quot;Date&quot;, ylab = &quot;Index, 2012 = 100&quot;, main = &quot;Labor Share Index of U.S. Nonfarm Business Sector&quot;) LABr &lt;- LAB[range(index(LAB))] legend_text &lt;- paste0(&quot;Labor Share Index (&quot;, paste(format(index(LABr), &quot;%b %Y&quot;), collapse = &quot; - &quot;), &quot;: &quot;, round(100 * diff(log(LABr))[2], 1), &quot;%)&quot;) legend(x = &quot;bottomleft&quot;, legend = legend_text, col = 1, lwd = 2, bg=&quot;white&quot;) 16.3 Seasonality Seasonal patterns or seasonal cycles are predictable patterns that recur each year and are driven by the seasons. They are a common occurrence in numerous economic indicators, exhibiting variation due to a multitude of factors such as the holiday season, weather conditions, and seasonal employment. These patterns can distort the real trends in data. For instance, an economic indicator may show a contraction compared to the previous period, suggesting a downturn. However, this could simply reflect a seasonal pattern rather than an economic downturn. Therefore, to examine the underlying trends and business cycles, economists often use seasonally adjusted data. These are data from which seasonal effects have been removed, making it easier to observe the non-seasonal trends. More information on seasonal adjustment can be found in Chapter 13.7.3. 16.3.1 Holiday Season Sales data often exhibit a significant seasonal pattern, characterized by a pronounced peak in December and a subsequent dip in January. This trend is driven by consumer shopping behavior during the holiday season, particularly Christmas and New Year’s Day, when people tend to buy more gifts, decorations, food, and beverages. The January dip can be attributed to post-holiday frugality, when consumers typically cut back on spending after the festive season. Figure 16.12: U.S. Retail and Food Services Sales This cyclical pattern can be seen in the Retail and Food Services Sales data from the U.S. Census Bureau (see the R code below to plot this data). The resulting Figure 16.12 shows the strong seasonality of retail and food services sales, reflecting the considerable impact of holiday shopping on consumer spending behavior. The R code to generate Figure 16.12 is: # Load the quantmod package to downloaded data with getSymbols() library(&quot;quantmod&quot;) # Download retail and food services sales SYM &lt;- list(&quot;RSAFSNA&quot;, &quot;RSAFS&quot;) RET &lt;- do.call(merge, lapply(SYM, getSymbols, src = &quot;FRED&quot;, auto.assign = FALSE)) # Express in billions of dollars instead of millions RET &lt;- RET / 1000 # Plot retail and food services sales plot.zoo.rec(x = RET, col = c(4, 1), lwd = 2, plot.type = &quot;single&quot;, xlab = &quot;Date&quot;, ylab = &quot;Billions of Dollars&quot;, main = &quot;U.S. Retail and Food Services Sales&quot;, ylim = range(RET) * c(.8, 1.1)) # Add legend legend(x = &quot;topleft&quot;, legend = c(&quot;Not Seasonally Adjusted (NSA)&quot;,&quot;Seasonally Adjusted (SA)&quot;), col = c(4, 1), lwd = 2, bg=&quot;white&quot;) # Label seasonal peaks December &lt;- RET$RSAFSNA[format(index(RET), &quot;%m&quot;) == &quot;12&quot;] text(index(December), coredata(December), &quot;Dec&quot;, cex = 0.65, pos = 3, col = 4) # Label seasonal troughs January &lt;- RET$RSAFSNA[format(index(RET), &quot;%m&quot;) == &quot;01&quot;] text(index(January), coredata(January), &quot;Jan&quot;, cex = 0.65, pos = 1, col = 4) 16.3.2 Seasonal Weather In many industries, weather conditions can have a substantial impact on economic activity. Construction is a prime example of this, as construction work often requires outdoor activity which is sensitive to weather conditions. Thus, construction spending exhibits a clear seasonal pattern: increasing in the warmer months and decreasing in the colder months. Figure 16.13: Construction Spending in the United States This pattern can be seen in Figure 16.13, which presents the U.S. construction spending data obtained from the U.S. Census Bureau (see R code below to download and plot this data). The plot reveals a pronounced seasonality in construction spending, where expenditures are noticeably lower during the winter months and peak during the summer, primarily due to weather conditions. The R code to generate Figure 16.13 is: # Load the quantmod package to downloaded data with getSymbols() library(&quot;quantmod&quot;) # Download total construction spending SYM &lt;- list(&quot;TTLCON&quot;, &quot;TTLCONS&quot;) CST &lt;- do.call(merge, lapply(SYM, getSymbols, src = &quot;FRED&quot;, auto.assign = FALSE)) # Express in billions of dollars instead of millions CST &lt;- CST / 1000 # Convert from annual rate to monthly rate CST$TTLCONS &lt;- CST$TTLCONS / 12 # Plot total construction spending plot.zoo.rec(x = CST, col = c(4, 1), lwd = 2, plot.type = &quot;single&quot;, xlab = &quot;Date&quot;, ylab = &quot;Billions of Dollars&quot;, main = &quot;Construction Spending in the United States&quot;, ylim = range(CST) * c(.8, 1.1)) # Add legend legend(x = &quot;topleft&quot;, legend = c(&quot;Not Seasonally Adjusted (NSA)&quot;,&quot;Seasonally Adjusted (SA)&quot;), col = c(4, 1), lwd = 2, bg=&quot;white&quot;) # Label seasonal peaks July &lt;- CST$TTLCON[format(index(CST), &quot;%m&quot;) == &quot;07&quot;] text(index(July), coredata(July), &quot;Jul&quot;, cex = 0.65, pos = 3, col = 4) # Label seasonal troughs January &lt;- CST$TTLCON[format(index(CST), &quot;%m&quot;) == &quot;01&quot;] text(index(January), coredata(January), &quot;Jan&quot;, cex = 0.65, pos = 1, col = 4) 16.3.3 Seasonal Employment Labor market activity, such as the rate of unemployment, often reflects seasonal patterns. The unemployment rate, in particular, is subject to variations throughout the year due to fluctuations in demand for labor across different seasons. This is particularly prominent in sectors where the work is seasonal, such as retail (higher demand during the holiday season), agriculture (higher demand during harvest season), and tourism (higher demand during vacation periods). Figure 16.14: U.S. Unemployment Rate Figure 16.14 demonstrates the U.S. unemployment rate’s seasonal and nonseasonal patterns, obtained from the U.S. Bureau of Labor Statistics (see R code below to download and plot this data). The plot shows a clear seasonal pattern in the unemployment rate, which typically peaks in January and June/July, reflecting the end of the holiday and school years respectively. The R code to generate Figure 16.14 is: # Load the quantmod package to downloaded data with getSymbols() library(&quot;quantmod&quot;) # Download unemployment rate SYM &lt;- list(&quot;UNRATENSA&quot;, &quot;UNRATE&quot;) UNR &lt;- do.call(merge, lapply(SYM, getSymbols, src = &quot;FRED&quot;, auto.assign = FALSE, from = as.Date(&quot;2000-01-01&quot;))) # Plot unemployment rate plot.zoo.rec(x = UNR, col = c(4, 1), lwd = 2, plot.type = &quot;single&quot;, xlab = &quot;Date&quot;, ylab = &quot;%&quot;, main = &quot;U.S. Unemployment Rate&quot;, ylim = range(UNR) * c(.8, 1.1)) # Add legend legend(x = &quot;topleft&quot;, legend = c(&quot;Not Seasonally Adjusted (NSA)&quot;,&quot;Seasonally Adjusted (SA)&quot;), col = c(4, 1), lwd = 2, bg=&quot;white&quot;) # Label seasonal peaks January &lt;- UNR$UNRATENSA[format(index(UNR), &quot;%m&quot;) == &quot;01&quot;] text(index(January), coredata(January), &quot;Jan&quot;, cex = 0.55, pos = 3, col = 4) June &lt;- UNR$UNRATENSA[format(index(UNR), &quot;%m&quot;) == &quot;06&quot;] text(index(June), coredata(June), &quot;Jun&quot;, cex = 0.4, pos = 3, col = 3) # Label seasonal troughs January &lt;- UNR$UNRATENSA[format(index(UNR), &quot;%m&quot;) == &quot;04&quot;] text(index(January), coredata(January), &quot;Apr&quot;, cex = 0.55, pos = 1, col = 4) December &lt;- UNR$UNRATENSA[format(index(UNR), &quot;%m&quot;) == &quot;12&quot;] text(index(December), coredata(December), &quot;Dec&quot;, cex = 0.4, pos = 1, col = 3) 16.4 Business Cycle A business cycle refers to non-seasonal fluctuations in economic activity around the trend. These cycles typically consist of periods of expansion (or boom) with growth in real output and periods of contraction (or recession) with a decline in real output. The peak of the cycle represents the end of an expansion and the start of a recession, while the trough marks the end of a recession and the beginning of an expansion. 16.4.1 Costs of Business Cycles Business cycles can have substantial effects on employment, income, consumption, and overall economic stability. This chapter will discuss the various costs associated with business cycles, highlighting the implications for businesses, households, and policymakers. Figure 16.15: Cost of Business Cycle The diagram depicted in Figure 16.15 illustrates the dynamic relationship between full employment (represented by a dashed line) and actual employment (denoted by a solid line) throughout business cycles. The traditional view of business cycles is that during a recession, both employment and production fall below their potential levels. This state continues from the onset of the recession until full recovery is achieved, signifying the start of a new recessionary cycle. This employment gap has a consequential cost: lost output. The difference between potential output (the economic output possible at full employment) and actual output (at actual employment) represents this lost production. The inability to reach full employment during downturns results in diminished production and consumption, reflecting one of the most significant costs of business cycles. Another cost of business cycles is economic uncertainty. Fears around the timing and impact of the next downturn lurk during expansion periods, while concerns about the duration and depth of a downturn amplify during recessions. This uncertainty can lead to cautious consumption and investment behavior, as businesses and households may choose to delay decisions, inadvertently exacerbating the economic downturn. Another form of uncertainty is the income instability for households that is caused by business cycles. During a recession, incomes may drop due to job loss or wage reduction. Conversely, during an expansion, incomes might increase due to factors such as bonuses, overtime, or wage hikes. This ebb and flow can cause significant financial stress and reduce the overall well-being of households. Business cycles can also induce economic inefficiencies and resource misallocation. During boom periods, over-investment in certain sectors can lead to asset bubbles. When these bubbles burst, resources invested in those sectors can essentially go to waste. During downturns, productive resources such as labor and capital can be underutilized due to decreased demand, leading to economic inefficiency. Due to these costs, business cycles necessitate the implementation of stabilization policies by governments and central banks. During a recession, policies like increased government spending or tax reductions may be employed to stimulate the economy, while during an expansion, contractionary measures such as spending cuts or tax hikes might be used to prevent overheating. These measures, while necessary for economic stability, are not without costs. For instance, expansionary fiscal policies can lead to higher public debt, while contractionary policies might result in job losses and reduced public services. 16.4.2 Cyclicality and Timing Understanding macroeconomic indicators and their behavior during various phases of the business cycle can provide policymakers and investors with vital insights. These indicators can be classified as either “procyclical”, “countercyclical”, or “acyclical”, depending on their behavior in relation to the broader economic cycle. Procyclical indicators move in the same direction as the economy, meaning they increase during an economic expansion and decrease during a contraction. Conversely, countercyclical indicators move in the opposite direction, increasing when the economy is slowing down and decreasing during an expansion. Acyclical indicators, on the other hand, are those that are generally independent of the economic cycle. Furthermore, these indicators can also be classified as “leading”, “lagging”, or “coincident”. Leading indicators change before the overall economy does, thus providing a glimpse of future economic activity. Lagging indicators, on the other hand, change after the economy has already begun to follow a specific pattern, offering confirmation that certain patterns have occurred. Lastly, coincident indicators change at approximately the same time as the overall economy, accurately reflecting the current state of economic activity. With a solid understanding of these economic indicators, policymakers and investors can anticipate economic trends and make informed decisions. 16.4.2.1 Procyclical Indicators Procyclical indicators are those that move in the same direction as the overall economy. They increase during periods of economic expansion and decrease during periods of contraction. Real GDP: This is a measure of the total output of the economy adjusted for inflation. It is procyclical, as it rises during economic expansions when output increases, and falls during recessions when output decreases. Employment: The number of individuals employed is also procyclical. It rises during periods of economic expansion as firms increase production and hire more workers, and it decreases during recessions when firms cut back on production. The observed association, wherein an increase in employment typically correlates with a rise in GDP, is referred to as Okun’s Law. Consumption and investment: These components of GDP are procyclical as well. During periods of economic growth, income rises, boosting consumer confidence and leading to increased consumption. Higher business confidence also spurs investment. Conversely, during downturns, consumption and investment tend to decrease. Interestingly, consumption is less vulnerable to the fluctuations of business cycles compared to investment, a concept explored in Chapter 16.4.3. Interest rates: During periods of economic expansion there are more investment opportunities than during recessionary periods. Hence, during a boom, there is a higher demand for loans driving up the interest rate. Conversely, in a downturn, demand for loans can decrease, causing interest rates to fall. Moreover, central banks often adjust interest rates in a procyclical manner. They may lower interest rates to stimulate borrowing and investment during recessions and raise rates to prevent overheating and control inflation during periods of economic growth. Inflation: Inflation is generally considered a procyclical indicator. During periods of economic expansion, the increased demand for goods and services tends to push prices up, leading to inflation. This phenomenon occurs as consumers and businesses have more disposable income, increasing spending, which puts upward pressure on prices if the supply of goods and services cannot be quickly scaled up. Conversely, during economic downturns or recessions, demand for goods and services often decreases as consumers tighten their belts and businesses cut back on spending. This reduced demand can lead to lower prices, thus lowering inflation or even leading to deflation in some instances. The relationship between inflation and the business cycle is often examined in light of the Philips Curve, a concept that suggests an inverse relationship between inflation and unemployment. As per this theory, in times of economic expansion, as inflation rises, unemployment falls and vice-versa. 16.4.2.2 Countercyclical Indicators Countercyclical indicators move in the opposite direction of the overall economy. They decrease during economic expansions and increase during economic contractions. Unemployment rate: The unemployment rate is a key countercyclical indicator. It decreases during economic expansions as firms hire more labor and increases during economic contractions as firms lay off workers. Government spending: While not always the case, government spending is often countercyclical. In economic downturns, governments may increase spending in an attempt to stimulate the economy. This is particularly evident in the implementation of fiscal stimulus measures during recessions. Savings rate: The savings rate, or the proportion of disposable income that households save rather than spend, can also be countercyclical. During economic downturns, uncertainty may lead households to save more, and during expansions, they may save less and spend more. Slope of yield curve: When the economy is recovering (recession phase), the yield curve tends to steepen as investors anticipate higher inflation and higher short-term interest rates in the future. In contrast, during an expansion, just before it reaches the peak, the yield curve tends to flatten or even invert. This inversion happens as investors expect lower inflation and lower short-term interest rates in the future as investors expect the economy to cool down. Therefore, the yield curve slope behaves countercyclically. It increases during economic downturns (signaling an expected recovery) and decreases during economic expansions (signaling an expected slowdown or recession). While not strictly classified as economic indicators, fiscal and monetary policies often operate in a countercyclical fashion to stabilize the economy: Countercyclical fiscal policy: This refers to the government’s fiscal policies that act in opposition to the business cycle. For example, governments may choose to increase spending and cut taxes during an economic downturn, and cut spending and raise taxes during an economic upturn. Countercyclical monetary policy: Central banks use monetary policy tools to counteract the business cycle. During an economic downturn, they tend to lower interest rates to stimulate borrowing and investment, and hence boost economic activity. Conversely, during an economic upturn, they may increase interest rates to keep the economy from overheating and control inflation. The aim is to reduce the magnitude of economic fluctuations, contributing to economic stability. Tools of monetary policy include open market operations, the discount rate, and the reserve requirements. 16.4.2.3 Acyclical Indicators Acyclical economic indicators are those that do not show a clear pattern of correlation with the phases of a business cycle. These indicators do not consistently rise or fall with economic expansions or contractions. Some examples include: Real Wages: Real wages, which are wages adjusted for inflation, are often considered acyclical. While they may rise during economic expansions due to increased demand for labor, other factors, such as changes in productivity and the labor market, can influence real wages, making their relationship with the business cycle less straightforward. Population: The growth rate of the population does not typically show a clear relationship with the business cycle. It is generally determined by birth, death, and migration rates rather than economic conditions. Technological Progress: Technological innovations and their adoption in an economy do not strictly follow the business cycle. Instead, they depend more on research and development efforts, invention, and innovation. 16.4.2.4 Leading Indicators Leading indicators predict changes in the economy, shifting before the economy as a whole shifts. Investment: Investment is a leading indicator as companies and people often adjust their investment levels based on their expectations of future economic conditions. Increased investment may indicate confidence in future economic growth, while decreased investment can signal anticipation of a downturn. Investment is forward-looking because it involves deploying resources now with the expectation of gaining some return or benefit in the future: Residential Investment: When someone purchases a house, they are making a long-term commitment to pay for it over the span of decades. As such, the decision to purchase a house is based not just on current economic conditions, but also on expectations of future economic stability and growth. For example, if potential homeowners expect their income to rise, they might be more willing to take on a mortgage. Therefore, an increase in residential investment can be an early sign of expected economic growth. Business Investment: Similarly, when a company decides to build a new factory, open a new store, or invest in new technology, it’s looking at the expected future profits from that investment. If businesses are optimistic about the future, they are more likely to make these kinds of investments. As such, increases in business investment can precede periods of economic expansion, making business investment a leading indicator. Interest rates: Interest rates are a function of investment, making them leading indicators of economic change. Moreover, central banks often adjust rates before shifts in the broader economy become apparent. They lower rates to encourage borrowing and stimulate investment when they foresee a downturn, and they raise rates to moderate investment and prevent overheating when they anticipate economic growth. Slope of the yield curve: The yield curve is typically considered a leading indicator as it often changes shape prior to a shift in the broader economy. A flattening or inverting yield curve can signal an impending economic downturn. 16.4.2.5 Coincident Indicators Coincident indicators change at approximately the same time as the overall economy, accurately reflecting the current state of economic activity. Real GDP: Real GDP accurately reflects the current state of the economy, increasing during expansions and decreasing during contractions. Consumption: Consumption tends to respond to the current state of the economy, generally increasing during expansions and decreasing during downturns. 16.4.2.6 Lagging Indicators Lagging indicators change after the economy has already begun to follow a specific trend, offering confirmation that certain economic patterns have occurred. Unemployment Rate: The unemployment rate is a classic lagging indicator, as it tends to increase or decrease only after the economy has already started to expand or contract. Government spending: Government spending often changes after a shift in the economy. In response to an economic downturn, governments may increase spending to stimulate the economy, but these measures often take time to implement, hence it lags the economic cycle. Inflation: Inflation is typically a lagging indicator. It tends to rise after the economy has been growing for a while, and it often continues to slow after the onset of an economic downturn. Real Wages: Changes in real wages often lag behind changes in the economy. Wage increases can be delayed in an expansion due to contracts or other institutional delays, and similarly, wages might not fall immediately in a downturn due to factors such as labor laws or the unwillingness of firms to cut pay. Countercyclical fiscal policy and countercyclical monetary policy: These policies are usually implemented in response to changes in the economy and hence tend to lag the economic cycle. In conclusion, understanding these classifications of economic indicators—procyclical, countercyclical, acyclical, leading, lagging, and coincident—can help economists and policymakers analyse the current state of the economy, anticipate future economic conditions, and craft informed economic policies. 16.4.3 Consumption Smoothing Consumption smoothing is a concept in economics that suggests individuals prefer to have a stable path of consumption, choosing to spend at a consistent level over their lifetime, rather than fluctuating in sync with their income. When income levels are high, individuals are more likely to save. Conversely, during periods of low income, they either dip into savings or borrow to maintain a steady consumption level. Understanding the principle of consumption smoothing can help us unravel the complex relationship between various components of the Gross Domestic Product (GDP) and their behavior during different phases of business cycles. GDP, a widely used measure of aggregate output, reflects the total value of all goods and services produced in an economy over a specific period. The following equation represents GDP, denoted by \\(Y\\): \\[ Y = C + I + G + (X - M) \\] In this equation: \\(C\\) signifies consumption or the total spending by households on goods and services. \\(I\\) stands for investment, encompassing private expenditures on tools, plants, and equipment used for future goods and services production. \\(G\\) represents government expenditures, which includes total government spending. \\((X-M)\\) denotes net exports, capturing the difference between a country’s exports (\\(X\\)) and imports (\\(M\\)). Figure 16.16: GDP Components Figure 16.16 traces the evolution of GDP and its components from 1947 to the present day. Notably, the consumption line is smoother and less volatile than the investment line. This reflects the essence of the consumption smoothing hypothesis, which suggests that individuals prefer to maintain stable consumption patterns over time, even when their income (in this case, national income or GDP) fluctuates. On the other hand, the investment line experiences more significant swings, reflecting that investment levels are more sensitive to business cycle fluctuations. During periods of economic expansion, investments grow as businesses capitalize on positive economic conditions. Conversely, in periods of economic downturn, investments contract as businesses become more cautious, contributing to the pronounced volatility in the investment line. Figure 16.17: Log of GDP Components Figure 16.17 presents the logarithmic transformation of GDP and its components, enabling us to interpret the data in terms of cumulated growth rates. This transformation linearizes the exponential growth of the variables, making it easier to identify patterns in the earlier years. More details on interpreting logarithm measures can be found in Chapter 13.4. The relatively stable line for consumption compared to the noticeably more volatile line for investment serves as empirical evidence supporting the principle of consumption smoothing. The logarithmic transformation underlines the disparities in the growth rates of consumption and investment, further emphasizing that consumption changes tend to be more moderate, whereas investment growth can experience substantial fluctuations depending on economic conditions. Table 16.1: Key Statistics of GDP Growth Rates 1947 Q2 - 2024 Q1 \\(\\%\\Delta Y_t\\) \\(\\%\\Delta C_t\\) \\(\\%\\Delta I_t\\) \\(\\%\\Delta G_t\\) \\(\\%\\Delta X_t\\) \\(\\%\\Delta M_t\\) Mean 1.54 1.56 1.60 1.56 1.66 2.03 Standard Deviation 1.28 1.24 5.02 1.72 4.76 4.57 Autocorrelation 0.26 0.04 0.19 0.60 0.12 0.18 Correlation to \\(\\%\\Delta Y_t\\) 1.00 0.81 0.73 0.23 0.47 0.59 Correlation to \\(\\%\\Delta Y_{t+1}\\) 0.26 0.22 0.23 0.07 0.02 0.21 Correlation to \\(\\%\\Delta Y_{t+2}\\) 0.25 0.23 0.22 0.01 0.02 0.18 Correlation to \\(\\%\\Delta Y_{t-1}\\) 0.26 0.15 0.16 0.31 0.24 0.29 Correlation to \\(\\%\\Delta Y_{t-2}\\) 0.25 0.22 0.01 0.38 0.15 0.09 Table 16.1 provides a more granular analysis by listing the mean, standard deviations, and autocorrelations (correlation to the value of the previous quarter) of the growth rates of the GDP components. It also includes the correlation of these components with output growth in preceding and succeeding periods. A noteworthy observation from this table is that the standard deviation of investment growth is larger than that of consumption growth, implying more volatility. This pattern aligns with the consumption smoothing hypothesis, which posits that consumption is less sensitive to business cycle fluctuations than investment. Investment growth tends to be a leading indicator, meaning it often changes direction before output growth does, represented by \\(\\text{cor}(\\%\\Delta I_t,\\%\\Delta Y_{t+h})&gt;\\text{cor}(\\%\\Delta I_t,\\%\\Delta Y_{t-h})\\), for \\(h &gt;0\\). In other words, changes in investment can signal upcoming changes in output growth, providing early indications of shifts in the economy’s overall direction. In summary, the consumption smoothing principle tells us that consumption tends to be more stable over time, whereas investment can vary greatly depending on economic conditions. This knowledge is crucial in formulating and implementing economic policies to ensure stability and growth. To create Figure 16.16, which plots the components of GDP, you can use the following R code: # Load the quantmod package to downloaded data with getSymbols() library(&quot;quantmod&quot;) # Start date start_date &lt;- as.Date(&quot;1900-01-01&quot;) # Download GDP and its components getSymbols(&quot;GDP&quot;, src = &quot;FRED&quot;) # Total getSymbols(&quot;PCEC&quot;, src = &quot;FRED&quot;) # Private Consumption getSymbols(&quot;GPDI&quot;, src = &quot;FRED&quot;) # Private Investment getSymbols(&quot;GCE&quot;, src = &quot;FRED&quot;) # Government Expenditures getSymbols(&quot;EXPGS&quot;, src = &quot;FRED&quot;) # Export getSymbols(&quot;IMPGS&quot;, src = &quot;FRED&quot;) # Import # Plot GDP and its components plot.zoo.rec(x = merge(GDP, PCEC, GPDI, GCE, EXPGS - IMPGS), xlab = &quot;Date&quot;, ylab = &quot;Billions of USD&quot;, main = &quot;GDP Components, Annualized and SA&quot;, plot.type = &quot;single&quot;, col = 1:6, lwd = 2, lty = c(rep(1, 4), 2)) legend(x = &quot;topleft&quot;, col = 1:6, lwd = 2, lty = c(rep(1, 4), 2), legend = c(&quot;GDP&quot;, &quot;Private Consumption&quot;, &quot;Private Investment&quot;, &quot;Government Expenditures&quot;, &quot;Net Export&quot;)) Moving on, if you want to replicate Figure 16.17, which displays the log of the GDP components, the subsequent R code serves as your guide: # Plot log of GDP and the log of its components plot.zoo.rec(x = log(merge(GDP, PCEC, GPDI, GCE, EXPGS, IMPGS)), xlab = &quot;Date&quot;, ylab = &quot;Log&quot;, main = &quot;Log of GDP Components, Annualized and SA&quot;, plot.type = &quot;single&quot;, col = 1:6, lwd = 2, lty = c(rep(1, 4), rep(2, 2))) legend(x = &quot;bottomright&quot;, col = 1:6, lwd = 2, lty = c(rep(1, 4), rep(2, 2)), legend = c(&quot;GDP&quot;, &quot;Private Consumption&quot;, &quot;Private Investment&quot;, &quot;Government Expenditures&quot;, &quot;Export&quot;, &quot;Import&quot;)) Finally, to generate Table 16.1, detailing the mean, standard deviations, and autocorrelations of the growth rates of GDP components, as well as the correlations with lead and lags of output growth, utilize the following R code: # Load the knitr package to make tables library(&quot;knitr&quot;) # Compute growth rate of GDP and its components GDP_growth &lt;- na.omit(100 * diff(log(merge(GDP, PCEC, GPDI, GCE, EXPGS, IMPGS)))) # Get the data&#39;s start and end date: GDP_growth_periods &lt;- paste(range(as.yearqtr(index(GDP_growth))), collapse = &quot; - &quot;) # Compute statistics GDP_growth_table &lt;- rbind( &quot;Mean&quot; = apply(GDP_growth, 2, mean), &quot;Standard Deviation&quot; = apply(GDP_growth, 2, sd), &quot;Autocorrelation&quot; = apply(GDP_growth, 2, function(x) cor(x, lag.xts(x, k = 1), use = &quot;complete.obs&quot;)), &quot;Correlation to $\\\\%\\\\Delta Y_t$&quot; = apply(GDP_growth, 2, cor, y = GDP_growth$GDP), &quot;Correlation to $\\\\%\\\\Delta Y_{t+1}$&quot; = apply(GDP_growth, 2, cor, y = lag.xts(GDP_growth$GDP, k = -1), use = &quot;complete.obs&quot;), &quot;Correlation to $\\\\%\\\\Delta Y_{t+2}$&quot; = apply(GDP_growth, 2, cor, y = lag.xts(GDP_growth$GDP, k = -2), use = &quot;complete.obs&quot;), &quot;Correlation to $\\\\%\\\\Delta Y_{t-1}$&quot; = apply(GDP_growth, 2, cor, y = lag.xts(GDP_growth$GDP, k = 1), use = &quot;complete.obs&quot;), &quot;Correlation to $\\\\%\\\\Delta Y_{t-2}$&quot; = apply(GDP_growth, 2, cor, y = lag.xts(GDP_growth$GDP, k = 2), use = &quot;complete.obs&quot;)) colnames(GDP_growth_table) &lt;- c( &quot;$\\\\%\\\\Delta Y_t$&quot;, &quot;$\\\\%\\\\Delta C_t$&quot;, &quot;$\\\\%\\\\Delta I_t$&quot;, &quot;$\\\\%\\\\Delta G_t$&quot;, &quot;$\\\\%\\\\Delta X_t$&quot;, &quot;$\\\\%\\\\Delta M_t$&quot; ) # Make nice looking table with statistics kable(round(GDP_growth_table, 2), caption = paste(&quot;Key Statistics of GDP Growth Rates&quot;, GDP_growth_periods)) "],["regional-patterns.html", "Chapter 17 Regional Patterns 17.1 Regional Data 17.2 County-Level Patterns 17.3 State-Level Patterns 17.4 Country-Level Patterns", " Chapter 17 Regional Patterns 17.1 Regional Data The economy does not operate uniformly across regions. Variations in economic performance can be observed at different geographical levels: from county to state, and state to country. This chapter delves into understanding these regional patterns and the factors influencing them. 17.2 County-Level Patterns At the county level, economic performance can vary significantly due to factors such as the availability of natural resources, the local industrial mix, population demographics, and the quality of local institutions. For instance, counties with abundant natural resources may experience economic booms when commodity prices are high. On the other hand, counties with a high concentration of manufacturing jobs might suffer during a downturn in the industrial sector. County-level economic data can help local policymakers understand the economic conditions of their jurisdictions and develop strategies to promote economic growth and resilience. Examples of relevant metrics at this level include employment rates, median income, poverty rates, and business activity. 17.3 State-Level Patterns State-level economic patterns can be shaped by a variety of factors including state tax policies, quality of infrastructure, education levels, and the presence of large corporations. For instance, states with favorable business climates may attract more companies, leading to higher employment rates and stronger economic growth. Metrics commonly used to analyze state-level economic performance include GDP per capita, unemployment rate, and measures of income inequality. Comparing these metrics across states can highlight disparities and potential areas for policy intervention. 17.4 Country-Level Patterns At the country level, economic performance is influenced by factors such as national policies, international trade relations, technological advancement, and geopolitical stability. Differences in these factors can lead to wide disparities in economic outcomes across countries. Key metrics for analyzing country-level economic performance include GDP growth rate, inflation rate, balance of trade, and Human Development Index (HDI). Additionally, tools like the GINI index are used to compare income inequality across countries. International organizations such as the World Bank, the International Monetary Fund (IMF), and the Organisation for Economic Co-operation and Development (OECD) regularly publish country-level economic data. These resources can be valuable for comparing economic performance across countries and understanding the global economic landscape. Trend Inflation vs. Money Growth Milton Friedman’s seminal work “Dollars and Deficits” (1968) provides a classic illustration of country-level analysis. Friedman plotted inflation against money growth, demonstrating a clear correlation. Countries with historically high rates of money growth also exhibited high rates of inflation and vice versa. Based on this pattern, he famously concluded that “inflation is always and everywhere a monetary phenomenon” (Milton Friedman, 1968, “Dollars and Deficits”, Prentice-Hall, p.39). Figure 17.1: Average Inflation Rate vs. Average Growth of Money Figure 17.1 presents an updated version of Friedman’s original analysis, reinforcing his findings. The data still show that countries with high money growth generally experience high inflation and vice versa. The R code used to generate Figure 17.1 is given below: #------------------------------------------------------------------------------- # Get country names #------------------------------------------------------------------------------- # Download country and region names from Wikipedia library(&quot;rvest&quot;) wiki_html &lt;- read_html(&quot;https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2&quot;) wiki_tables &lt;- html_table(html_elements(wiki_html, &quot;tbody&quot;), fill=TRUE) alpha2 &lt;- merge(wiki_tables[[5]], wiki_tables[[6]], by = &quot;Code&quot;, all = TRUE) alpha2$name &lt;- alpha2$`Country name (using title case)` alpha2$name[is.na(alpha2$name)] &lt;- alpha2$`Area name or country name`[is.na(alpha2$name)] alpha2 &lt;- setNames(na.omit(alpha2[, c(&quot;Code&quot;, &quot;name&quot;)]), c(&quot;alpha2&quot;, &quot;name&quot;)) # Rename some of the countries alpha2$name[alpha2$alpha2 == &quot;RU&quot;] &lt;- &quot;Russia&quot; alpha2$name[alpha2$alpha2 == &quot;GB&quot;] &lt;- &quot;Great Britain&quot; alpha2$name[alpha2$alpha2 == &quot;US&quot;] &lt;- &quot;USA&quot; alpha2$name[alpha2$alpha2 == &quot;KR&quot;] &lt;- &quot;South Korea&quot; #------------------------------------------------------------------------------- # Transform and combine data #------------------------------------------------------------------------------- # Compute annualized M1 growth money_inflation &lt;- data_FRED money_inflation[, symbols_all[, &quot;M1&quot;]] &lt;- 12 * 100 * diff.xts(money_inflation[, symbols_all[, &quot;M1&quot;]], log = TRUE) # Organize data as a &quot;data.table&quot;-object library(&quot;data.table&quot;) MI &lt;- as.data.table(money_inflation, keep.rownames = &quot;index&quot;) MI &lt;- melt(MI, id.vars = &quot;index&quot;, variable.name = &quot;symbol&quot;, value.name = &quot;value&quot;) CTRY &lt;- as.data.table(symbols_all, keep.rownames = &quot;alpha2&quot;) CTRY &lt;- melt(CTRY, id.vars = &quot;alpha2&quot;, variable.name = &quot;variable&quot;, value.name = &quot;symbol&quot;) MI &lt;- CTRY[MI, on = c(&quot;symbol&quot;)] MI &lt;- as.data.table(alpha2)[MI, on = &quot;alpha2&quot;] MI[variable == &quot;M1&quot;, variable:= &quot;M1 Growth&quot;] MI[variable == &quot;CPIInflation&quot;, variable:= &quot;CPI Inflation&quot;] MI &lt;- dcast(MI[, -c(&quot;symbol&quot;)], formula = ... ~ variable, value.var = &quot;value&quot;, drop = TRUE) MI &lt;- na.omit(MI) #------------------------------------------------------------------------------- # Plot figure #------------------------------------------------------------------------------- # Get R packages for ggplot(), stat_poly_line(), and geom_text_repel() library(&quot;ggplot2&quot;) library(&quot;ggpmisc&quot;) library(&quot;ggrepel&quot;) # Take the mean per country since 2000 MI2000 &lt;- MI[index &gt;= as.Date(&quot;2000-01-01&quot;), lapply(.SD, &quot;mean&quot;), by = .(alpha2, name), .SDcols = c(&quot;M1 Growth&quot;, &quot;CPI Inflation&quot;)] # Make a scatterplot ggplot(MI2000, aes(x = `M1 Growth`, y = `CPI Inflation`, label = name)) + stat_poly_line(fullrange = TRUE, size = .6, se = FALSE, col = rgb(0, 0, 0, .3)) + geom_point(size = 1, shape = 19, col = &quot;#006ddb&quot;) + geom_text_repel(size = 2, segment.size = .3, box.padding = .15, col = &quot;#006ddb&quot;) + stat_poly_eq(aes(label = after_stat(..eq.label..)), col = &quot;black&quot;, alpha = .6) + stat_poly_eq(aes(label = after_stat(..rr.label..)), label.y = 0.9, col = &quot;black&quot;, alpha = .6) + ggtitle(paste0(&quot;Average Inflation Rate vs. Average Growth of Money, 2000 - &quot;, format(Sys.Date(), &quot;%Y&quot;))) + theme_classic() + theme(plot.title = element_text(face = &quot;bold&quot;)) "],["microeconomic-patterns.html", "Chapter 18 Microeconomic Patterns 18.1 Microeconomic Data 18.2 Firm Patterns 18.3 Household Patterns", " Chapter 18 Microeconomic Patterns 18.1 Microeconomic Data 18.2 Firm Patterns 18.3 Household Patterns "],["causal-relationships.html", "Chapter 19 Causal Relationships 19.1 Elasticities 19.2 Multipliers 19.3 Causality", " Chapter 19 Causal Relationships Timer series and regional patterns are associative statements not causal statements. Leading indicator doesn’t mean it causes the other variables to move. Similarly, being a southern country doesn’t cause the country to have a lower economic performance. 19.1 Elasticities Elasticity is a concept used in economics to understand the relative changes between two variables. In the case of price elasticity of demand, it measures how the quantity demanded of a good changes in response to a change in the good’s price. # Load required libraries library(quantmod) library(dplyr) # Get commodity price data from World Bank (Example: Crude Oil prices) # Note: You need to find the right symbol or provide a direct URL to download the data. oil_data &lt;- getSymbols(&quot;Crude Oil (petroleum); Dubai Fateh&quot;, src = &quot;FRED&quot;) Here we can calculate the price elasticity of demand, which is the percentage change in quantity demanded divided by the percentage change in price. We would need data on quantity as well, which might need to be sourced from a different provider. 19.2 Multipliers In macroeconomics, multipliers refer to the effect an initial change in a specific input, such as government spending or tax rates, has on output. A classic example is the government spending multiplier, which measures the change in aggregate output (GDP) due to an initial change in government spending. # Get GDP and Government Spending data gdp &lt;- getSymbols(&quot;GDP&quot;, src = &quot;FRED&quot;) gov_spending &lt;- getSymbols(&quot;W068RCQ027SBEA&quot;, src = &quot;FRED&quot;) # symbol for Real federal government consumption expenditures and gross investment # Create a lagged version of the spending data gov_spending_lag &lt;- lag(gov_spending, 1) # Run a simple regression to estimate the multiplier model &lt;- lm(gdp ~ gov_spending + gov_spending_lag) # Print the estimated government spending multiplier summary(model)$coefficients[2, 1] This gives us an estimate of the government spending multiplier, which tells us how much GDP is expected to change in response to a one-unit change in government spending. Remember, when using these measures, always consider the underlying assumptions and the economic context. Both elasticity and multiplier values can vary significantly depending on the specifics of the situation and the timeframe being considered. 19.3 Causality Both multipliers and elasticities are causal concepts in the sense that they measure the effect of a change in one variable (the cause) on another variable (the outcome). However, establishing causal relationships from observational data is challenging and relies on several assumptions: Stable Unit Treatment Value Assumption (SUTVA): This assumption implies that there is no interference between units (i.e., the treatment on one unit does not affect the outcome on another unit) and that for each unit, there are potential outcomes for each level of treatment. Unconfoundedness or Ignorability: This assumption suggests that assignment to the treatment is independent of the potential outcomes, given a set of observed variables. In the context of the examples, this would mean that changes in the treatment variables (oil prices in the first example and government spending in the second) are not systematically related to other unobserved factors that could also be affecting the outcomes (demand for oil and GDP, respectively). Exogeneity: The changes in the independent variable are not influenced by changes in the dependent variable. In the two examples provided: In the elasticity example, we implicitly assume that changes in oil prices are not influenced by changes in oil demand (which is typically not true in real-world situations). In reality, supply and demand factors often interact in a complex way to determine oil prices, so isolating a causal effect in this way can be challenging. In the multiplier example, we assume that changes in government spending are not influenced by changes in GDP (or are sufficiently controlled by including lagged spending in the regression). In reality, government spending decisions are often influenced by the state of the economy, and this reverse causality can confound our estimates. These assumptions highlight why careful experimental or quasi-experimental design is important when trying to estimate causal relationships in economics, and why the results from such simple analyses should be interpreted with caution. In practice, economists often use more sophisticated techniques such as instrumental variables, difference-in-differences, or regression discontinuity designs to try to get around these issues and identify causal effects. "],["part-v.html", "Part V: Insights From Real-Time Data and Surveys", " Part V: Insights From Real-Time Data and Surveys Part V offers a comprehensive examination of the significance of subjective information, real-time data, and forecasts in understanding the behaviors and sentiments of individuals, businesses, and policymakers. The exploration of consumer sentiments reveals that subjective beliefs about the current and future economic states significantly influence decision-making processes, from individual purchases to business expansions. The importance of real-time data is further underscored, highlighting the discrepancies between initial macroeconomic indicators and their revised versions, which can notably impact interpretation of historical events. Part V concludes with an analysis of predictions made by decision-makers, evaluating their optimism, pessimism, and levels of consensus regarding the economy’s direction, shedding light on the uncertainties faced by decision makers. Included chapters: Chapter 20: “Why Subjective Information Matters” emphasizes the role of subjective beliefs and anticipations in economic decision-making, highlighting their tangible impact on the economy beyond objective measures. Chapter 21: “Insights From Consumer Surveys” delves into the University of Michigan’s Survey of Consumers, explaining its significance in capturing U.S. households’ economic perceptions and detailing its methodology, while also highlighting the creation and implications of key consumer indices like the ICS, ICC, and ICE. Chapter 22: “Insights From Real-Time Data” emphasizes the significance of real-time GDP and inflation data, highlighting how these figures can substantially differ from later revisions, which is crucial for accurate historical analysis like understanding the 1970s inflation surge. Chapter 23: “Insights From Forecaster Data” explores the forecasts of policymakers and businesses, assessing their sentiments about the future economy and the level of consensus or uncertainty among them. "],["why-subjective-information-matters.html", "Chapter 20 Why Subjective Information Matters", " Chapter 20 Why Subjective Information Matters Traditional economic indicators primarily focus on objective information, such as the quantity of goods produced in an economy. However, subjective factors, like beliefs and expectations, also play a crucial role in shaping the economy. Unlike some processes in physics, the economy is not driven solely by external factors. Instead, it involves decisions made by individuals, businesses, and policymakers based on their perceptions of the future. Consequently, the trajectory of an economy is influenced not only by its past but also by what people anticipate in the future. Moreover, people’s anticipations are not solely based on objective information; they are also influenced by subjective information, which can sometimes lead to mistakes with real implications. This chapter delves into the significance of such subjective information in economic analysis and decision-making, shedding light on how beliefs can have a tangible impact on the overall economy. Figure 20.1: Beliefs and their Impact on the Economy The diagram in Figure 20.1 illustrates how beliefs can affect the economy. When individuals and firms hold optimistic expectations, anticipating a significant rise in output or economic prosperity, they tend to adjust their behavior accordingly. This optimistic outlook leads to increased spending, heightened investment, and expanded production to cater to the expected high demand in the future. Conversely, when people and firms become overly pessimistic about the economic prospects, they tend to reduce their spending and investment, anticipating a decline in demand or adverse economic conditions. Consequently, this pessimistic sentiment results in lower actual output than would be the case if optimism prevailed. The significance of subjective information lies in its forward-looking nature. Economic decisions are rarely made based solely on historical data and past performances. Instead, they are heavily influenced by people’s perceptions of what lies ahead. The forward-looking aspect arises from the inherent time lags in various economic processes. For firms, the decision-making process involves planning and production, often taking months or even years before the final product reaches the market. Consequently, they must make choices based on their beliefs about future demand, market conditions, and overall economic stability. Similarly, households’ decisions are also forward-looking, particularly when it comes to long-term investments. For instance, investing in education involves anticipating future career prospects and employment opportunities. Likewise, purchasing a house entails evaluating the capacity to pay the mortgage in the future. Therefore, households make decisions today based on their beliefs about the future economic situation. Even policy makers are not immune to the influence of subjective information. When formulating policies to improve the economy, they must take into account the expectations and beliefs of various economic actors. Moreover, the impact of policy changes is often delayed, as it takes time for these changes to be fully transmitted and realized within the economy. For instance, a change in the policy rate may take months to influence overall inflation and other economic indicators. The interplay between beliefs and economic decisions creates a feedback loop that can amplify economic fluctuations. Optimistic beliefs can fuel economic expansions, while pessimistic beliefs can exacerbate downturns. This dynamic relationship underscores the significance of understanding and incorporating subjective information into economic analyses. In summary, beliefs and expectations play a crucial role in shaping economic outcomes. Subjective information influences economic decision making, leading to real-world consequences that impact production, consumption, investment, and overall economic growth. Acknowledging the significance of subjective information allows economists and policymakers to gain deeper insights into the mechanisms that drive economic fluctuations and to formulate more effective strategies for managing economic stability and growth. In the upcoming chapters, we will delve into datasets that capture subjective information, including survey data, forecaster data, real-time data, and textual information like newspapers and social media posts. We will learn how to utilize this subjective information to construct economic indicators such as consumer confidence indices, forecast disagreement indices, and text-based uncertainty indices. Embracing the subjectivity of economic decision-making will enable us to gain a more comprehensive understanding of the economy and enhance our forecasting capabilities. "],["insights-from-consumer-surveys.html", "Chapter 21 Insights From Consumer Surveys 21.1 Introduction 21.2 Conducting the Survey 21.3 Michigan Consumer Indices 21.4 Creating Custom Consumer Indices 21.5 Importing and Cleaning Individual-Level Surveys 21.6 Exploring Individual-Level Data 21.7 Replicate ICS 21.8 Conclusion", " Chapter 21 Insights From Consumer Surveys 21.1 Introduction The University of Michigan’s Survey of Consumers has been capturing the mood of U.S. households about the economy since the late 1940s. The monthly survey captures U.S. households’ views about the economy, providing essential insights into their perceptions and attitudes towards the U.S. economic landscape. The surveys play a significant role in shaping economic and policy decisions, given that consumer spending accounts for approximately 70% of the U.S. economy. The University of Michigan combines the survey into three key indices: the Index of Consumer Sentiment (ICS), the Index of Current Economic Conditions (ICC), and the Index of Consumer Expectations (ICE). The ICS, frequently cited in newspapers, economic studies, and financial reports, provides a comprehensive view of consumer confidence every month. The ICC concentrates on consumer attitudes towards the current economic environment, while the ICE forecasts future economic conditions. These famous consumer indices are based on just five out of the survey’s fifty questions. This chapter not only explains how the ICS, ICC, and ICE are made, but also looks at the rest of the survey data to create new consumer indices. 21.2 Conducting the Survey Detailed information about the University of Michigan’s Surveys of Consumers is readily accessible on their official website at data.sca.isr.umich.edu. The survey information section of the website provides critical details about the survey process. This includes a thorough survey description, an overview of the sample design, and the actual questionnaire used in the survey. Below is some of the information summarized. Sample Population The Surveys of Consumers samples adults 18 years of age and older residing in households in the continental U.S. It’s designed to be a representative sample of all such households. Sampling Method The survey uses a rotating panel design, with a new panel of respondents each month, selected to reflect the demographic and geographic distribution of the U.S. population. Data Collection Telephone interviews are the primary mode of data collection, although they have been increasingly integrating web-based interviews in recent years. Interviews typically last about 15 minutes. Survey Questions The survey consists of approximately 50 core questions, and the rest of the questionnaire can vary depending on current economic events. The core questions elicit information on respondents’ financial situation, buying attitudes, and expectations about future economic conditions. Survey Frequency and Timing Starting from the late 1940s, the Surveys of Consumers were initially conducted quarterly. However, since the mid-1970s, the frequency has increased to monthly. Each month’s survey involves about 500 interviews, with roughly 50-60% completed by the 10th of the month. Preliminary results are released mid-month, with remaining interviews completed by month-end. Final results are published at the beginning of the following month. 21.3 Michigan Consumer Indices 21.3.1 Index of Consumer Sentiment (ICS) The ICS is a composite index that captures consumers’ perceptions of their financial situation and attitudes about the economy in general. It is derived from five questions related to personal finances, general economic conditions, and purchasing conditions. To calculate the Index of Consumer Sentiment (ICS), we first compute the relative scores for each of the five index questions (\\(x_1,...,x_5\\) listed below). The relative score is the percent of favorable replies minus the percent of unfavorable replies, with 100 added. Each relative score is then rounded to the nearest whole number. The ICS is calculated using the following formula: \\[ ICS = \\frac{x_1+x_2+x_3+x_4+x_5}{6.7558} + 2.0 \\] The constant 2.0 is added to correct for sample design changes from the 1950s. The ICS is derived from the responses to the following five questions: \\(x_1\\) = PAGO_R: “We are interested in how people are getting along financially these days. Would you say that you (and your family living there) are better off or worse off financially than you were a year ago?” \\(x_2\\) = PEXP_R: “Now looking ahead–do you think that a year from now you (and your family living there) will be better off financially, or worse off, or just about the same as now?” \\(x_3\\) = BUS12_R: “Now turning to business conditions in the country as a whole–do you think that during the next twelve months we’ll have good times financially, or bad times, or what?” \\(x_4\\) = BUS5_R: “Looking ahead, which would you say is more likely–that in the country as a whole we’ll have continuous good times during the next five years or so, or that we will have periods of widespread unemployment or depression, or what?” \\(x_5\\) = DUR_R: “About the big things people buy for their homes–such as furniture, a refrigerator, stove, television, and things like that. Generally speaking, do you think now is a good or bad time for people to buy major household items?” The responses to these questions are combined to create the indices. For each question, there are a certain number of points assigned for positive, neutral, and negative responses. After the responses are scored, they are combined to create each index. The indices are then normalized to a base year (currently 1966 = 100) to create the final index values. 21.3.2 Index of Current Economic Conditions (ICC) The ICC specifically focuses on consumers’ perceptions of their current financial situation and whether it’s a good time to buy big-ticket items like cars and homes. It is based on responses to just two questions. The ICC is constructed from questions 1 and 5 from the ICS list using the following formula: \\[ ICC = \\frac{x_1+x_5}{2.6424} + 2.0, \\qquad \\] 21.3.3 Index of Consumer Expectations (ICE) The ICE aims to gauge consumer expectations for the future, specifically regarding their financial situation, general economic conditions, and purchasing conditions in the next year and next five years. It is derived from the remaining three questions. The ICE is constructed from questions 2, 3, and 4 from the ICS list using the following formula: \\[ ICE = \\frac{x_2+x_3+x_4}{4.1134} + 2.0 \\] 21.3.4 Importing in R This chapter illustrates how to import the three consumer indices, in addition to a selection of time series, which include the five that form the ICS, ICE, and ICC. For detailed descriptions of these time series, please refer to the Time-Series Variable Codebook provided on the survey’s website. In this chapter, we’ll demonstrate how to import a TSV file using real-world consumer survey data collected by the University of Michigan. This data is gathered through surveys that ask people about their opinions and feelings regarding the economy. It helps to understand how consumers perceive the current economic conditions and their expectations for the future. The data provides valuable insights into consumer behavior and helps economists, policymakers, and businesses make informed decisions. To obtain the Michigan consumer survey data, follow these steps: Visit the website of University of Michigan’s surveys of consumers by clicking here. Click on “DATA” in the menu bar, then select “Time Series.” On the data page, under Table, select “All: All Tables (Tab-delimited or CSV only)” to obtain the consumer survey data on all topics. To access all the consumer survey data since 1978, type “1978” under the “Oldest Year” option. Click on “Tab-Deliminated (Excel)” under the “format” option. Save the TSV file in a location of your choice, ensuring that it is saved in a familiar folder for easy access. The dataset contains 360 variables with coded column names such as ics_inc31 or pago_dk_all. To understand the meaning of these columns, you can visit the same website here and click on SURVEY INFORMATION. From there, select the Time-Series Variable Codebook which is a PDF document that provides detailed explanations for all the column names. By referring to this codebook, you can gain a better understanding of the variables and their corresponding meanings in the dataset. 21.3.5 Import TSV File TSV (Tab Separated Values) is a common file format used to store tabular data. As the name suggests, the values in each row of a TSV file are separated by tabs. Here’s an example of how data is stored in a TSV file: Male    8    100    3 Female    9    20    3 To import the consumer survey TSV file, you need to install and load the readr package if you haven’t done so already. Once the package is loaded, you can use either the read_tsv() or read_delim() function to read the TSV (Tab-Separated Values) file. # Load the package library(&quot;readr&quot;) # Import TSV file cs &lt;- read_tsv(file = &quot;files/sca-tableall-on-2023-Jul-01.tsv&quot;, skip = 1) # Import TSV file using the read_delim() function cs &lt;- read_delim(file = &quot;files/sca-tableall-on-2023-Jul-01.tsv&quot;, skip = 1, col_names = TRUE, delim = &quot;\\t&quot;) In the provided code snippets, the file input specifies the file path or URL of the TSV file to be imported. The skip input is used to specify the number of rows to skip at the beginning of the file. In this case, skip = 1 indicates that the first line of the TSV file, which contains the title “All Tables”, should be skipped. The col_names input is set to TRUE to indicate that the second line of the TSV file (after skipping 1 row) contains the column names. Lastly, the delim input is set to \"\\t\" to specify that the columns in the TSV file are separated by tabs, which is the standard delimiter for TSV (Tab Separated Values) files. Note that if the file is neither CSV nor TSV, but rather has an exotic format where columns are separated by a different character that is neither a comma nor a tab, such as “/”, you can use the read_delim() function with the delim = \"/\" argument to specify the custom delimiter. To inspect the first few rows of the data, print the cs object in the console. For an overview of the entire dataset, execute View(cs). # Display the data cs ## # A tibble: 545 × 360 ## Month yyyy ics_all ics_inc31 ics_inc32 ics_inc33 ics_a1834 ics_a3554 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1978 83.7 NA NA NA 93.7 86.7 ## 2 2 1978 84.3 NA NA NA 99.7 82.3 ## 3 3 1978 78.8 NA NA NA 91.7 76.8 ## 4 4 1978 81.6 NA NA NA 91.8 79.7 ## 5 5 1978 82.9 NA NA NA 95.1 78.9 ## 6 6 1978 80 NA NA NA 91.7 75.7 ## 7 7 1978 82.4 NA NA NA 92.2 78.4 ## 8 8 1978 78.4 NA NA NA 87.8 77.2 ## 9 9 1978 80.4 NA NA NA 86.6 83.9 ## 10 10 1978 79.3 NA NA NA 90.6 76.7 ## # ℹ 535 more rows ## # ℹ 352 more variables: ics_a5597 &lt;dbl&gt;, ics_ne &lt;dbl&gt;, ics_nc &lt;dbl&gt;, ## # ics_s &lt;dbl&gt;, ics_w &lt;dbl&gt;, icc_all &lt;dbl&gt;, ice_all &lt;dbl&gt;, pago_f_all &lt;dbl&gt;, ## # pago_s_all &lt;dbl&gt;, pago_u_all &lt;dbl&gt;, pago_dk_all &lt;dbl&gt;, pago_r_all &lt;dbl&gt;, ## # pagorn_hy_all &lt;dbl&gt;, pagorn_ha_all &lt;dbl&gt;, pagorn_ld_all &lt;dbl&gt;, ## # pagorn_ly_all &lt;dbl&gt;, pagorn_hp_all &lt;dbl&gt;, pagorn_la_all &lt;dbl&gt;, ## # pagorn_hd_all &lt;dbl&gt;, pagorn_ny_all &lt;dbl&gt;, pagorn_nad_all &lt;dbl&gt;, … Use sapply(cs, class) to check the data type of each column, to make sure all columns are indeed numeric: # Check the data type of each column table(sapply(cs, class)) ## ## logical numeric ## 1 359 Here, since there are 360 columns, the summary() function is applied, which reveals that there are 359 numerical columns, and 1 logical column, which makes sense. Instead of a date column, the consumer survey has a year (yyyy) and a month (Month) column. To create a date column from the year and month columns, combine them with the paste() function to create a date format of the form Year-Month-Day or %Y-%m-%d: # Create date column cs$Date &lt;- as.Date(paste(cs$yyyy, cs$Month, &quot;01&quot;, sep = &quot;-&quot;)) head(cs$Date) ## [1] &quot;1978-01-01&quot; &quot;1978-02-01&quot; &quot;1978-03-01&quot; &quot;1978-04-01&quot; &quot;1978-05-01&quot; ## [6] &quot;1978-06-01&quot; 21.3.6 Plotting Consumer Indices The Michigan Consumer Survey consists of a wide range of survey responses from a sample of households collected every month. These survey responses are gathered to produce indices about how consumers feel each period. The University of Michigan produces three main indices: the Index of Consumer Confidence (ICC), the Index of Current Economic Conditions (ICE), and the Index of Consumer Sentiment (ICS). These indices are designed to measure different aspects of consumer attitudes and perceptions regarding the economy. Index of Consumer Confidence (ICC): The ICC reflects consumers’ expectations about future economic conditions and their overall optimism or pessimism. It is based on consumers’ assessments of their future financial prospects, job availability, and economic outlook. A higher ICC value indicates greater consumer confidence and positive expectations for the economy. Index of Current Economic Conditions (ICE): The ICE assesses consumers’ perceptions of the current economic environment. It reflects their evaluations of their personal financial situation, job security, and their perception of whether it is a good time to make major purchases. The ICE provides insights into the current economic conditions as perceived by consumers. Index of Consumer Sentiment (ICS): The ICS combines both the ICC and ICE to provide an overall measure of consumer sentiment. It takes into account consumers’ expectations for the future as well as their assessment of the present economic conditions. The ICS is often used as an indicator of consumer behavior and their likelihood of making purchases and engaging in economic activities. These indices are calculated based on survey responses from a sample of households, and they serve as important indicators of consumer sentiment and economic trends. They are widely followed by economists, policymakers, and financial markets as they provide valuable insights into consumers’ attitudes and perceptions, which can impact their spending behavior and overall economic activity. Let’s use the plot() function to visualize the imported Michigan consumer survey data. In this case, we will plot the three key indices: ICC, ICE, and ICS over time, using the Date column as the x-axis and the three indices as the y-axis: # Plot ICC, ICE, and ICS over time plot(x = cs$Date, y = cs$icc_all, type = &quot;l&quot;, col = 5, lwd = 3, ylim = c(40, 140), xlab = &quot;Date&quot;, ylab = &quot;Index&quot;, main = &quot;Key Indices of the Michigan Consumer Survey&quot;) lines(x = cs$Date, y = cs$ice_all, col = 2, lwd = 2) lines(x = cs$Date, y = cs$ics_all, col = 1, lwd = 1.5) legend(x = &quot;topleft&quot;, legend = c(&quot;ICC&quot;, &quot;ICE&quot;, &quot;ICS&quot;), col = c(5, 2, 1), lwd = c(3, 2, 1.5), horiz = TRUE) In the code snippet provided, the appearance and behavior of the plot are customized using several functions and arguments: x: This argument specifies the data to be used for the x-axis of the plot. In this case, it is cs$Date, indicating the “Date” column of the Michigan consumer survey data. y: This argument specifies the data to be used for the y-axis of the plot. In this case, it is cs$icc_all, cs$ice_all, and cs$ics_all, representing the ICC, ICE, and ICS indices from the Michigan consumer survey data. type: This argument determines the type of plot to be created. In this case, it is set to \"l\", which stands for “line plot”. This will create a line plot of the data points. col: This argument specifies the color of the lines in the plot. In the code snippet, different colors are used for each index: 5 for ICC, 2 for ICE, and 1 for ICS. lwd: This argument controls the line width of the plot. It is set to 3 for ICC, 2 for ICE, and 1.5 for ICS, indicating different line widths for each index. ylim: This argument sets the limits of the y-axis. In this case, it is set to c(40, 140), which defines the range of the y-axis from 40 to 140. xlab: This argument specifies the label for the x-axis of the plot. In the code snippet, it is set to \"Date\". ylab: This argument specifies the label for the y-axis of the plot. In the code snippet, it is set to \"Index\". main: This argument specifies the main title of the plot. In the code snippet, it is set to \"Key Indices of the Michigan Consumer Survey\". lines: This function is used to add additional lines to the plot. legend: This function adds a legend to the plot. It is used to create a legend in the top-left corner (x = \"topleft\") with labels corresponding to each index (\"ICC\", \"ICE\", \"ICS\") and their respective line colors and widths. Figure 21.1: Key Indices of the Michigan Consumer Survey The resulting plot, shown in Figure 21.1, displays the historical evolution of the three Michigan consumer survey indices: ICC, ICE, and ICS. These indices are considered leading indicators because they provide early signals about changes in consumer sentiment and economic conditions. They often reflect consumers’ expectations and attitudes before these changes are fully manifested in traditional economic indicators, such as unemployment rates or GDP growth. Consumer sentiment plays a crucial role in shaping consumer behavior, including spending patterns, saving habits, and investment decisions. When consumer confidence is high, individuals are more likely to spend and invest, stimulating economic growth. Conversely, low consumer confidence can lead to reduced spending and investment, potentially dampening economic activity. Hence, these indices can serve as an early warning system for potential shifts in economic activity. By incorporating the consumer survey indices alongside traditional economic indicators, policymakers and analysts can gain a more comprehensive understanding of the economic landscape. While traditional indicators like unemployment rates provide objective measures of economic conditions, the consumer survey indices offer a subjective perspective, reflecting consumers’ beliefs, expectations, and intentions. This subjective insight can provide additional context and help anticipate changes in consumer behavior and overall economic activity. Therefore, by monitoring both traditional economic indicators and the Michigan consumer survey indices, policymakers and analysts can obtain a more holistic view of the economy, enabling them to make more informed decisions and implement timely interventions to support economic stability and growth. 21.4 Creating Custom Consumer Indices While the ICC, ICE, and ICS are the most well-known indices generated from the Michigan Consumer Survey data, researchers and analysts can create custom indices based on their specific interests. These custom indices might focus on specific demographics, geographic regions, or consumer attitudes about specific sectors of the economy. Consider, for example, an index that captures consumers’ attitudes towards the housing market. This could be particularly useful for real estate investors and homebuilders. Such an index could be created by focusing on survey questions about consumers’ perceptions of current housing prices, their expectations for future housing prices, and their perceptions of the current home buying conditions. The simplest way to create a new index is by combining several time series from the Time-Series Variable Codebook (see Chapter ?? on how to import this data in R). These time series aggregate individual responses into a single observation per time period. A more advanced approach to creating a new index involves working with the raw individual-level surveys then turning them into an index. This method allows for insights into the degree of disagreement among consumers, rather than just looking at averages. The next sections will explain how to work with these individual surveys and form them into time series. 21.5 Importing and Cleaning Individual-Level Surveys In this chapter, we will go through the process of importing and cleaning the individual-level Michigan Consumer Survey data. This process involves downloading the raw data, importing the required R packages, importing data labels, making them tidy, and finally cleaning the data set. Let’s get started. # Load necessary libraries library(&quot;tidyverse&quot;) library(&quot;xml2&quot;) 21.5.1 Data Source The data for this analysis is obtained from the University of Michigan Consumer Survey website. Follow the steps below to get the links: Visit data.sca.isr.umich.edu Click on “Data” - “Downloads” - “Cross-Section Archive” Click “CSV file (Comma Separated Values with header record)” and “DDI (XML)” Select “All” everywhere and click “continue” Click “Create the Files” Right click on the “Data file”, copy link, and save the link as data_file Right click on the “DDI XML file”, copy link, and save the link as data_label_file Here are the direct links used in this example: data_file &lt;- &quot;https://sda.umsurvey.org/tmpdir/AAcZmZrm.csv&quot; data_label_file &lt;- &quot;https://sda.umsurvey.org/tmpdir/AAIPR4qc.txt&quot; 21.5.2 Data Labels First, we import the labels and convert them into a tidy format, ready for analysis. This involves converting the imported list to a tibble and tidying up the labels. #================================================================================================ # Data Labels #================================================================================================ #------------------------------------------------------------------------------------------------ # Import labels #------------------------------------------------------------------------------------------------ raw_labels_list &lt;- read_xml(data_label_file) %&gt;% as_list %&gt;% as_tibble #------------------------------------------------------------------------------------------------ # Make labels tidy #------------------------------------------------------------------------------------------------ # Convert list to tibble raw_labels &lt;- raw_labels_list %&gt;% unnest_longer(codeBook, indices_to = &quot;folder&quot;) %&gt;% filter(folder == &quot;var&quot;) %&gt;% group_by(index = row_number()) %&gt;% mutate(variable = modify_in(codeBook, list(1), attributes)) %&gt;% unnest_wider(variable) %&gt;% select(-names, -folder) %&gt;% ungroup() %&gt;% select(-index) %&gt;% unnest_longer(codeBook, values_to = &quot;value_as_list&quot;, indices_to = &quot;feature&quot;) %&gt;% mutate(name = factor(name, levels = unique(name), ordered = TRUE), feature = factor(feature, levels = unique(feature), ordered = TRUE)) # Extract unnamed vs. named list elements raw_labels &lt;- bind_rows( # unnamed list elements raw_labels %&gt;% filter(lengths(value_as_list) &gt; 1) %&gt;% unnest_wider(value_as_list, transform = as.character), # named list elements raw_labels %&gt;% filter(lengths(value_as_list) &lt;= 1) %&gt;% hoist(value_as_list, labl = list(1, 1)) %&gt;% select(-value_as_list)) %&gt;% # back to original order arrange(name, feature) # Remove &quot;\\n&quot; and spaces raw_labels &lt;- raw_labels %&gt;% mutate(across(c(&quot;labl&quot;, &quot;catValu&quot;, &quot;txt&quot;), function(x) str_trim(str_remove_all(x, &quot;\\\\\\\\n&quot;))), dcml = as.integer(dcml), catValu = as.integer(catValu)) #------------------------------------------------------------------------------------------------ # Rename and extract relevant labels #------------------------------------------------------------------------------------------------ # Check out the following columns, which seem to be useless if(FALSE){ raw_labels %&gt;% filter(!is.na(item)) raw_labels %&gt;% filter(!is.na(range)) raw_labels %&gt;% filter(!is.na(txt) &amp; !is.na(labl)) raw_labels %&gt;% filter(feature == &quot;invalrng&quot; &amp; !is.na(labl)) raw_labels %&gt;% pull(feature) %&gt;% unique() } # Extract the relevant columns and make tidy var_labels &lt;- raw_labels %&gt;% select(variable = name, feature = feature, value_cat = catValu, info = labl, txt = txt, dcml = dcml) %&gt;% filter(!(is.na(info) &amp; is.na(value_cat)) &amp; feature %in% c(&quot;labl&quot;, &quot;qstn&quot;, &quot;catgry&quot;)) %&gt;% unite(&quot;info&quot;, c(&quot;info&quot;, &quot;txt&quot;), sep = &quot;_&quot;, na.rm = TRUE) %&gt;% mutate(feature = factor(feature, levels = levels(feature), labels = str_replace_all( levels(feature), c(&quot;labl&quot; = &quot;label_var&quot;, &quot;qstn&quot; = &quot;question&quot;, &quot;catgry&quot; = &quot;label_cat&quot;)))) %&gt;% pivot_wider(names_from = feature, values_from = info) %&gt;% mutate(label_var = str_replace_all(tools::toTitleCase(tolower(label_var)), c(&quot; Id &quot; = &quot; ID &quot;, &quot; Id$&quot; = &quot; ID&quot;, &quot;Rdd&quot; = &quot;RDD&quot;))) # Rename category &quot;NA&quot; = &quot;Not Applicable&quot; so that it is not confused with NA = missing var_labels &lt;- var_labels %&gt;% mutate(label_cat = ifelse(label_cat == &quot;NA&quot;, &quot;Not Applicable&quot;, label_cat)) # Remove unnecessary rows var_labels &lt;- var_labels %&gt;% group_by(variable) %&gt;% mutate(across(c(&quot;label_var&quot;, &quot;question&quot;), function(x) ifelse(is.na(x), first(x[!is.na(x)]), x))) %&gt;% filter(!(n() &gt; 1 &amp; is.na(label_cat))) %&gt;% ungroup() %&gt;% relocate(variable, label_var, dcml, value_cat, label_cat, question) #------------------------------------------------------------------------------------------------ # Create new variables and missing labels #------------------------------------------------------------------------------------------------ # Labels for new variables defined later new_variables &lt;- tribble( ~variable, ~label_var, ~dcml, ~question, &quot;DATE&quot;, &quot;Date&quot;, NA, &quot;Date&quot;, &quot;R&quot;, &quot;Respondent ID&quot;, 0L, paste(&quot;Respondent ID that remains fixed over time&quot;, &quot;if the respondent participates multiple times.&quot;) ) # Add new labels to existing labels var_labels &lt;- new_variables %&gt;% bind_rows(var_labels) # Labels for categories that were not properly defined new_categories &lt;- tribble( ~variable, ~value_cat, ~label_cat, &quot;HOMEAMT&quot;, 9999998L, &quot;DK&quot;, &quot;INVAMT&quot;, 99999998L, &quot;DK&quot; ) new_categories &lt;- var_labels %&gt;% select(-label_cat, -value_cat) %&gt;% distinct() %&gt;% right_join(new_categories, by = c(&quot;variable&quot;)) # Add new labels to existing labels var_labels &lt;- var_labels %&gt;% bind_rows(new_categories) %&gt;% mutate(variable = factor(variable, levels = c(&quot;CASEID&quot;, setdiff(unique(variable), &quot;CASEID&quot;)), ordered = TRUE)) %&gt;% arrange(variable, value_cat) # Print labels var_labels 21.5.3 Importing the Data Set The data is imported as a CSV file and cleaned. The data includes creating new date variables and respondent ID that remains constant across multiple survey participation. #================================================================================================ # Data Set #================================================================================================ #------------------------------------------------------------------------------------------------ # Import data set #------------------------------------------------------------------------------------------------ CS_raw &lt;- read_csv(data_file) CS_tidy &lt;- CS_raw #------------------------------------------------------------------------------------------------ # Create date variables #------------------------------------------------------------------------------------------------ CS_tidy &lt;- CS_tidy %&gt;% mutate(DATE = as.Date(paste0(CS_tidy$YYYYMM, &quot;01&quot;), format = &quot;%Y%m%d&quot;), DATEPREV = as.Date(paste0(CS_tidy$DATEPR, &quot;01&quot;), format = &quot;%Y%m%d&quot;)) %&gt;% relocate(CASEID, DATE, DATEPREV) #------------------------------------------------------------------------------------------------ # Respondents can participate multiple times -&gt; Generate respondent ID that remains fixed #------------------------------------------------------------------------------------------------ # ID and IDPREV count individuals from scratch every period, whereas CASEID is unique if(FALSE){ CS_tidy %&gt;% select(CASEID) %&gt;% duplicated() %&gt;% sum() CS_tidy %&gt;% select(ID) %&gt;% duplicated() %&gt;% sum() CS_tidy %&gt;% select(ID, DATE) %&gt;% duplicated() %&gt;% sum() } # Data matches ID/DATE with IDPREV/DATEPREV, but doesn&#39;t go further back - find oldest ID/DATE pair r_matches &lt;- CS_tidy %&gt;% filter(!is.na(IDPREV) &amp; !is.na(DATEPREV)) %&gt;% select(ID, DATE, IDPREV, DATEPREV) while(nrow(semi_join(r_matches, r_matches, by = c(&quot;IDPREV&quot; = &quot;ID&quot;, &quot;DATEPREV&quot; = &quot;DATE&quot;))) &gt; 0){ r_matches &lt;- r_matches %&gt;% left_join(r_matches, by = c(&quot;IDPREV&quot; = &quot;ID&quot;, &quot;DATEPREV&quot; = &quot;DATE&quot;), suffix = c(&quot;&quot;, &quot;2&quot;)) %&gt;% mutate(IDPREV = if_else(!is.na(IDPREV2), IDPREV2, IDPREV), DATEPREV = if_else(!is.na(DATEPREV2), DATEPREV2, DATEPREV)) %&gt;% select(-IDPREV2, -DATEPREV2) } # Create &quot;R&quot; (Respondent) variable that remains fixed over time r_matches &lt;- r_matches %&gt;% mutate(R = paste(IDPREV, DATEPREV, sep = &quot;_&quot;)) # Include &quot;R&quot; in the data set CS_tidy &lt;- CS_tidy %&gt;% left_join(select(r_matches, c(&quot;R&quot;, &quot;ID&quot;, &quot;DATE&quot;)), by = c(&quot;ID&quot;, &quot;DATE&quot;)) %&gt;% relocate(DATE, R) %&gt;% mutate(R = ifelse(test = is.na(IDPREV), yes = paste(ID, DATE, sep = &quot;_&quot;), no = R)) %&gt;% # Use numbers for &quot;R&quot; from 1 to length(unique(CS_tidy$R)) instead of R = ID_DATE mutate(R = as.numeric(factor(R, levels = unique(R)))) %&gt;% select(-DATEPREV) # Check how many times the same respondent participated in the survey if(FALSE){ CS_tidy %&gt;% filter(is.na(R)) CS_tidy %&gt;% count(R) %&gt;% group_by(n) %&gt;% summarize(freq = n(), sum = sum(n)) } # Add variable labels attr(CS_tidy, &quot;labels&quot;) &lt;- var_labels # Print data CS_tidy 21.5.4 Long Format Convert the data from wide to long format. This changes the layout of the data such that each row is an observation, and each column is a variable. #================================================================================================ # Long Format #================================================================================================ # Convert wide (= tidy, each column is a variable) to long format CS &lt;- CS_tidy %&gt;% pivot_longer(cols = -c(&quot;CASEID&quot;, &quot;R&quot;, &quot;DATE&quot;, &quot;WT&quot;), names_to = &quot;variable&quot;, values_to = &quot;value&quot;, values_transform = as.numeric, values_drop_na = TRUE) # Keep label attributes attr(CS, &quot;labels&quot;) &lt;- attr(CS_tidy, &quot;labels&quot;) # Add variable labels CS &lt;- CS %&gt;% left_join( y = attr(CS, &quot;labels&quot;) %&gt;% select(c(&quot;variable&quot;, &quot;label_var&quot;, &quot;dcml&quot;)) %&gt;% distinct(), by = &quot;variable&quot; ) # Add factor labels CS &lt;- CS %&gt;% mutate(value_cat = ifelse(dcml == 0, as.integer(value), NA_integer_)) %&gt;% left_join( y = attr(CS, &quot;labels&quot;) %&gt;% select(c(&quot;variable&quot;, &quot;label_cat&quot;, &quot;value_cat&quot;)), by = c(&quot;variable&quot;, &quot;value_cat&quot;) ) # Separate between categorical values and numerical values CS &lt;- CS %&gt;% mutate(value_cat = ifelse(!is.na(label_cat), value_cat, NA_integer_), value = ifelse(!is.na(value_cat), NA_real_, value)) # Organize data CS &lt;- CS %&gt;% mutate(variable = factor(variable, levels = unique(variable), ordered = TRUE)) %&gt;% arrange(DATE, R, variable, value_cat) %&gt;% relocate(CASEID, DATE, R, WT, variable, dcml, value, value_cat, label_cat, label_var) # Check if there is only one answer per CASEID, person, time period, and question: if(FALSE){ CS %&gt;% count() CS %&gt;% distinct(CASEID, variable) %&gt;% count() } # Print data CS 21.5.5 Saving the Data The cleaned data is saved as both CSV and RDS files under the downloads folder. # Create downloads folder dir.create(&quot;downloads&quot;) # Save data as RDS files saveRDS(CS_tidy, file.path(&quot;downloads&quot;, &quot;CS_tidy.rds&quot;)) saveRDS(CS, file.path(&quot;downloads&quot;, &quot;CS.rds&quot;)) # Save data as CSV files write_csv(CS_tidy, file.path(&quot;downloads&quot;, &quot;CS_tidy.csv&quot;)) write_csv(CS, file.path(&quot;downloads&quot;, &quot;CS.csv&quot;)) write_csv(attr(CS, &quot;labels&quot;), file.path(&quot;downloads&quot;, &quot;CS_labels.csv&quot;)) # Save raw data write_csv(CS_raw, file.path(&quot;downloads&quot;, &quot;CS_raw_data.csv&quot;)) writeLines(readLines(data_label_file), file.path(&quot;downloads&quot;, &quot;CS_raw_labels.txt&quot;)) That wraps up the process of importing and cleaning the Michigan Consumer Survey data. You can now proceed to analyze the data. To load the data in a future session, you can use the readRDS() function. # Load data # data &lt;- readRDS(&quot;downloads/CS.rds&quot;) 21.6 Exploring Individual-Level Data Let’s visualize some of the individual-level data: #================================================================================================ # Individual-Level Analysis #================================================================================================ # Load packages library(&quot;tidyverse&quot;) library(&quot;ggplot2&quot;) library(&quot;quantmod&quot;) library(&quot;lubridate&quot;) # Colorblind-friendly Figure Style cb_col &lt;- c(&quot;#000000&quot;,&quot;#004949&quot;,&quot;#009292&quot;,&quot;#ff6db6&quot;,&quot;#ffb6db&quot;, &quot;#490092&quot;,&quot;#006ddb&quot;,&quot;#b66dff&quot;,&quot;#6db6ff&quot;,&quot;#b6dbff&quot;, &quot;#920000&quot;,&quot;#924900&quot;,&quot;#db6d00&quot;,&quot;#24ff24&quot;,&quot;#ffff6d&quot;) fig_style &lt;- list() fig_style$color &lt;- cb_col[c(1, 4, 3, 9, 6, 12, 2, 11, 13, 5, 7, 8, 10, 14, rep(1, 100))] fig_style$alpha &lt;- c(1, .8, 1, 1, 1, rep(.5, 100)) fig_style$linetype &lt;- rep(1, 100) fig_style$size &lt;- c(.7, .9, .7, 1.3, .4, 1, .4, 1.3, .7, 1, rep(.7, 100)) # Load data CS &lt;- readRDS(&quot;downloads/CS.rds&quot;) # Input cs_select &lt;- c(&quot;PAGO&quot;, &quot;PEXP&quot;, &quot;BUS12&quot;, &quot;BUS5&quot;, &quot;DUR&quot;) # Extract relevant data cs_CS &lt;- CS %&gt;% filter(variable %in% c(&quot;R&quot;, &quot;DATE&quot;, cs_select)) # Normalize answers across variables cs_CS &lt;- cs_CS %&gt;% mutate(category = factor(value_cat, levels = c(5, 4, 3, 2, 1, 8, 9, 98, 99), labels = c(&quot;Bad&quot;, &quot;Bad&quot;, &quot;Same&quot;, &quot;Good&quot;, &quot;Good&quot;, &quot;Don&#39;t Know&quot;, &quot;Missing&quot;, &quot;Don&#39;t Know&quot;, &quot;Missing&quot;), ordered = TRUE)) # Select specific dates date_now &lt;- max(cs_CS$DATE) recent_dates &lt;- add_with_rollback(date_now, - c(months(0), months(6), months(12))) # Aggregation cs_freq &lt;- cs_CS %&gt;% group_by(variable, category) %&gt;% summarize(n = sum(WT), n_now = sum(WT[DATE == date_now]), n_prev = sum(WT[format(DATE, &quot;%Y&quot;) == format(date_now, &quot;%Y&quot;)]), n_prev2 = sum(WT[format(DATE, &quot;%Y&quot;) %in% as.character(as.numeric(format(date_now, &quot;%Y&quot;)) - (0:2))])) %&gt;% ungroup(category) %&gt;% summarize(category = category, freq = 100 * n / sum(n), freq_now = 100 * n_now / sum(n_now), freq_prev = 100 * n_prev / sum(n_prev), freq_prev2 = 100 * n_prev2 / sum(n_prev2)) %&gt;% ungroup() %&gt;% pivot_longer(cols = -c(&quot;variable&quot;, &quot;category&quot;), names_to = &quot;date&quot;, values_to = &quot;freq&quot;) # Labeling cs_freq &lt;- cs_freq %&gt;% mutate(date = factor(date, levels = rev(c(&quot;freq&quot;, &quot;freq_prev2&quot;, &quot;freq_prev&quot;, &quot;freq_now&quot;)), labels = rev(c(paste0(&quot;1978&quot;, &quot; - &quot;, format(date_now, &quot;%Y&quot;)), paste0(as.numeric(format(date_now, &quot;%Y&quot;)) - 2, &quot; - &quot;, format(date_now, &quot;%Y&quot;)), paste0(&quot;Jan - &quot;, format(date_now, &quot;%b &quot;), format(date_now, &quot;%Y&quot;)), format(date_now, &quot;%b %Y&quot;))), ordered = TRUE)) # Make plot cross_plot &lt;- cs_freq %&gt;% ggplot(aes(x = freq, y = date, fill = category))+ geom_col(position = position_stack(reverse = TRUE))+ scale_x_continuous(expand = c(0, 0))+ scale_fill_manual(name = NULL, values = fig_style$color)+ facet_grid(rows = vars(variable), switch = &quot;both&quot;)+ labs(x = &quot;Share of Responses in %&quot;, y = &quot;&quot;)+ theme_classic()+ theme(axis.title.x = element_text(size = 9), legend.key.size = unit(.5, &quot;cm&quot;)) # Show plot cross_plot # Save plot dir.create(file.path(&quot;downloads&quot;, &quot;figures&quot;), showWarnings = FALSE) ggsave(filename = file.path(&quot;downloads&quot;, &quot;figures&quot;, &quot;cs_crossplot.png&quot;), plot = cross_plot, width = 5, height = 4, dpi = 600) 21.7 Replicate ICS Let’s use the individual-level data to reproduce the Index of Consumer Sentiment (ICS). You can tailor this process to create any index based on any set of questions. But first, let’s define the recession shade function, as discussed in Chapter 12.6.2: #-------------------------------- # Recession shades #-------------------------------- # Load NBER based recession indicators for the United States quantmod::getSymbols(&quot;USRECM&quot;, src = &quot;FRED&quot;) # Create a data frame with dates referring to start and end of recessions REC &lt;- data.frame(index = zoo::index(USRECM), USRECM = zoo::coredata(USRECM)) REC &lt;- rbind(list(REC[1, &quot;index&quot;], 0), REC, list(REC[nrow(REC), &quot;index&quot;], 0)) REC$dUSRECM &lt;- REC$USRECM - c(NA, REC$USRECM[-nrow(REC)]) REC &lt;- data.frame(rec_start = REC$index[REC$dUSRECM == 1 &amp; !is.na(REC$dUSRECM)], rec_end = REC$index[REC$dUSRECM == -1 &amp; !is.na(REC$dUSRECM)]) # Add a ggplot() layer that draws rectangles for those recession periods geom_recession_shades &lt;- function(xlim = c(min(REC$rec_start), max(REC$rec_end))){ geom_rect(data = REC[REC$rec_start &gt;= xlim[1] &amp; REC$rec_end &lt;= xlim[2], ], inherit.aes = FALSE, aes(xmin = rec_start, xmax = rec_end, ymin = -Inf, ymax = +Inf), fill = &quot;black&quot;, alpha = .15) } Now construct the ICS: #================================================================================================ # Construct the Index of Consumer Sentiment #================================================================================================ # Input ics_select &lt;- c(&quot;PAGO&quot;, &quot;PEXP&quot;, &quot;BUS12&quot;, &quot;BUS5&quot;, &quot;DUR&quot;) ics_variables &lt;- attr(CS, &quot;labels&quot;) %&gt;% select(variable, label_var, question) %&gt;% distinct() %&gt;% filter(variable %in% cs_select) %&gt;% mutate(variable = factor(x = variable, levels = cs_select, ordered = TRUE)) %&gt;% arrange(variable) # Standardize variables ics_data &lt;- CS %&gt;% filter(variable %in% ics_variables$variable) %&gt;% mutate(category = factor(value_cat, levels = c(5, 4, 3, 2, 1, 8, 9, 98, 99), labels = c(&quot;Bad&quot;, &quot;Bad&quot;, &quot;Same&quot;, &quot;Good&quot;, &quot;Good&quot;, &quot;Don&#39;t Know&quot;, &quot;Missing&quot;, &quot;Don&#39;t Know&quot;, &quot;Missing&quot;), ordered = TRUE)) # Aggregate ics_data &lt;- ics_data %&gt;% count(DATE, variable, category, wt = WT) %&gt;% group_by(DATE, variable) %&gt;% mutate(freq = 100 * n / sum(n)) # Variable labels ics_data &lt;- ics_data %&gt;% mutate(label_var = factor(x = variable, levels = ics_variables$variable, labels = ics_variables$label_var, ordered = TRUE)) # Compute scores ics_data &lt;- ics_data %&gt;% group_by(DATE, variable, label_var) %&gt;% summarize(freq = 100 + freq[category == &quot;Good&quot;] - freq[category == &quot;Bad&quot;], category = &quot;Score&quot;) %&gt;% ungroup() %&gt;% bind_rows(ics_data) %&gt;% mutate(category = factor(category, levels = c(levels(ics_data$category), &quot;Score&quot;), ordered = TRUE)) %&gt;% arrange(DATE, variable, category) # Compute consumer indexes ics_indexes &lt;- ics_data %&gt;% filter(category == &quot;Score&quot;) %&gt;% mutate(intercept = ifelse(DATE &lt;= as.Date(&quot;1981-11-01&quot;), 2.7, 2)) %&gt;% group_by(DATE) %&gt;% summarize(ICC = first(intercept) + sum(freq * (variable %in% c(&quot;PAGO&quot;, &quot;DUR&quot;)) / 2.6424), ICE = first(intercept) + sum(freq * (variable %in% c(&quot;PEXP&quot;, &quot;BUS12&quot;, &quot;BUS5&quot;)) / 4.1134), ICS = first(intercept) + sum(freq) / 6.7558) # Check if ICS is correct UMCSENT &lt;- quantmod::getSymbols(&quot;UMCSENT&quot;, src = &quot;FRED&quot;, auto.assign = FALSE) plot_FRED &lt;- tibble(DATE = index(UMCSENT), ICS_official = as.numeric(UMCSENT)) %&gt;% filter(DATE &gt;= min(ics_indexes$DATE)) %&gt;% left_join(ics_indexes, by = &quot;DATE&quot;, suffix = c(&quot;_official&quot;, &quot;_self&quot;)) %&gt;% mutate(diff = ICS_official - ICS)%&gt;% ggplot(aes(x = DATE, y = diff))+ geom_line() plot_FRED # Save plot dir.create(file.path(&quot;downloads&quot;, &quot;figures&quot;), showWarnings = FALSE) ggsave(filename = file.path(&quot;downloads&quot;, &quot;figures&quot;, &quot;FRED_comparison.png&quot;), plot = plot_FRED, width = 12, height = 6, dpi = 600) # Plot ts_plot &lt;- ics_data %&gt;% filter(category %in% c(&quot;Good&quot;, &quot;Bad&quot;, &quot;Score&quot;)) %&gt;% ggplot(aes(x = DATE, y = freq, col = paste(variable, label_var, sep = &quot;: &quot;)))+ geom_line()+ scale_color_manual(name = NULL, values = fig_style$color)+ facet_grid(rows = vars(category), scales = &quot;free_y&quot;, switch = &quot;both&quot;)+ geom_recession_shades(xlim = range(ics_data$DATE))+ labs(y = &quot; Share of Responses in %&quot;, subtitle = &quot;Score = 100 + Good - Bad&quot;)+ theme_classic()+ theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1), axis.title.x = element_blank(), axis.title.y = element_text(size = 9), plot.subtitle = element_text(size = 9), legend.key.size = unit(.5, &quot;cm&quot;), legend.position = &quot;bottom&quot;, legend.direction = &quot;horizontal&quot;, legend.justification = &quot;left&quot;, legend.margin = margin(t = 0, r = 0, b = -4, l = -12), legend.box.margin = margin(t = -8, r = 0, b = 0, l = 0))+ guides(alpha = guide_legend(ncol = 2), color = guide_legend(ncol = 2), fill = guide_legend(ncol = 2)) ts_plot # Save plot dir.create(file.path(&quot;downloads&quot;, &quot;figures&quot;), showWarnings = FALSE) ggsave(filename = file.path(&quot;downloads&quot;, &quot;figures&quot;, &quot;ics_good_bad.png&quot;), plot = ts_plot, width = 6, height = 5, dpi = 600) 21.8 Conclusion Throughout this chapter, we have explored the Michigan Consumer Survey. This data, collected monthly, offers profound insights into consumers’ attitudes, and can serve as a valuable tool for researchers, economists, investors, and policymakers. We delved into the three indices generated from this survey data - ICC, ICE, and ICS - and also discussed how to import these indices for further analysis. Beyond these existing indices, we discussed the potential for creating custom indices tailored to specific research interests or sectors of the economy. An in-depth example of creating a new index from the ground up was presented with a step-by-step guide to replicating the Index of Consumer Sentiment (ICS) from individual-level data. The approach provided can be adjusted to create any index based on any set of questions. "],["insights-from-real-time-data.html", "Chapter 22 Insights From Real-Time Data", " Chapter 22 Insights From Real-Time Data Content Coming Soon "],["insights-from-forecaster-data.html", "Chapter 23 Insights From Forecaster Data 23.1 Tealbook", " Chapter 23 Insights From Forecaster Data 23.1 Tealbook In this chapter, we’ll demonstrate how to import an Excel workbook (xls or xlsx) by importing projections data from Tealbooks (formerly known as Greenbooks). Tealbooks are an important resource as they consist of economic forecasts created by the research staff at the Federal Reserve Board of Governors. These forecasts are prepared before each meeting of the Federal Open Market Committee (FOMC), which is a key decision-making body responsible for determining U.S. monetary policy. The Tealbooks’ projections provide valuable insights into the expected performance of the economy, including indicators such as inflation, GDP growth, unemployment rates, and interest rates. The data from Tealbooks are made available to the public with a five-year lag, allowing researchers and analysts to examine past economic forecasts and compare them to actual outcomes. By importing and analyzing this data, we can gain a deeper understanding of the economic outlook as perceived by the Federal Reserve’s research staff and the FOMC prior to their policy meetings. To obtain the Tealbook forecasts, perform the following steps: Visit the website of the Federal Reserve Bank of Philadelphia by clicking here. Navigate to SURVEYS &amp; DATA, select Real-Time Data Research, choose Tealbook (formerly Greenbook) Data Sets, and finally select Philadelphia Fed’s Tealbook (formerly Greenbook) Data Set. On the data page, download Philadelphia Fed’s Tealbook/Greenbook Data Set: Row Format. Save the Excel workbook as ‘GBweb_Row_Format.xlsx’ in a familiar location for easy retrieval. The Excel workbook contains multiple sheets, each representing different variables. The sheet gRGDP, for example, holds real GDP growth forecasts. Columns gRGDPF1 to gRGDPF9 represent one- to nine-quarter-ahead forecasts, gRGDPF0 represents the nowcast, and gRGDPB1 to gRGDPB4 represent one- to four-quarter-behind backcasts. 23.1.1 Import Excel Workbook An Excel workbook (xlsx) is a file that contains one or more spreadsheets (worksheets), which are the separate “pages” or “tabs” within the file. An Excel worksheet, or simply a sheet, is a single spreadsheet within an Excel workbook. It consists of a grid of cells formed by intersecting rows (labeled by numbers) and columns (labeled by letters), which can hold data such as text, numbers, and formulas. To import the Tealbook, which is an Excel workbook, install and load the readxl package by running install.packages(\"readxl\") in the console and include the package at the beginning of your R script. You can then use the excel_sheets() function to print the names of all Excel worksheets in the workbook, and the read_excel() function to import a particular worksheet: # Load the package library(&quot;readxl&quot;) # Get names of all Excel worksheets sheet_names &lt;- excel_sheets(path = &quot;files/GBweb_Row_Format.xlsx&quot;) sheet_names ## [1] &quot;Documentation&quot; &quot;gRGDP&quot; &quot;gPGDP&quot; &quot;UNEMP&quot; ## [5] &quot;gPCPI&quot; &quot;gPCPIX&quot; &quot;gPPCE&quot; &quot;gPPCEX&quot; ## [9] &quot;gRPCE&quot; &quot;gRBF&quot; &quot;gRRES&quot; &quot;gRGOVF&quot; ## [13] &quot;gRGOVSL&quot; &quot;gNGDP&quot; &quot;HSTART&quot; &quot;gIP&quot; # Import one of the Excel worksheets: &quot;gRGDP&quot; gRGDP &lt;- read_excel(path = &quot;files/GBweb_Row_Format.xlsx&quot;, sheet = &quot;gRGDP&quot;) gRGDP ## # A tibble: 466 × 16 ## DATE gRGDPB4 gRGDPB3 gRGDPB2 gRGDPB1 gRGDPF0 gRGDPF1 gRGDPF2 gRGDPF3 gRGDPF4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1967. 5.9 1.9 4 4.5 0.5 1 NA NA NA ## 2 1967. 1.9 4 4.5 0 1.6 4.1 NA NA NA ## 3 1967. 1.9 4 4.5 -0.3 1.4 4.7 NA NA NA ## 4 1967. 1.9 4 4.5 -0.3 2.3 4.5 NA NA NA ## 5 1967. 3.4 3.8 -0.2 2.4 4.3 NA NA NA NA ## 6 1967. 3.4 3.8 -0.2 2.4 4.2 4.5 NA NA NA ## 7 1967. 3.4 3.8 -0.2 2.4 4.8 6.4 NA NA NA ## 8 1967. 3.4 3.8 -0.2 2.4 4.9 6.4 NA NA NA ## 9 1967. 3.8 -0.2 2.4 4.2 6.1 NA NA NA NA ## 10 1967. 3.8 -0.2 2.4 4.2 5.1 NA NA NA NA ## # ℹ 456 more rows ## # ℹ 6 more variables: gRGDPF5 &lt;dbl&gt;, gRGDPF6 &lt;dbl&gt;, gRGDPF7 &lt;dbl&gt;, ## # gRGDPF8 &lt;dbl&gt;, gRGDPF9 &lt;dbl&gt;, GBdate &lt;dbl&gt; To import all sheets, use the following code: # Import all worksheets of Excel workbook TB &lt;- lapply(sheet_names, function(x) read_excel(path = &quot;files/GBweb_Row_Format.xlsx&quot;, sheet = x)) names(TB) &lt;- sheet_names # Summarize the list containing all Excel worksheets summary(TB) ## Length Class Mode ## Documentation 2 tbl_df list ## gRGDP 16 tbl_df list ## gPGDP 16 tbl_df list ## UNEMP 16 tbl_df list ## gPCPI 16 tbl_df list ## gPCPIX 16 tbl_df list ## gPPCE 16 tbl_df list ## gPPCEX 16 tbl_df list ## gRPCE 16 tbl_df list ## gRBF 16 tbl_df list ## gRRES 16 tbl_df list ## gRGOVF 16 tbl_df list ## gRGOVSL 16 tbl_df list ## gNGDP 16 tbl_df list ## HSTART 16 tbl_df list ## gIP 16 tbl_df list In the provided code snippet, the lapply() function is used to apply the read_excel() function to each element in the sheet_names list, returning a list that is the same length as the input. Consequently, the TB object becomes a list in which each element is a tibble corresponding to an individual Excel worksheet. For instance, to access the worksheet that contains unemployment forecasts, you would use the following code: # Print one of the worksheets (unemployment forecasts): TB[[&quot;UNEMP&quot;]] ## # A tibble: 466 × 16 ## DATE UNEMPB4 UNEMPB3 UNEMPB2 UNEMPB1 UNEMPF0 UNEMPF1 UNEMPF2 UNEMPF3 UNEMPF4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1967. 3.8 3.8 3.8 3.7 3.8 4.1 NA NA NA ## 2 1967. 3.8 3.8 3.7 3.7 4 4.1 NA NA NA ## 3 1967. 3.8 3.8 3.7 3.7 3.9 4 NA NA NA ## 4 1967. 3.8 3.8 3.7 3.7 3.9 3.9 NA NA NA ## 5 1967. 3.8 3.7 3.7 3.8 3.9 NA NA NA NA ## 6 1967. 3.8 3.7 3.7 3.8 3.9 3.8 NA NA NA ## 7 1967. 3.8 3.7 3.7 3.8 3.8 3.6 NA NA NA ## 8 1967. 3.8 3.7 3.7 3.8 3.8 3.6 NA NA NA ## 9 1967. 3.7 3.7 3.8 3.9 3.8 NA NA NA NA ## 10 1967. 3.7 3.7 3.8 3.9 4 NA NA NA NA ## # ℹ 456 more rows ## # ℹ 6 more variables: UNEMPF5 &lt;dbl&gt;, UNEMPF6 &lt;dbl&gt;, UNEMPF7 &lt;dbl&gt;, ## # UNEMPF8 &lt;dbl&gt;, UNEMPF9 &lt;dbl&gt;, GBdate &lt;chr&gt; 23.1.2 Nowcast and Backcast Let’s explore the forecaster data, interpreting the information provided in the Tealbook Documentation file. This file is accessible on the same page as the data, albeit further down. The data set extends beyond simple forecasts, encompassing nowcasts and backcasts as well. The imported Excel workbook GBweb_Row_Format.xlsx includes multiple sheets, each one representing distinct variables. Take, for instance, the sheet gPGDP, which contains forecasts for inflation (GDP deflator). Columns gPGDPF1 through gPGDPF9 represent forecasts for one to nine quarters ahead respectively, while gPGDPF0 denotes the nowcast. The columns gPGDPB1 to gPGDPB4, on the other hand, symbolize backcasts for one to four quarters in the past. # Print column names of inflation back-, now-, and forecasts colnames(TB[[&quot;gPGDP&quot;]]) ## [1] &quot;DATE&quot; &quot;gPGDPB4&quot; &quot;gPGDPB3&quot; &quot;gPGDPB2&quot; &quot;gPGDPB1&quot; &quot;gPGDPF0&quot; &quot;gPGDPF1&quot; ## [8] &quot;gPGDPF2&quot; &quot;gPGDPF3&quot; &quot;gPGDPF4&quot; &quot;gPGDPF5&quot; &quot;gPGDPF6&quot; &quot;gPGDPF7&quot; &quot;gPGDPF8&quot; ## [15] &quot;gPGDPF9&quot; &quot;GBdate&quot; A nowcast (gPGDPF0) effectively serves as a present-time prediction. Given the delay in the release of economic data, nowcasting equips economists with the tools to make informed estimates about what current data will ultimately reveal once it is officially published. Conversely, a backcast (gPGDPB1 through gPGDPB4) is a forecast formulated for a time period that has already transpired. This might seem counterintuitive, but it’s necessary because the initial estimates of economic data are often revised significantly as more information becomes available. It is noteworthy that financial data typically doesn’t experience such revisions, since interest rates or asset prices are directly observed on exchanges, eliminating the need for revisions. In contrast, inflation is not directly observable and requires deduction from a wide array of information sources. These sources can encompass export data, industrial production statistics, retail sales data, employment figures, consumer spending metrics, and capital investment data, among others. In summary, while forecasting seeks to predict future economic scenarios, nowcasting aspires to deliver an accurate snapshot of the current economy, and backcasting strives to enhance our understanding of historical economic conditions. 23.1.3 Information Date and Realization Date The imported Excel workbook GBweb_Row_Format.xlsx is structured in a row format as opposed to a column format. As clarified in the Tealbook Documentation file, this arrangement means that (1) each row corresponds to a specific Tealbook publication date, and (2) the columns represent the forecasts generated for that particular Tealbook. The forecast horizons include, at most, the four quarters preceding the nowcast quarter, the nowcast quarter itself, and up to nine quarters into the future. Thus, all the values in a single row refer to the same forecast date, yet different forecast horizons. Let’s define information date as the specific date when a forecast is generated, and realization date as the exact time period the forecast is referring to. In the row format data structure, each row in the data set corresponds to a specific information date, with the columns indicating different forecast horizons, and hence, different realization dates. This allows a comparative view of how the forecast relates to both current and past data. For instance, if a GDP growth forecast exceeds the nowcast, it suggests an optimistic outlook on that particular information date. However, it is often useful to compare different information dates that all correspond to the same realization date. For example, the forecast error is calculated by comparing a forecast from an earlier information date with a backcast created at a later information date, while both the forecast and backcast relate to the same realization date. If the data is organized in column format, then each row pertains to a certain realization date, and the columns correspond to different information dates. Regardless of whether we import the data in row or column format, we need to be able to manipulate the data set to compare both different information and realization dates with each other. 23.1.4 Plotting Forecasts Let’s use the plot() function to visualize some of the real GDP growth forecasts. We will plot the Tealbook forecasts for three different information dates, where the x-axis represents the realization date and the y-axis represents the real GDP growth forecasts made at the information date: # Extract the real GDP growth worksheet gRGDP &lt;- TB[[&quot;gRGDP&quot;]] # Create a date column capturing the information date gRGDP$date_info &lt;- as.Date(as.character(gRGDP$GBdate), format = &quot;%Y%m%d&quot;) # Select information dates to plot dates &lt;- as.Date(c(&quot;2008-06-18&quot;, &quot;2008-12-10&quot;, &quot;2009-08-06&quot;)) # Plot forecasts of second information date plot(x = as.yearqtr(dates[2]) + seq(-4, 9) / 4, y = as.numeric(gRGDP[gRGDP$date_info == dates[2], 2:15]), type = &quot;l&quot;, col = 5, lwd = 5, lty = 1, ylim = c(-7, 5), xlim = as.yearqtr(dates[2]) + c(-6, 8) / 4, xlab = &quot;Realization Date&quot;, ylab = &quot;%&quot;, main = &quot;Back-, Now-, and Forecasts of Real GDP Growth&quot;) abline(v = as.yearqtr(dates[2]), col = 5, lwd = 5, lty = 1) # Plot forecasts of first information date lines(x = as.yearqtr(dates[1]) + seq(-4, 9) / 4, y = as.numeric(gRGDP[gRGDP$date_info == dates[1], 2:15]), type = &quot;l&quot;, col = 2, lwd = 2, lty = 2) abline(v = as.yearqtr(dates[1]), col = 2, lwd = 2, lty = 2) # Plot forecasts of third information date lines(x = as.yearqtr(dates[3]) + seq(-4, 9) / 4, y = as.numeric(gRGDP[gRGDP$date_info == dates[3], 2:15]), type = &quot;l&quot;, col = 1, lwd = 1, lty = 1) abline(v = as.yearqtr(dates[3]), col = 1, lwd = 1, lty = 1) # Add legend legend(x = &quot;bottomright&quot;, legend = format(dates, format = &quot;%b %d, %Y&quot;), title=&quot;Information Date&quot;, col = c(2, 5, 1), lty = c(2, 1, 1), lwd = c(2, 5, 1)) In the code snippet provided, the appearance and behavior of the plot are customized using several functions and arguments: x = as.yearqtr(dates[2]) + seq(-4, 9) / 4: This argument specifies the x-values for the line plot. as.yearqtr(dates[2]) converts the second date in the dates vector into a quarterly format. seq(-4, 9) / 4 generates a sequence of quarterly offsets, which are added to the second date to generate the x-values. y = as.numeric(gRGDP[gRGDP$date_info == dates[2], 2:15]): This argument specifies the y-values for the line plot. It selects the back-, now-, and forecasts (columns 2 through 15) from the gRGDP dataframe for rows where the date_info column equals the second date in the dates vector. type = \"l\": This argument specifies that the plot should be a line plot. col = 5: This argument sets the color of the line plot. 5 corresponds to magenta in R’s default color palette. lwd = 5: This argument specifies the line width. The higher the value, the thicker the line. lty = 1: This argument sets the line type. 1 corresponds to a solid line. ylim = c(-7, 5): This argument sets the y-axis limits. xlim = as.yearqtr(dates[2]) + c(-6, 8) / 4: This argument sets the x-axis limits. The limits are calculated in a similar way to the x-values, but with different offsets. xlab = \"Realization Date\" and ylab = \"%\": These arguments label the x-axis and y-axis, respectively. main = \"Back-, Now-, and Forecasts of Real GDP Growth\": This argument sets the main title of the plot. abline(v = as.yearqtr(dates[2]), col = 5, lwd = 5, lty = 1): This function adds a vertical line to the plot at the second date in the dates vector. The color, line width, and line type of the vertical line are specified by the col, lwd, and lty arguments, respectively. lines: This function is used to add additional lines to the plot, corresponding the the first and third date in the dates vector. legend: This function adds a legend to the plot. The x argument specifies the position of the legend, the legend argument specifies the labels, and the title argument specifies the title of the legend. The color, line type, and line width of each label are specified by the col, lty, and lwd arguments, respectively. Figure 23.1: Back-, Now-, and Forecasts of Real GDP Growth The resulting plot, as shown in Figure 23.1, exhibits backcasts, nowcasts, and forecasts derived at three distinct time periods: The dashed red line corresponds to the backcasts, nowcasts, and forecasts made on June 18, 2008. The thick magenta line represents the backcasts, nowcasts, and forecasts made on December 10, 2008. The solid black line signifies the backcasts, nowcasts, and forecasts made on August 06, 2009. The vertical lines depict the three information dates. This figure indicates that considerable revisions occurred between June 18 and December 10, 2008, implying that the Federal Reserve did not anticipate the Great Recession. While the revisions between December 10, 2008 and August 06, 2009 are less drastic, it’s noteworthy that there are considerable revisions in the December 10, 2008 nowcasts and backcasts. Consequently, the Federal Reserve had to operate based on incorrect information about growth in the current and preceding periods. 23.1.5 Plotting Forecast Errors The forecast error for a given period is computed by subtracting the forecasted value from the actual value. This error is determined using an \\(h\\)-step ahead forecast, represented as follows: \\[ e_{t,h} = y_t - f_{t,h} \\] In this equation, \\(e_{t,h}\\) stands for the forecast error, \\(y_t\\) is the actual value, \\(f_{t,h}\\) is the forecasted value, and \\(t\\) designates the realization date. The information date of the \\(h\\)-step ahead forecast \\(f_{t,h}\\) is \\(t-h\\), and for \\(y_t\\) it is either \\(t\\) or \\(t+k\\) with \\(k&gt;0\\) depending on whether the value is observed like asset prices or revised post realization like inflation or GDP growth. It’s essential to note that the forecast error compares the same realization date with the values of two different information dates. For simplification, we assume that \\(y_t=f_{t,-1}\\); that is, the 1-step behind backcast is a good approximation of the actual value. When utilizing the Tealbook inflation data in row format, one might infer that the forecast error is computed by subtracting gPGDPB1 from gPGDPF1 for each row. However, this would be incorrect as it results in \\(y_{t-1} - f_{t+1,h}\\), where the information date for both variables is \\(t\\), but the realization dates are \\(t-1\\) and \\(t+1\\) respectively, and thus do not match. Therefore, we need to shift the backcast gPGDPB1 one period forward, and the one-step ahead forecast gPGDPF1 one period backward to compute the one-step ahead forecast error. However, as the realization periods are quarterly, and there are eight FOMC (Federal Open Market Committee) meetings per year, each necessitating Tealbook forecasts, we need to aggregate the information frequency to quarterly as well. The R code for performing these operations and computing the forecast error is as follows: # Extract the GDP deflator (inflation) worksheet gPGDP &lt;- TB[[&quot;gPGDP&quot;]] # Create a date column capturing the information date gPGDP$date_info &lt;- as.Date(as.character(gPGDP$GBdate), format = &quot;%Y%m%d&quot;) # Aggregate information frequency to quarterly gPGDP$date_info_qtr &lt;- as.yearqtr(gPGDP$date_info) gPGDP_qtr &lt;- aggregate( x = gPGDP, by = list(date = gPGDP$date_info_qtr), FUN = last ) # Lag the one-step ahead forecasts gPGDP_qtr$gPGDPF1_lag1 &lt;- c( NA, gPGDP_qtr$gPGDPF1[-nrow(gPGDP_qtr)] ) # Lead the one-step behind backcasts gPGDP_qtr$gPGDPB1_lead1 &lt;- c(gPGDP_qtr$gPGDPB1[-1], NA) # Compute forecast error gPGDP_qtr$error_F1_B1 &lt;- gPGDP_qtr$gPGDPB1_lead1 - gPGDP_qtr$gPGDPF1_lag1 To visualize the inflation forecast errors, we use the plot() function as demonstrated below: # Plot inflation forecasts for each realization date plot(x = gPGDP_qtr$date, y = gPGDP_qtr$gPGDPF1_lag1, type = &quot;l&quot;, col = 5, lty = 1, lwd = 3, ylim = c(-3, 15), xlab = &quot;Realization Date&quot;, ylab = &quot;%&quot;, main = &quot;Forecast Error of Inflation&quot;) abline(h = 0, lty = 3) # Plot inflation backcasts for each realization date lines(x = gPGDP_qtr$date, y = gPGDP_qtr$gPGDPB1_lead1, type = &quot;l&quot;, col = 1, lty = 1, lwd = 1) # Plot forecast error for each realization date lines(x = gPGDP_qtr$date, y = gPGDP_qtr$error_F1_B1, type = &quot;l&quot;, col = 2, lty = 1, lwd = 1.5) # Add legend legend(x = &quot;topright&quot;, legend = c(&quot;Forecasted Value&quot;, &quot;Actual Value (Backcast)&quot;, &quot;Forecast Error&quot;), col = c(5, 1, 2), lty = c(1, 1, 1), lwd = c(3, 1, 1.5)) The customization of the plot is achieved using a variety of parameters, as discussed earlier. Figure 23.2: Forecast Error of Inflation The resulting plot, shown in Figure 23.2, showcases the one-quarter ahead forecast error of inflation (GDP deflator) since 1967. A notable observation is the decrease in the forecast error variance throughout the “Great Moderation” period, which spanned from the mid-1980s to 2007. The term “Great Moderation” was coined by economists to describe the substantial reduction in the volatility of business cycle fluctuations, which resulted in a more stable economic environment. Multiple factors contributed to this trend. Technological advancements, structural changes, improved inventory management, financial innovation, and, crucially, improved monetary policy, are all credited for this period of economic stability. Monetary policies became more proactive and preemptive during this period, largely due to a better understanding of the economy and advances in economic modeling. These models, equipped with improved data, allowed for more accurate forecasts and enhanced decision-making capabilities. Consequently, policy actions were often executed before economic disruptions could materialize, thus moderating the volatility of the business cycle. It is worth noting that the forecast error tends to hover around zero. If the forecast error were consistently smaller than zero, one could simply lower the forecasted value until the forecast error reaches zero, on average, consequently improving the accuracy of the forecasts. Thus, in circumstances where the objective is to minimize forecast error, it is reasonable to expect that the forecast error will average out to zero over time. The following calculations are in line with the description provided above: # Compute mean of forecast error mean(gPGDP_qtr$error_F1_B1, na.rm = TRUE) ## [1] 0.07871287 # Compute volatility of forecast error (FE) sd(gPGDP_qtr$error_F1_B1, na.rm = TRUE) ## [1] 1.237391 # Compute volatility of FE before Great Moderation sd(gPGDP_qtr$error_F1_B1[gPGDP_qtr$date &lt;= as.yearqtr(&quot;1985-01-01&quot;)], na.rm = TRUE) ## [1] 1.728324 # Compute volatility of FE at &amp; after Great Moderation sd(gPGDP_qtr$error_F1_B1[gPGDP_qtr$date &gt; as.yearqtr(&quot;1985-01-01&quot;)], na.rm = TRUE) ## [1] 0.8499342 "],["part-vi.html", "Part VI: Insights From Textual Data", " Part VI: Insights From Textual Data Part VI delves into understanding public sentiments and economic trends through diverse textual sources. Google search trends serve as a barometer for public sentiment by analyzing frequently searched words or questions over time and across regions. Newspapers, reflective of societal focuses, offer economic signals, and this section introduces methodologies to transform articles into quantifiable data using text mining tools, ranging from simple techniques like the bag-of-words to more intricate ones detecting tonal nuances. Lastly, social media platforms, including Twitter, Facebook, and LinkedIn, are leveraged to sift through vast digital interactions, enabling the extraction of pertinent economic sentiments and trends from online conversations. Included chapters: Chapter 24: “Text Analysis and Mining” introduces the fundamental concepts of text analysis and mining, discussing techniques for cleaning, transforming, and extracting insights from textual data. Chapter 25: “Insights From Google Trends” explores economic trends and public sentiment by analyzing frequently searched terms on Google, providing a novel perspective on collective thoughts and emotions. Chapter 26: “Insights From Newspapers” focuses on analyzing written content from newspapers, discussing techniques specific to this medium and the economic insights one might derive. Chapter 27: “Insights From Social Media” focuses on extracting economic insights from social media platforms such as Twitter, Facebook, and LinkedIn, aiming to filter relevant information from digital chatter to understand prevailing economic sentiments and trends. "],["text-analysis-and-mining.html", "Chapter 24 Text Analysis and Mining", " Chapter 24 Text Analysis and Mining This chapter introduces the tools, techniques, and methodologies for extracting meaningful information from textual data. "],["insights-from-google-trends.html", "Chapter 25 Insights From Google Trends", " Chapter 25 Insights From Google Trends This chapter delves into how search query data from Google Trends can offer economic insights. "],["insights-from-newspapers.html", "Chapter 26 Insights From Newspapers", " Chapter 26 Insights From Newspapers This chapter focuses on analyzing written content from newspapers, discussing techniques specific to this medium and the economic insights one might derive. "],["insights-from-social-media.html", "Chapter 27 Insights From Social Media", " Chapter 27 Insights From Social Media This chapter explores the unique challenges and opportunities presented by social media text data, from analyzing sentiment to tracking real-time economic events. "],["modules.html", "Modules Overview", " Modules Overview A module, in the context of this book, is a self-contained unit of study that focuses on specific concepts, tools, or applications in economic data analysis. These modules serve as building blocks, each progressively introducing you to more complex or varied facets of the subject matter. In this section, we’ll outline a series of modules tailored to provide both foundational knowledge and advanced insights, ensuring a comprehensive understanding of economic data analysis. The book is divided into six main parts, which are further broken down into the following modules: Part I: Introduction to R Module A: Introduction to R Part II: Measurement in Economics Module B: Financial Performance Indicators Module C: Macro Indicators Module D: Financial Market Indicators Part III: Economic Data Processing Module E: Data Transformation Module F: Statistical Measures Module G: Data Aggregation Part IV: Patterns in Economics Module H: Temporal Patterns in Economics Module I: Regional Patterns in Economics Module J: Microeconomic Patterns Part V: Insights From Real-Time Data and Surveys Module K: Insights From Consumer Surveys Module L: Insights From Real-Time Data Module M: Insights From Forecaster Data Part VI: Insights From Textual Data Module N: Insights From Google Trends Module O: Insights From Newspapers Module P: Insights From Social Media Within each module, you’ll find (a) reading assignments from the Economic Data Analysis textbook, (b) associated courses from DataCamp, and (c) detailed instructions on drafting a data report that addresses the module’s subject matter. The following sections detail these three module components: Textbook Engagement Learn R with DataCamp Data Report Instructions "],["textbook.html", "Textbook Engagement", " Textbook Engagement Engaging with your textbook actively enhances comprehension and application. For effective engagement, read the designated chapters and focus on reproducing the content provided. Specifically: Reproduce content: As you progress, input the code from the chapters into RStudio. Verify outputs: Ensure that your results align with the chapter outputs. This not only confirms your understanding but also ensures practical applicability. Play with inputs: Experiment by applying the code to alternative datasets, tweak variables or conditions, and observe how the output changes. For assistance with the online textbook format, such as adjusting font size, navigating between chapters, copy-pasting R code, or utilizing the left sidebar, refer to the Book Navigation section. "],["datacamp.html", "Learn R with DataCamp Prepare for the Course XP System and Course Completion Develop Your Skills", " Learn R with DataCamp DataCamp is an online learning platform that offers programming courses in R, among other languages. All university students can avail themselves of a complimentary 6-month license from DataCamp. If you are enrolled in one of my courses, you should have received this license by email, but if you haven’t, please reach out to me. To start learning, first create an account on DataCamp using the student license you received. Once set up, I recommend beginning with the Introduction to R for Finance and Intermediate R for Finance courses. Prepare for the Course Before beginning a course on DataCamp, consider downloading and printing out the course slides. This will allow you to add your own notes as you work through the material. You can access the slides from the course dashboard by scrolling down to any of the chapters and clicking on Continue Chapter or Completed (if you’ve already finished that chapter). Then, locate the Show Slides icon in the top-right corner and click on the download symbol. As you progress through the course, try to apply what you’re learning to a financial or economic dataset that interests you using the RStudio software on your desktop. This immediate application of newly acquired knowledge helps solidify understanding and allows you to save useful functions in your R script for future reference. This method is particularly effective for bridging the gap between course material and your area of interest, especially in more advanced courses. While the Introduction to R for Finance and Intermediate R for Finance courses mainly focus on introducing basic functions and don’t necessarily involve specific datasets, this approach is highly beneficial for most other courses. XP System and Course Completion In DataCamp, each course is equipped with an experience point (XP) system. XP is a measure of your progress and engagement within a course. You earn XP by successfully completing exercises, practice sessions, and assessments within a course. The more XP you gain, the more it showcases your dedication and understanding of the subject matter. Upon successfully completing a course, you receive a certificate from DataCamp, which is a testament to your accomplishment and newly acquired skills. These certificates can be shared on professional networking sites like LinkedIn or included in your CV, showcasing your dedication to learning and skill development. Courses are divided into chapters that each focus on a specific topic. In turn, each chapter consists of numerous exercises. You complete a course by working through each exercise and solving them successfully. When Completed appears beneath every chapter of a course, it signifies that you have finished the course. If you find an exercise challenging, you can opt for Take Hint to get some help or Show Answer to see the correct solution. However, it’s most beneficial to grasp the concepts and find solutions independently. Importantly, DataCamp allows unlimited course retakes. If you wish to reset a course, go to the course dashboard, select Replay Course (or Continue Course), then Course Outline, and finally Reset Course Progress. This way, if you’re unsatisfied with your XP score in a course, you can simply redo it. The aim is to thoroughly understand the course content and develop the associated skills, not to rush through it. Learn at your own pace and remember to enjoy the learning journey! Develop Your Skills After completing the initial courses, I encourage you to delve into more courses and skill tracks to uncover the wide array of possibilities that R (and Python) offer. One effective method of exploring new courses is by following career or skill tracks. These are curated collections of courses designed to provide comprehensive knowledge in a specific field. You can find these tracks by navigating to app.datacamp.com/learn, clicking on Tracks in the left navigation pane, and then selecting either Career Tracks or Skill Tracks. You can then choose R for courses based on the R software, or select other programs such as Python. For those studying Economics and Business, the following tracks are particularly pertinent: The complete skill track Finance Fundamentals in R The complete skill track Applied Finance in R The complete skill track Importing &amp; Cleaning Data with R The complete skill track Data Visualization with R The complete skill track Interactive Data Visualization in R The complete career track Quantitative Analyst with R For honing report writing skills, Reporting with R Markdown is recommended, and for those keen on Excel, the Spreadsheet Fundamentals skill track is worth considering. "],["datareport.html", "Data Report Instructions Versions of the Data Report Standards for the Two PDFs Collaboration Guidelines Example of a Data Report Final Remarks", " Data Report Instructions Each module culminates in the creation of a data report using R Markdown. This report will act as a hands-on demonstration of your understanding of the corresponding module’s material, showcasing your ability to process, visualize, and interpret economic data. The data report must be at least four pages in length and should include both data visualizations and interpretations. The specific content requirements will vary by module, with details provided in each respective module section. For instance, the data report on stock returns of Module A calls for the visualization and interpretation of stock return data. Versions of the Data Report You’ll need to complete two versions of the data report: An official version for your customer or supervisor, without R code. For an example, see www.julianfludwig.com/teaching/datareport/example_official.pdf. An internal version for your team, with R code. For an example, see www.julianfludwig.com/teaching/datareport/example_internal.pdf. Both versions must be PDF files and are crafted using R Markdown. If you are new to R Markdown, refer to Chapter 6. Familiarizing yourself with this chapter before proceeding will make the following instructions more accessible. Unlike a static report in Microsoft Word, where graphs are embedded as images, an R Markdown report is dynamic. Thus, running the R Markdown file updates the graphs within the resulting PDF, making it a living document that can change as the data changes. Hence, the R Markdown file, named something like data_report.Rmd, will include both text and R code. When run, this will produce an output file like data_report.pdf. The visibility of R code within the output file depends on the global settings in the R Markdown file. For instance, knitr::opts_chunk$set(echo=FALSE) hides the R code chunks, while knitr::opts_chunk$set(echo=TRUE) includes them. Additional settings like message=FALSE and warning=FALSE prevent R messages from displaying in the output file (see Chapter 6.4). Standards for the Two PDFs The two PDFs produced by your R Markdown file must adhere to the following standards: Both versions must be PDF files, directly generated from RStudio, not converted from another format like a Word file. The official version must conceal the dynamic nature of the report, meaning that R messages or code should not be visible. Consider the official version as what you’d present to clients - a polished data report without the technical details of the R code behind the visuals. Carefully consider the following for the official version: To suppress R messages (e.g., Loading required package: xts), use message = FALSE and warning = FALSE. See Chapter 6.4. To hide R code (e.g., PV &lt;- FV / (1 + YTM)^4), use echo = FALSE. More details are in Chapter 6.4. Avoid hash tags in your document (e.g., \\(\\#\\# \\ 123.45\\)), which appear when printing numbers or tables directly from an R chunk. Instead, weave numbers into the Markdown text, as described in Chapter 6.5, and use the kable function for tables, as explained in Chapter 6.7. Label every number, plot, and table accurately, and provide descriptive text. Ensure that all sentences are grammatically correct and complete, without missing verbs or unfinished thoughts. Adhering to these guidelines will help guarantee that your report is professional, understandable, and aesthetically pleasing, resulting in a comprehensive presentation of the data. Collaboration Guidelines When writing the data report, working with study groups is allowed, but please follow these rules: Each student should independently select their own topic for the data report. Every student is responsible for writing their own R code and producing their individual reports. If you collaborate with other students on the report, acknowledge their contributions. By adhering to these guidelines, you’ll have the chance to showcase your individual skills and understanding of the course content while still benefiting from the collaborative environment. Example of a Data Report I recommend downloading the following example for a data report on stock returns of Module A: www.julianfludwig.com/teaching/datareport/example_report.Rmd This R Markdown file produces the two PDF files referenced above for the official and internal versions. You can use this example report as a template and modify it to create your own report. To use this file, download it, open it in RStudio, and then click Knit: (or use the shortcut Ctrl + Shift + K or Cmd + Shift + K). If it doesn’t produce a PDF file, you may need to install the necessary LaTeX software as described in Chapter 2. The example report uses standard R programming syntax introduced in Chapter 3, xts syntax introduced in Chapter 4.8, quantmod syntax, and R Markdown syntax introduced in Chapter 6. Chapter 5.2 provides a good example of xts and quantmod syntax in action. If some of the code doesn’t make sense, you can access help and documentation as described in Chapter 3.5.4. For example, you can retrieve information about a function using the ? prefix in R (e.g., ?xts::apply.monthly), read the vignettes of the used packages (e.g., vignette(\"xts\")), or use Google as a source of information. Final Remarks Given the requirement that the data report must be produced in R Markdown, it’s important to try running the example Rmd file ahead of time. This ensures that all necessary software is installed and functioning correctly. Without the proper setup, you won’t be able to complete the report, even if you have produced the content of the report. Preparing in advance will help you avoid technical issues and allow you to focus on demonstrating your data analysis skills. Additionally, it’s crucial to adhere to the data report standards. A data report that includes programming code or contains incomplete sentences will not be acceptable to customers, regardless of the quality of the analysis. "],["module-a.html", "A Introduction to R A.1 Overview A.2 Learning Objectives A.3 Learning Activities &amp; Assessments A.4 Data Report on Stock Returns", " A Introduction to R The first module provides an overview of freely accessible software widely used for data analysis in economic research. The central focus is R, a programming language specifically developed for statistical computing and graphics. Complementary software - RStudio, R Markdown, and LaTeX - are introduced as supportive tools for dynamic document creation based on R. This module will guide you through the installation process and will familiarize you with the utilization of these tools by exploring key syntax. Specifically, the module explores the syntax necessary to import, process, and visualize data in R, and then how to incorporate the resulting graphs into a professional data report ready to be sent to customers. A.1 Overview This module consists of the following chapters: Chapter 1: “Software Overview” provides an overview of software commonly used for data analysis in economic research. Chapter 2: “Software Installation” provides a step-by-step guide to installing these software tools. Chapter 3: “R Basics” walks you through RStudio’s interface, introduces fundamental R operations, showcases efficient coding practices, and highlights pivotal R packages. Chapter 4: “Data Structures in R” discusses the fundamental data types and structures in R. Chapter 5: “Process Data in R” introduces the key functions to import, process, manipulate, and visualize data in R. Chapter 6: “Write Reports with R Markdown” covers the use of R Markdown for creating dynamic, reproducible data reports. Moreover, the following DataCamp courses supplement the chapters mentioned above: Introduction to R for Finance: An introductory R course on the essentials of the language. Intermediate R for Finance: A more advanced R course focusing on data analysis tools like date handling, conditional statements, loops, functions, and apply functions, all using finance-related examples. The module culminates in the creation of a data report: Data Report on Stock Returns: This evaluates your skills in importing, processing, and visualizing data in R. It also tests your ability to craft dynamic reports using R Markdown and to convey insights on stock market indicators. A.2 Learning Objectives By the end of this module, you should be able to: Identify the role and importance of software tools such as R, RStudio, R Markdown, and LaTeX in conducting economic research. Install and set up necessary software including R, RStudio, R Markdown, and LaTeX for economic data analysis. Navigate the RStudio interface, understanding its various panels and functionalities. Understand and apply basic R data types and structures for data storage and manipulation. Import external datasets into R for analysis, and download data directly within the R environment. Create professional data reports using R Markdown, incorporating R code, text, plots, and tables seamlessly. Utilize online learning resources like DataCamp to enhance R skills and knowledge further. Apply the knowledge and skills acquired to write a data report, demonstrating proficiency in data import, manipulation, and presentation, as well as interpretation of economic data. A.3 Learning Activities &amp; Assessments Throughout this module, you’ll engage in the following activities: Textbook Engagement: Read the textbook chapters 1 to 6 and reproduce the content provided. Input the code from the chapters into RStudio and verify that your results match the chapter outputs. DataCamp Training: Work through the Introduction to R for Finance and Intermediate R for Finance courses on DataCamp. While progressing through the courses, keep an R script handy and apply the learned functions to any of the datasets introduced in this module. This preserves the new functions of the course and potentially offers new insights from the chosen dataset. Data Report Creation: To consolidate your learning, craft a Data Report on Stock Markets using R Markdown. Your report should clearly display data, provide meaningful analyses, and embody the module’s content. Your assessment will be based on: DataCamp Course Completion: Finish the designated DataCamp courses for this module. Your grade is based on course completion; thus, you receive full credit by completing all chapters and obtaining a minimum of 75% XPs (experience points) by the deadline. Hence, while the Take Hint and Show Answer options reduce your XPs, they won’t affect your module grade if you stay above the 75% threshold. Data Report Evaluation: The quality of the data report is central to module assessment. It should be professional, suitable for clients, and reflect the module’s content. Ensure you adhere to the data report instructions. For further guidance on maximizing your textbook and DataCamp experience, consult the Textbook Engagement and Learn R with DataCamp sections. For specifications on the data report’s format and content, consult the Data Report Instructions and the subsequent section below. A.4 Data Report on Stock Returns You are tasked with preparing a data report on stock returns. Your report should analyze a company’s stock returns and compare them with those of related companies and stock market indices. Visualize historical stock market data, identify patterns, and examine significant historical events. As preparation for this assignment, please work through the module chapters and DataCamp courses listed under the overview section above. Ensure your report adheres to the guidelines detailed in Data Report Instructions, which dictates the creation of two versions: official and internal. Upon completion, submit both versions on Blackboard. "],["module-b.html", "B Financial Performance Indicators B.1 Overview B.2 Learning Objectives B.3 Learning Activities &amp; Assessments B.4 Data Report on Financial Performance Indicators", " B Financial Performance Indicators This module explores key measures used in Economics and Finance, outlining their respective data sources. It covers financial performance indicators of individual firms, necessitating a thorough understanding of business accounting and its key metrics. Building on the tools introduced in the first module, this section provides guidance on presenting and interpreting financial and economic data using R. B.1 Overview Outlined below are the chapters contained in this module: Chapter 7: “Economic Data Sources” explores various indicators that provide insights into the health and direction of economies and markets, and introduces key data providers. Chapter 8: “Business Accounting” discusses how accounting is a systematic method of recording, analyzing, and summarizing financial transactions. The outcome of accounting is a comprehensive snapshot of a company’s financial health, communicated through financial statements. Chapter 9: “Financial Performance Indicators” discusses key numbers that provide a comprehensive view of a company’s financial health. These indicators are based on the three major financial statements: the balance sheet, income statement, and cash flow statement. Additionally, the following DataCamp courses supplement the chapters mentioned above: Introduction to the Tidyverse: An introductory R course centered on the “Tidyverse”, which is a collection of R packages useful for data manipulation and visualization. Data Manipulation with dplyr: A course on using the dplyr package for data transformations and analysis. The module culminates in the creation of a data report: Data Report on Financial Performance Indicators: This tests your ability to analyze financial performance indicators, focusing on metrics derived from the balance sheet, income statement, and cash flow statement. You will evaluate a company’s financial health and compare its performance to industry benchmarks. B.2 Learning Objectives Upon completing this module, students will be able to: Comprehend the fundamental concepts of business accounting and the structure of financial statements. Understand and identify key financial performance indicators and accounting metrics. Locate and interpret data sources relevant to financial and accounting measures. Utilize R to effectively visualize financial performance and accounting data. Draw insights from these visualizations and understand their implications. B.3 Learning Activities &amp; Assessments Throughout this module, you’ll engage in the following activities: Textbook Engagement: Read the textbook chapters 7 to 9 and reproduce the content provided. Input the code from the chapters into RStudio and verify that your results match the chapter outputs. DataCamp Training: Work through the Introduction to the Tidyverse and Data Manipulation with dplyr courses on DataCamp. While progressing through the courses, keep an R script handy and apply the learned functions to any of the datasets introduced in this module. This preserves the new functions of the course and potentially offers new insights from the chosen dataset. Data Report Creation: To consolidate your learning, craft a Data Report on Financial Performance Indicators using R Markdown. Your report should clearly display data, provide meaningful analyses, and embody the module’s content. Your assessment will be based on: DataCamp Course Completion: Finish the designated DataCamp courses for this module. Your grade is based on course completion; thus, you receive full credit by completing all chapters and obtaining a minimum of 75% XPs (experience points) by the deadline. Hence, while the Take Hint and Show Answer options reduce your XPs, they won’t affect your module grade if you stay above the 75% threshold. Data Report Evaluation: The quality of the data report is central to module assessment. It should be professional, suitable for clients, and reflect the module’s content. Ensure you adhere to the data report instructions. For further guidance on maximizing your textbook and DataCamp experience, consult the Textbook Engagement and Learn R with DataCamp sections. For specifications on the data report’s format and content, consult the Data Report Instructions and the subsequent section below. B.4 Data Report on Financial Performance Indicators For this assignment, select a publicly traded company of your choice and analyze its financial performance indicators. Your analysis should include a comprehensive evaluation of the company’s balance sheet, income statement, and cash flow statement. Compare these indicators with those of key competitors and industry benchmarks. Additionally, visualize historical trends and identify significant events that have impacted the company’s financial performance. Your final report should provide a clear, data-driven assessment of the company’s financial health and offer well-supported recommendations. As preparation for this assignment, please work through the module chapters and DataCamp courses listed under the overview section above. Ensure your report adheres to the guidelines detailed in Data Report Instructions, which dictates the creation of two versions: official and internal. Upon completion, submit both versions on Blackboard. "],["module-c.html", "C Macroeconomic Accounting C.1 Overview C.2 Learning Objectives C.3 Learning Activities &amp; Assessments C.4 Data Report on Macroeconomic Indicators", " C Macroeconomic Accounting This module provides a comprehensive overview of macroeconomic indicators, including GDP, the unemployment rate, and inflation, with a particular emphasis on the national accounting framework that underlies these indicators. It explores how these indicators are measured, the data sources used, and their significance in assessing the health and stability of an economy. C.1 Overview Outlined below are the chapters contained in this module: Chapter 10: “Macroeconomic Accounting” explains how key economic indicators are systematically measured and recorded to assess the performance and stability of an economy. It covers essential metrics such as GDP, consumption, investment, international trade, government budgets, prices, money supply, and the balance of payments. Moreover, the following DataCamp course supplements the chapters mentioned above: Joining Data with dplyr: A guide to merging multiple tables using the dplyr package. The module culminates in the creation of a data report: Data Report on Macroeconomic Indicators: This tests your ability to analyze macroeconomic indicators, such as GDP, the unemployment rate, and inflation, using the national accounting framework. C.2 Learning Objectives Upon completing this module, students will be able to: Understand the principles of national accounting. Identify key macroeconomic indicators within the national accounting framework. Locate and interpret data sources relevant to these indicators. Utilize R to effectively visualize macroeconomic data. Draw insights from these visualizations and understand their implications for economic analysis. C.3 Learning Activities &amp; Assessments Throughout this module, you’ll engage in the following activities: Textbook Engagement: Read the textbook chapter 10 and reproduce the content provided. Input the code from the chapters into RStudio and verify that your results match the chapter outputs. DataCamp Training: Work through the Joining Data with dplyr course on DataCamp. While progressing through the courses, keep an R script handy and apply the learned functions to any of the datasets introduced in this module. This preserves the new functions of the course and potentially offers new insights from the chosen dataset. Data Report Creation: To consolidate your learning, craft a Data Report on Macroeconomic Indicators using R Markdown. Your report should clearly display data, provide meaningful analyses, and embody the module’s content. Your assessment will be based on: DataCamp Course Completion: Finish the designated DataCamp courses for this module. Your grade is based on course completion; thus, you receive full credit by completing all chapters and obtaining a minimum of 75% XPs (experience points) by the deadline. Hence, while the Take Hint and Show Answer options reduce your XPs, they won’t affect your module grade if you stay above the 75% threshold. Data Report Evaluation: The quality of the data report is central to module assessment. It should be professional, suitable for clients, and reflect the module’s content. Ensure you adhere to the data report instructions. For further guidance on maximizing your textbook and DataCamp experience, consult the Textbook Engagement and Learn R with DataCamp sections. For specifications on the data report’s format and content, consult the Data Report Instructions and the subsequent section below. C.4 Data Report on Macroeconomic Indicators For this assignment, select a country of your choice and analyze its macroeconomic indicators. Discuss how these indicators are measured and the data sources used. Visualize the indicators to observe how the selected country’s indicators have evolved over time, identify historical trends, and highlight significant events that have impacted these macroeconomic indicators. As preparation for this assignment, please work through the module chapters and DataCamp courses listed under the overview section above. Ensure your report adheres to the guidelines detailed in Data Report Instructions, which dictates the creation of two versions: official and internal. Upon completion, submit both versions on Blackboard. "],["module-d.html", "D Financial Market Indicators D.1 Overview D.2 Learning Objectives D.3 Learning Activities &amp; Assessments D.4 Data Report on Yield Curves", " D Financial Market Indicators This module delves into key financial market indicators such as interest rates, stock market indices, commodity prices, yield curves, and credit spreads. It provides a detailed exploration of how these indicators are measured, the data sources used, and their importance in assessing the financial markets’ performance and stability. D.1 Overview Outlined below are the chapters contained in this module: Chapter 11: “Financial Market Indicators” delves into topics such as interest rates, stock market metrics, and indicators of financial stability. Additionally, the following DataCamp course supplements the chapter mentioned above: Introduction to Data Visualization with ggplot2: This course introduces the principles of effective data visualization using the ggplot2 package in R, covering foundational plotting concepts and enabling you to create professional exploratory graphics. The module culminates in the creation of a data report: Data Report on Yield Curves: This tests your ability in analyzing yield curves and their forecasting potential for macroeconomic indicators. D.2 Learning Objectives Upon completing this module, students will be able to: Understand and identify key financial market indicators. Locate and interpret data sources relevant to these measures. Utilize R to effectively visualize financial market data. Draw insights from these visualizations and understand their implications. D.3 Learning Activities &amp; Assessments Throughout this module, you’ll engage in the following activities: Textbook Engagement: Read the textbook chapter 11 and reproduce the content provided. Input the code from the chapters into RStudio and verify that your results match the chapter outputs. DataCamp Training: Work through the Introduction to Data Visualization with ggplot2 course on DataCamp. While progressing through the courses, keep an R script handy and apply the learned functions to any of the datasets introduced in this module. This preserves the new functions of the course and potentially offers new insights from the chosen dataset. Data Report Creation: To consolidate your learning, craft a Data Report on Yield Curves using R Markdown. Your report should clearly display data, provide meaningful analyses, and embody the module’s content. Your assessment will be based on: DataCamp Course Completion: Finish the designated DataCamp courses for this module. Your grade is based on course completion; thus, you receive full credit by completing all chapters and obtaining a minimum of 75% XPs (experience points) by the deadline. Hence, while the Take Hint and Show Answer options reduce your XPs, they won’t affect your module grade if you stay above the 75% threshold. Data Report Evaluation: The quality of the data report is central to module assessment. It should be professional, suitable for clients, and reflect the module’s content. Ensure you adhere to the data report instructions. For further guidance on maximizing your textbook and DataCamp experience, consult the Textbook Engagement and Learn R with DataCamp sections. For specifications on the data report’s format and content, consult the Data Report Instructions and the subsequent section below. D.4 Data Report on Yield Curves Analyze the yield curves from your birth date alongside the most recent one. Plot both curves, and if data isn’t available for your birth date (either due to a pre-1990 birth or a non-trading day), use the next available trading day’s data. Utilize insights from Chapter 11 to interpret the curve shapes and compare them against macroeconomic indicators of the respective periods. If you’re unfamiliar with yield curves, consider further research, starting with the U.S. Treasury website where you sourced your data. As preparation for this assignment, please work through the module chapters and DataCamp courses listed under the overview section above. Ensure your report adheres to the guidelines detailed in Data Report Instructions, which dictates the creation of two versions: official and internal. Upon completion, submit both versions on Blackboard. "],["references.html", "References", " References "]]
